From dcb9c0cbbe309faf0eca562a016eb8240614ded9 Mon Sep 17 00:00:00 2001
From: Valentin Raevsky <valentin@compulab.co.il>
Date: Wed, 4 May 2022 21:33:31 +0300
Subject: [PATCH] compulab: em9190: Add modem driver

Signed-off-by: Valentin Raevsky <valentin@compulab.co.il>
---
 drivers/staging/Kconfig                       |    2 +
 drivers/staging/Makefile                      |    1 +
 drivers/staging/em9190/10-mhi-template.rules  |    7 +
 drivers/staging/em9190/Kconfig                |    5 +
 drivers/staging/em9190/Makefile               |    7 +
 drivers/staging/em9190/controllers/Makefile   |   29 +
 .../em9190/controllers/mhi_arch_qcom.c        |  840 +++++
 drivers/staging/em9190/controllers/mhi_qcom.c | 1373 +++++++
 drivers/staging/em9190/controllers/mhi_qcom.h |  125 +
 drivers/staging/em9190/core/Makefile          |   25 +
 drivers/staging/em9190/core/mhi_boot.c        |  608 ++++
 drivers/staging/em9190/core/mhi_dtr.c         |  249 ++
 drivers/staging/em9190/core/mhi_init.c        | 2404 +++++++++++++
 drivers/staging/em9190/core/mhi_internal.h    |  966 +++++
 drivers/staging/em9190/core/mhi_main.c        | 3178 +++++++++++++++++
 drivers/staging/em9190/core/mhi_pm.c          | 1693 +++++++++
 drivers/staging/em9190/devices/Makefile       |   33 +
 drivers/staging/em9190/devices/mhi_netdev.c   | 2286 ++++++++++++
 drivers/staging/em9190/devices/mhi_tty.c      | 1210 +++++++
 drivers/staging/em9190/devices/mhi_tty.h      |  117 +
 drivers/staging/em9190/devices/mhi_uci.c      |  892 +++++
 drivers/staging/em9190/devices/mhi_uci.h      |   96 +
 drivers/staging/em9190/devices/qmap.c         |  350 ++
 drivers/staging/em9190/devices/qmap.h         |  287 ++
 drivers/staging/em9190/inc/define_trace.h     |  127 +
 drivers/staging/em9190/inc/devicetable.h      |   31 +
 drivers/staging/em9190/inc/esoc_client.h      |   92 +
 drivers/staging/em9190/inc/esoc_ctrl.h        |   94 +
 drivers/staging/em9190/inc/if_link_rmnet.h    |   14 +
 drivers/staging/em9190/inc/io.h               |  300 ++
 drivers/staging/em9190/inc/ipc_logging.h      |  290 ++
 drivers/staging/em9190/inc/mhi.h              |  822 +++++
 drivers/staging/em9190/inc/mod_devicetable.h  |  724 ++++
 drivers/staging/em9190/inc/msm-bus.h          |  262 ++
 drivers/staging/em9190/inc/msm_ep_pcie.h      |  291 ++
 drivers/staging/em9190/inc/msm_mhi_dev.h      |  260 ++
 drivers/staging/em9190/inc/msm_pcie.h         |  265 ++
 drivers/staging/em9190/inc/msm_rmnet.h        |  164 +
 drivers/staging/em9190/inc/qmi_rmnet.h        |  143 +
 drivers/staging/em9190/inc/rmnet.h            |   44 +
 drivers/staging/em9190/inc/rmnet_qmi.h        |   88 +
 drivers/staging/em9190/readme.txt             |  247 ++
 drivers/staging/em9190/xdp_readme.txt         |   33 +
 43 files changed, 21074 insertions(+)
 create mode 100644 drivers/staging/em9190/10-mhi-template.rules
 create mode 100644 drivers/staging/em9190/Kconfig
 create mode 100644 drivers/staging/em9190/Makefile
 create mode 100644 drivers/staging/em9190/controllers/Makefile
 create mode 100644 drivers/staging/em9190/controllers/mhi_arch_qcom.c
 create mode 100644 drivers/staging/em9190/controllers/mhi_qcom.c
 create mode 100644 drivers/staging/em9190/controllers/mhi_qcom.h
 create mode 100644 drivers/staging/em9190/core/Makefile
 create mode 100644 drivers/staging/em9190/core/mhi_boot.c
 create mode 100644 drivers/staging/em9190/core/mhi_dtr.c
 create mode 100644 drivers/staging/em9190/core/mhi_init.c
 create mode 100644 drivers/staging/em9190/core/mhi_internal.h
 create mode 100644 drivers/staging/em9190/core/mhi_main.c
 create mode 100644 drivers/staging/em9190/core/mhi_pm.c
 create mode 100644 drivers/staging/em9190/devices/Makefile
 create mode 100644 drivers/staging/em9190/devices/mhi_netdev.c
 create mode 100644 drivers/staging/em9190/devices/mhi_tty.c
 create mode 100644 drivers/staging/em9190/devices/mhi_tty.h
 create mode 100644 drivers/staging/em9190/devices/mhi_uci.c
 create mode 100644 drivers/staging/em9190/devices/mhi_uci.h
 create mode 100644 drivers/staging/em9190/devices/qmap.c
 create mode 100644 drivers/staging/em9190/devices/qmap.h
 create mode 100644 drivers/staging/em9190/inc/define_trace.h
 create mode 100644 drivers/staging/em9190/inc/devicetable.h
 create mode 100644 drivers/staging/em9190/inc/esoc_client.h
 create mode 100644 drivers/staging/em9190/inc/esoc_ctrl.h
 create mode 100644 drivers/staging/em9190/inc/if_link_rmnet.h
 create mode 100644 drivers/staging/em9190/inc/io.h
 create mode 100644 drivers/staging/em9190/inc/ipc_logging.h
 create mode 100644 drivers/staging/em9190/inc/mhi.h
 create mode 100644 drivers/staging/em9190/inc/mod_devicetable.h
 create mode 100644 drivers/staging/em9190/inc/msm-bus.h
 create mode 100644 drivers/staging/em9190/inc/msm_ep_pcie.h
 create mode 100644 drivers/staging/em9190/inc/msm_mhi_dev.h
 create mode 100644 drivers/staging/em9190/inc/msm_pcie.h
 create mode 100644 drivers/staging/em9190/inc/msm_rmnet.h
 create mode 100644 drivers/staging/em9190/inc/qmi_rmnet.h
 create mode 100644 drivers/staging/em9190/inc/rmnet.h
 create mode 100644 drivers/staging/em9190/inc/rmnet_qmi.h
 create mode 100644 drivers/staging/em9190/readme.txt
 create mode 100644 drivers/staging/em9190/xdp_readme.txt

diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index f6666a4e9e00..9b7e6269ad3d 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -108,4 +108,6 @@ source "drivers/staging/wfx/Kconfig"
 
 source "drivers/staging/fsl_ppfe/Kconfig"
 
+source "drivers/staging/em9190/Kconfig"
+
 endif # STAGING
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index 0069facc23aa..e483fd750992 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -44,3 +44,4 @@ obj-$(CONFIG_FIELDBUS_DEV)     += fieldbus/
 obj-$(CONFIG_QLGE)		+= qlge/
 obj-$(CONFIG_WFX)		+= wfx/
 obj-$(CONFIG_FSL_PPFE)		+= fsl_ppfe/
+obj-$(CONFIG_EM9190)		+= em9190/
diff --git a/drivers/staging/em9190/10-mhi-template.rules b/drivers/staging/em9190/10-mhi-template.rules
new file mode 100644
index 000000000000..a88878ecaf39
--- /dev/null
+++ b/drivers/staging/em9190/10-mhi-template.rules
@@ -0,0 +1,7 @@
+ACTION=="add" KERNEL=="????_??.??.??_DUN", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhitty.ko"
+ACTION=="add" KERNEL=="????_??.??.??_DIAG", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhitty.ko"
+ACTION=="add" KERNEL=="????_??.??.??_MBIM", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhiuci.ko"
+ACTION=="add" KERNEL=="????_??.??.??_QMI0", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhiuci.ko"
+ACTION=="add" KERNEL=="????_??.??.??_IP_HW0", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhinet.ko"
+ACTION=="add" KERNEL=="????_??.??.??_SAHARA", SUBSYSTEM=="mhi", RUN+="/sbin/insmod /lib/modules/KERNEL_VERSION/kernel/drivers/pci/mhitty.ko"
+
diff --git a/drivers/staging/em9190/Kconfig b/drivers/staging/em9190/Kconfig
new file mode 100644
index 000000000000..a8e0ca5551e0
--- /dev/null
+++ b/drivers/staging/em9190/Kconfig
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0
+config EM9190
+	tristate "EM9190 - Sierra Wireless PCIe Driver"
+	help
+	  This option allows you to enable support for the EM9190 modem
diff --git a/drivers/staging/em9190/Makefile b/drivers/staging/em9190/Makefile
new file mode 100644
index 000000000000..07583ad6d760
--- /dev/null
+++ b/drivers/staging/em9190/Makefile
@@ -0,0 +1,7 @@
+#
+# Makefile for the MHI stack
+#
+
+ccflags-y += -O2 -g -DMHINET_DEBUG
+
+obj-$(CONFIG_EM9190) := controllers/ devices/
diff --git a/drivers/staging/em9190/controllers/Makefile b/drivers/staging/em9190/controllers/Makefile
new file mode 100644
index 000000000000..a9388b22a008
--- /dev/null
+++ b/drivers/staging/em9190/controllers/Makefile
@@ -0,0 +1,29 @@
+# Comment/uncomment the following line to disable/enable debugging
+#DEBUG = y
+
+# Add your debugging flag (or not) to CFLAGS
+ifeq ($(DEBUG),y)
+  DEBFLAGS = -O2 -g -DMHINET_DEBUG # "-O" is needed to expand inlines
+else
+  DEBFLAGS = -O2
+endif
+
+ccflags-$(DEBUG):=$(DEBFLAGS)
+
+obj-m	:= mhictrl.o
+mhictrl-objs := mhi_qcom.o mhi_arch_qcom.o ../core/mhi_init.o ../core/mhi_main.o ../core/mhi_pm.o ../core/mhi_boot.o ../core/mhi_dtr.o
+
+KERNELDIR ?= /lib/modules/$(shell uname -r)/build
+PWD       := $(shell pwd)
+
+all:
+	$(MAKE) -C $(KERNELDIR) M=$(PWD)
+
+	cp ./Module.symvers ../devices/Module.symvers
+
+clean:
+	rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions *.o.ur-safe *.symvers *.order .cache.mk
+
+install:
+	sudo cp ./mhictrl.ko /lib/modules/`uname -r`/kernel/drivers/pci/mhictrl.ko
+
diff --git a/drivers/staging/em9190/controllers/mhi_arch_qcom.c b/drivers/staging/em9190/controllers/mhi_arch_qcom.c
new file mode 100644
index 000000000000..f2f71476ec05
--- /dev/null
+++ b/drivers/staging/em9190/controllers/mhi_arch_qcom.c
@@ -0,0 +1,840 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+//#include <asm/dma-iommu.h>
+#include <linux/async.h>
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include "../inc/esoc_client.h"
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/memblock.h>
+#include <linux/module.h>
+#include "../inc/msm-bus.h"
+#include "../inc/msm_pcie.h"
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/suspend.h>
+#include <linux/interrupt.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+
+#include "../inc/mhi.h"
+#include "mhi_qcom.h"
+
+struct arch_info {
+	struct mhi_dev *mhi_dev;
+	struct esoc_desc *esoc_client;
+	struct esoc_client_hook esoc_ops;
+	struct msm_bus_scale_pdata *msm_bus_pdata;
+	u32 bus_client;
+	struct msm_pcie_register_event pcie_reg_event;
+	struct pci_saved_state *pcie_state;
+	struct dma_iommu_mapping *mapping;
+	async_cookie_t cookie;
+	void *boot_ipc_log;
+	void *tsync_ipc_log;
+	struct mhi_device *boot_dev;
+	struct mhi_link_info current_link_info;
+	struct work_struct bw_scale_work;
+	struct notifier_block pm_notifier;
+	struct completion pm_completion;
+};
+
+/* ipc log markings */
+#define DLOG "Dev->Host: "
+#define HLOG "Host: "
+
+#define MHI_TSYNC_LOG_PAGES (10)
+
+#ifdef CONFIG_MHI_DEBUG
+
+#define MHI_IPC_LOG_PAGES (100)
+enum MHI_DEBUG_LEVEL  mhi_ipc_log_lvl = MHI_MSG_LVL_VERBOSE;
+
+#else
+
+#define MHI_IPC_LOG_PAGES (10)
+enum MHI_DEBUG_LEVEL  mhi_ipc_log_lvl = MHI_MSG_LVL_ERROR;
+
+#endif
+
+
+#if 0
+
+static int mhi_arch_pm_notifier(struct notifier_block *nb,
+				unsigned long event, void *unused)
+{
+	struct arch_info *arch_info =
+		container_of(nb, struct arch_info, pm_notifier);
+
+	switch (event) {
+	case PM_SUSPEND_PREPARE:
+		reinit_completion(&arch_info->pm_completion);
+		break;
+
+	case PM_POST_SUSPEND:
+		complete_all(&arch_info->pm_completion);
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void mhi_arch_timesync_log(struct mhi_controller *mhi_cntrl,
+				  u64 remote_time)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	if (remote_time != U64_MAX)
+		ipc_log_string(arch_info->tsync_ipc_log, "%6u.%06lu 0x%llx",
+			       REMOTE_TICKS_TO_SEC(remote_time),
+			       REMOTE_TIME_REMAINDER_US(remote_time),
+			       remote_time);
+}
+
+static int mhi_arch_set_bus_request(struct mhi_controller *mhi_cntrl, int index)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	MHI_LOG("Setting bus request to index %d\n", index);
+
+	if (arch_info->bus_client)
+		return msm_bus_scale_client_update_request(
+							arch_info->bus_client,
+							index);
+
+	/* default return success */
+	return 0;
+}
+
+static void mhi_arch_pci_link_state_cb(struct msm_pcie_notify *notify)
+{
+	struct mhi_controller *mhi_cntrl = notify->data;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+
+	switch (notify->event) {
+	case MSM_PCIE_EVENT_WAKEUP:
+		MHI_LOG("Received MSM_PCIE_EVENT_WAKE signal\n");
+
+		/* bring link out of d3cold */
+		if (mhi_dev->powered_on) {
+			pm_runtime_get(&pci_dev->dev);
+			pm_runtime_put_noidle(&pci_dev->dev);
+		}
+		break;
+	case MSM_PCIE_EVENT_L1SS_TIMEOUT:
+		MHI_VERB("Received MSM_PCIE_EVENT_L1SS_TIMEOUT signal\n");
+
+		pm_runtime_mark_last_busy(&pci_dev->dev);
+		pm_request_autosuspend(&pci_dev->dev);
+		break;
+	default:
+		MHI_ERR("Unhandled event 0x%x\n", notify->event);
+	}
+}
+
+static int mhi_arch_esoc_ops_power_on(void *priv, unsigned int flags)
+{
+	struct mhi_controller *mhi_cntrl = priv;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+	int ret;
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+	if (mhi_dev->powered_on) {
+		MHI_LOG("MHI still in active state\n");
+		mutex_unlock(&mhi_cntrl->pm_mutex);
+		return 0;
+	}
+
+	MHI_LOG("Enter\n");
+
+	/* reset rpm state */
+	pm_runtime_set_active(&pci_dev->dev);
+	pm_runtime_enable(&pci_dev->dev);
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+	pm_runtime_forbid(&pci_dev->dev);
+	ret = pm_runtime_get_sync(&pci_dev->dev);
+	if (ret < 0) {
+		MHI_ERR("Error with rpm resume, ret:%d\n", ret);
+		return ret;
+	}
+
+	/* re-start the link & recover default cfg states */
+	ret = msm_pcie_pm_control(MSM_PCIE_RESUME, pci_dev->bus->number,
+				  pci_dev, NULL, 0);
+	if (ret) {
+		MHI_ERR("Failed to resume pcie bus ret %d\n", ret);
+		return ret;
+	}
+
+	return mhi_pci_probe(pci_dev, NULL);
+}
+
+static void mhi_arch_link_off(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+
+	MHI_LOG("Entered\n");
+
+	pci_set_power_state(pci_dev, PCI_D3hot);
+
+	/* release the resources */
+	msm_pcie_pm_control(MSM_PCIE_SUSPEND, mhi_cntrl->bus, pci_dev, NULL, 0);
+	mhi_arch_set_bus_request(mhi_cntrl, 0);
+
+	MHI_LOG("Exited\n");
+}
+
+static void mhi_arch_esoc_ops_power_off(void *priv, unsigned int flags)
+{
+	struct mhi_controller *mhi_cntrl = priv;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	bool mdm_state = (flags & ESOC_HOOK_MDM_CRASH);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+
+	MHI_LOG("Enter: mdm_crashed:%d\n", mdm_state);
+
+	/*
+	 * Abort system suspend if system is preparing to go to suspend
+	 * by grabbing wake source.
+	 * If system is suspended, wait for pm notifier callback to notify
+	 * that resume has occurred with PM_POST_SUSPEND event.
+	 */
+	pm_stay_awake(&mhi_cntrl->mhi_dev->dev);
+	wait_for_completion(&arch_info->pm_completion);
+
+	/* if link is in drv suspend, wake it up */
+	pm_runtime_get_sync(&pci_dev->dev);
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+	if (!mhi_dev->powered_on) {
+		MHI_LOG("Not in active state\n");
+		mutex_unlock(&mhi_cntrl->pm_mutex);
+		pm_runtime_put_noidle(&pci_dev->dev);
+		return;
+	}
+	mhi_dev->powered_on = false;
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	pm_runtime_put_noidle(&pci_dev->dev);
+
+	MHI_LOG("Triggering shutdown process\n");
+	mhi_power_down(mhi_cntrl, !mdm_state);
+
+	/* turn the link off */
+	mhi_deinit_pci_dev(mhi_cntrl);
+	mhi_arch_link_off(mhi_cntrl);
+
+	/* wait for boot monitor to exit */
+	async_synchronize_cookie(arch_info->cookie + 1);
+
+	mhi_arch_iommu_deinit(mhi_cntrl);
+	mhi_arch_pcie_deinit(mhi_cntrl);
+
+	pm_relax(&mhi_cntrl->mhi_dev->dev);
+}
+
+static void mhi_arch_esoc_ops_mdm_error(void *priv)
+{
+	struct mhi_controller *mhi_cntrl = priv;
+
+	MHI_LOG("Enter: mdm asserted\n");
+
+	/* transition MHI state into error state */
+	mhi_control_error(mhi_cntrl);
+
+	MHI_LOG("Exit\n");
+}
+
+static void mhi_bl_dl_cb(struct mhi_device *mhi_device,
+			 struct mhi_result *mhi_result)
+{
+	struct mhi_controller *mhi_cntrl = mhi_device->mhi_cntrl;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	char *buf = mhi_result->buf_addr;
+
+	/* force a null at last character */
+	buf[mhi_result->bytes_xferd - 1] = 0;
+
+	ipc_log_string(arch_info->boot_ipc_log, "%s %s", DLOG, buf);
+}
+
+static void mhi_bl_dummy_cb(struct mhi_device *mhi_dev,
+			    struct mhi_result *mhi_result)
+{
+}
+
+static void mhi_bl_remove(struct mhi_device *mhi_device)
+{
+	struct mhi_controller *mhi_cntrl = mhi_device->mhi_cntrl;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	arch_info->boot_dev = NULL;
+	ipc_log_string(arch_info->boot_ipc_log,
+		       HLOG "Received Remove notif.\n");
+}
+
+static void mhi_boot_monitor(void *data, async_cookie_t cookie)
+{
+	struct mhi_controller *mhi_cntrl = data;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct mhi_device *boot_dev;
+	/* 15 sec timeout for booting device */
+	const u32 timeout = msecs_to_jiffies(15000);
+
+	/* wait for device to enter boot stage */
+	wait_event_timeout(mhi_cntrl->state_event, mhi_cntrl->ee == MHI_EE_AMSS
+			   || mhi_cntrl->ee == MHI_EE_DISABLE_TRANSITION,
+			   timeout);
+
+	ipc_log_string(arch_info->boot_ipc_log, HLOG "Device current ee = %s\n",
+		       TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	/* if we successfully booted to amss disable boot log channel */
+	if (mhi_cntrl->ee == MHI_EE_AMSS) {
+		boot_dev = arch_info->boot_dev;
+		if (boot_dev)
+			mhi_unprepare_from_transfer(boot_dev);
+
+		pm_runtime_allow(&mhi_dev->pci_dev->dev);
+	}
+}
+
+int mhi_arch_power_up(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	/* start a boot monitor */
+	arch_info->cookie = async_schedule(mhi_boot_monitor, mhi_cntrl);
+
+	return 0;
+}
+
+static  int mhi_arch_pcie_scale_bw(struct mhi_controller *mhi_cntrl,
+				   struct pci_dev *pci_dev,
+				   struct mhi_link_info *link_info)
+{
+	int ret, scale;
+
+	mhi_cntrl->lpm_disable(mhi_cntrl, mhi_cntrl->priv_data);
+	ret = msm_pcie_set_link_bandwidth(pci_dev, link_info->target_link_speed,
+					  link_info->target_link_width);
+	mhi_cntrl->lpm_enable(mhi_cntrl, mhi_cntrl->priv_data);
+
+	if (ret)
+		return ret;
+
+	/* if we switch to low bw release bus scale voting */
+	scale = !(link_info->target_link_speed == PCI_EXP_LNKSTA_CLS_2_5GB);
+	mhi_arch_set_bus_request(mhi_cntrl, scale);
+
+	MHI_VERB("bw changed to speed:0x%x width:0x%x bus_scale:%d\n",
+		 link_info->target_link_speed, link_info->target_link_width,
+		 scale);
+
+	return 0;
+}
+
+static void mhi_arch_pcie_bw_scale_work(struct work_struct *work)
+{
+	struct arch_info *arch_info = container_of(work,
+						   struct arch_info,
+						   bw_scale_work);
+	struct mhi_dev *mhi_dev = arch_info->mhi_dev;
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+	struct device *dev = &pci_dev->dev;
+	struct mhi_controller *mhi_cntrl = dev_get_drvdata(dev);
+	struct mhi_link_info mhi_link_info;
+	struct mhi_link_info *cur_info = &arch_info->current_link_info;
+	int ret;
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+	if (!mhi_dev->powered_on || MHI_IS_SUSPENDED(mhi_dev->suspend_mode))
+		goto exit_work;
+
+	/* copy the latest speed change */
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	mhi_link_info = mhi_cntrl->mhi_link_info;
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/* link is already set to current settings */
+	if (cur_info->target_link_speed == mhi_link_info.target_link_speed &&
+	    cur_info->target_link_width == mhi_link_info.target_link_width)
+		goto exit_work;
+
+	ret = mhi_arch_pcie_scale_bw(mhi_cntrl, pci_dev, &mhi_link_info);
+	if (ret)
+		goto exit_work;
+
+	*cur_info = mhi_link_info;
+
+exit_work:
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+}
+
+static void mhi_arch_pcie_bw_scale_cb(struct mhi_controller *mhi_cntrl,
+				      struct mhi_dev *mhi_dev)
+{
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	schedule_work(&arch_info->bw_scale_work);
+}
+
+static int mhi_bl_probe(struct mhi_device *mhi_device,
+			const struct mhi_device_id *id)
+{
+	char node_name[32];
+	struct mhi_controller *mhi_cntrl = mhi_device->mhi_cntrl;
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+
+	snprintf(node_name, sizeof(node_name), "mhi_bl_%04x_%02u.%02u.%02u",
+		 mhi_device->dev_id, mhi_device->domain, mhi_device->bus,
+		 mhi_device->slot);
+
+	arch_info->boot_dev = mhi_device;
+	arch_info->boot_ipc_log = ipc_log_context_create(MHI_IPC_LOG_PAGES,
+							 node_name, 0);
+	ipc_log_string(arch_info->boot_ipc_log, HLOG
+		       "Entered SBL, Session ID:0x%x\n", mhi_cntrl->session_id);
+
+	return 0;
+}
+
+static const struct mhi_device_id mhi_bl_match_table[] = {
+	{ .chan = "BL" },
+	{},
+};
+
+static struct mhi_driver mhi_bl_driver = {
+	.id_table = mhi_bl_match_table,
+	.remove = mhi_bl_remove,
+	.probe = mhi_bl_probe,
+	.ul_xfer_cb = mhi_bl_dummy_cb,
+	.dl_xfer_cb = mhi_bl_dl_cb,
+	.driver = {
+		.name = "MHI_BL",
+		.owner = THIS_MODULE,
+	},
+};
+
+int mhi_arch_pcie_init(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	char node[32];
+	int ret;
+	u16 linkstat;
+
+	if (!arch_info) {
+		struct msm_pcie_register_event *reg_event;
+		struct mhi_link_info *cur_link_info;
+
+		arch_info = devm_kzalloc(&mhi_dev->pci_dev->dev,
+					 sizeof(*arch_info), GFP_KERNEL);
+		if (!arch_info)
+			return -ENOMEM;
+
+		mhi_dev->arch_info = arch_info;
+		arch_info->mhi_dev = mhi_dev;
+
+		snprintf(node, sizeof(node), "mhi_%04x_%02u.%02u.%02u",
+			 mhi_cntrl->dev_id, mhi_cntrl->domain, mhi_cntrl->bus,
+			 mhi_cntrl->slot);
+		mhi_cntrl->log_buf = ipc_log_context_create(MHI_IPC_LOG_PAGES,
+							    node, 0);
+		mhi_cntrl->log_lvl = mhi_ipc_log_lvl;
+
+		snprintf(node, sizeof(node), "mhi_tsync_%04x_%02u.%02u.%02u",
+			 mhi_cntrl->dev_id, mhi_cntrl->domain, mhi_cntrl->bus,
+			 mhi_cntrl->slot);
+		arch_info->tsync_ipc_log = ipc_log_context_create(
+					   MHI_TSYNC_LOG_PAGES, node, 0);
+		if (arch_info->tsync_ipc_log)
+			mhi_cntrl->tsync_log = mhi_arch_timesync_log;
+
+		/* register for bus scale if defined */
+		arch_info->msm_bus_pdata = msm_bus_cl_get_pdata_from_dev(
+							&mhi_dev->pci_dev->dev);
+		if (arch_info->msm_bus_pdata) {
+			arch_info->bus_client =
+				msm_bus_scale_register_client(
+						arch_info->msm_bus_pdata);
+			if (!arch_info->bus_client)
+				return -EINVAL;
+		}
+
+		/* register with pcie rc for WAKE# events */
+		reg_event = &arch_info->pcie_reg_event;
+		reg_event->events =
+			MSM_PCIE_EVENT_WAKEUP | MSM_PCIE_EVENT_L1SS_TIMEOUT;
+
+		reg_event->user = mhi_dev->pci_dev;
+		reg_event->callback = mhi_arch_pci_link_state_cb;
+		reg_event->notify.data = mhi_cntrl;
+		ret = msm_pcie_register_event(reg_event);
+		if (ret)
+			MHI_LOG("Failed to reg. for link up notification\n");
+
+		init_completion(&arch_info->pm_completion);
+
+		/* register PM notifier to get post resume events */
+		arch_info->pm_notifier.notifier_call = mhi_arch_pm_notifier;
+		register_pm_notifier(&arch_info->pm_notifier);
+
+		/*
+		 * Mark as completed at initial boot-up to allow ESOC power on
+		 * callback to proceed if system has not gone to suspend
+		 */
+		complete_all(&arch_info->pm_completion);
+
+		arch_info->esoc_client = devm_register_esoc_client(
+						&mhi_dev->pci_dev->dev, "mdm");
+		if (IS_ERR_OR_NULL(arch_info->esoc_client)) {
+			MHI_ERR("Failed to register esoc client\n");
+		} else {
+			/* register for power on/off hooks */
+			struct esoc_client_hook *esoc_ops =
+				&arch_info->esoc_ops;
+
+			esoc_ops->priv = mhi_cntrl;
+			esoc_ops->prio = ESOC_MHI_HOOK;
+			esoc_ops->esoc_link_power_on =
+				mhi_arch_esoc_ops_power_on;
+			esoc_ops->esoc_link_power_off =
+				mhi_arch_esoc_ops_power_off;
+			esoc_ops->esoc_link_mdm_crash =
+				mhi_arch_esoc_ops_mdm_error;
+
+			ret = esoc_register_client_hook(arch_info->esoc_client,
+							esoc_ops);
+			if (ret)
+				MHI_ERR("Failed to register esoc ops\n");
+		}
+
+		/*
+		 * MHI host driver has full autonomy to manage power state.
+		 * Disable all automatic power collapse features
+		 */
+		msm_pcie_pm_control(MSM_PCIE_DISABLE_PC, mhi_cntrl->bus,
+				    mhi_dev->pci_dev, NULL, 0);
+		mhi_dev->pci_dev->no_d3hot = true;
+
+		INIT_WORK(&arch_info->bw_scale_work,
+			  mhi_arch_pcie_bw_scale_work);
+		mhi_dev->bw_scale = mhi_arch_pcie_bw_scale_cb;
+
+		/* store the current bw info */
+		ret = pcie_capability_read_word(mhi_dev->pci_dev,
+						PCI_EXP_LNKSTA, &linkstat);
+		if (ret)
+			return ret;
+
+		cur_link_info = &arch_info->current_link_info;
+		cur_link_info->target_link_speed =
+			linkstat & PCI_EXP_LNKSTA_CLS;
+		cur_link_info->target_link_width =
+			(linkstat & PCI_EXP_LNKSTA_NLW) >>
+			PCI_EXP_LNKSTA_NLW_SHIFT;
+		mhi_cntrl->mhi_link_info = *cur_link_info;
+
+		mhi_driver_register(&mhi_bl_driver);
+	}
+
+	return mhi_arch_set_bus_request(mhi_cntrl, 1);
+}
+
+void mhi_arch_pcie_deinit(struct mhi_controller *mhi_cntrl)
+{
+	mhi_arch_set_bus_request(mhi_cntrl, 0);
+}
+
+static struct dma_iommu_mapping *mhi_arch_create_iommu_mapping(
+					struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	dma_addr_t base;
+	size_t size;
+
+	/*
+	 * If S1_BYPASS enabled then iommu space is not used, however framework
+	 * still require clients to create a mapping space before attaching. So
+	 * set to smallest size required by iommu framework.
+	 */
+	if (mhi_dev->smmu_cfg & MHI_SMMU_S1_BYPASS) {
+		base = 0;
+		size = PAGE_SIZE;
+	} else {
+		base = mhi_dev->iova_start;
+		size = (mhi_dev->iova_stop - base) + 1;
+	}
+
+	MHI_LOG("Create iommu mapping of base:%pad size:%zu\n",
+		&base, size);
+	return arm_iommu_create_mapping(&pci_bus_type, base, size);
+}
+
+int mhi_arch_iommu_init(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	u32 smmu_config = mhi_dev->smmu_cfg;
+	struct dma_iommu_mapping *mapping = NULL;
+	int ret;
+
+	if (smmu_config) {
+		mapping = mhi_arch_create_iommu_mapping(mhi_cntrl);
+		if (IS_ERR(mapping)) {
+			MHI_ERR("Failed to create iommu mapping\n");
+			return PTR_ERR(mapping);
+		}
+	}
+
+	if (smmu_config & MHI_SMMU_S1_BYPASS) {
+		int s1_bypass = 1;
+
+		ret = iommu_domain_set_attr(mapping->domain,
+					    DOMAIN_ATTR_S1_BYPASS, &s1_bypass);
+		if (ret) {
+			MHI_ERR("Failed to set attribute S1_BYPASS\n");
+			goto release_mapping;
+		}
+	}
+
+	if (smmu_config & MHI_SMMU_FAST) {
+		int fast_map = 1;
+
+		ret = iommu_domain_set_attr(mapping->domain, DOMAIN_ATTR_FAST,
+					    &fast_map);
+		if (ret) {
+			MHI_ERR("Failed to set attribute FAST_MAP\n");
+			goto release_mapping;
+		}
+	}
+
+	if (smmu_config & MHI_SMMU_ATOMIC) {
+		int atomic = 1;
+
+		ret = iommu_domain_set_attr(mapping->domain, DOMAIN_ATTR_ATOMIC,
+					    &atomic);
+		if (ret) {
+			MHI_ERR("Failed to set attribute ATOMIC\n");
+			goto release_mapping;
+		}
+	}
+
+	if (smmu_config & MHI_SMMU_FORCE_COHERENT) {
+		int force_coherent = 1;
+
+		ret = iommu_domain_set_attr(mapping->domain,
+					DOMAIN_ATTR_PAGE_TABLE_FORCE_COHERENT,
+					&force_coherent);
+		if (ret) {
+			MHI_ERR("Failed to set attribute FORCE_COHERENT\n");
+			goto release_mapping;
+		}
+	}
+
+	if (smmu_config) {
+		ret = arm_iommu_attach_device(&mhi_dev->pci_dev->dev, mapping);
+
+		if (ret) {
+			MHI_ERR("Error attach device, ret:%d\n", ret);
+			goto release_mapping;
+		}
+		arch_info->mapping = mapping;
+	}
+
+	mhi_cntrl->dev = &mhi_dev->pci_dev->dev;
+
+	ret = dma_set_mask_and_coherent(mhi_cntrl->dev, DMA_BIT_MASK(64));
+	if (ret) {
+		MHI_ERR("Error setting dma mask, ret:%d\n", ret);
+		goto release_device;
+	}
+
+	return 0;
+
+release_device:
+	arm_iommu_detach_device(mhi_cntrl->dev);
+
+release_mapping:
+	arm_iommu_release_mapping(mapping);
+
+	return ret;
+}
+
+void mhi_arch_iommu_deinit(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct dma_iommu_mapping *mapping = arch_info->mapping;
+
+	if (mapping) {
+		arm_iommu_detach_device(mhi_cntrl->dev);
+		arm_iommu_release_mapping(mapping);
+	}
+	arch_info->mapping = NULL;
+	mhi_cntrl->dev = NULL;
+}
+
+int mhi_arch_link_suspend(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+	int ret = 0;
+
+	MHI_LOG("Entered\n");
+
+	/* disable inactivity timer */
+	msm_pcie_l1ss_timeout_disable(pci_dev);
+
+	switch (mhi_dev->suspend_mode) {
+	case MHI_DEFAULT_SUSPEND:
+		pci_clear_master(pci_dev);
+		ret = pci_save_state(mhi_dev->pci_dev);
+		if (ret) {
+			MHI_ERR("Failed with pci_save_state, ret:%d\n", ret);
+			goto exit_suspend;
+		}
+
+		arch_info->pcie_state = pci_store_saved_state(pci_dev);
+		pci_disable_device(pci_dev);
+
+		pci_set_power_state(pci_dev, PCI_D3hot);
+
+		/* release the resources */
+		msm_pcie_pm_control(MSM_PCIE_SUSPEND, mhi_cntrl->bus, pci_dev,
+				    NULL, 0);
+		mhi_arch_set_bus_request(mhi_cntrl, 0);
+		break;
+	case MHI_FAST_LINK_OFF:
+	case MHI_ACTIVE_STATE:
+	case MHI_FAST_LINK_ON:/* keeping link on do nothing */
+		break;
+	}
+
+exit_suspend:
+	if (ret)
+		msm_pcie_l1ss_timeout_enable(pci_dev);
+
+	MHI_LOG("Exited with ret:%d\n", ret);
+
+	return ret;
+}
+
+static int __mhi_arch_link_resume(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+	struct mhi_link_info *cur_info = &arch_info->current_link_info;
+	int ret;
+
+	MHI_LOG("Entered\n");
+
+	/* request bus scale voting if we're on Gen 2 or higher speed */
+	if (cur_info->target_link_speed != PCI_EXP_LNKSTA_CLS_2_5GB) {
+		ret = mhi_arch_set_bus_request(mhi_cntrl, 1);
+		if (ret)
+			MHI_LOG("Could not set bus frequency, ret:%d\n", ret);
+	}
+
+	ret = msm_pcie_pm_control(MSM_PCIE_RESUME, mhi_cntrl->bus, pci_dev,
+				  NULL, 0);
+	if (ret) {
+		MHI_ERR("Link training failed, ret:%d\n", ret);
+		return ret;
+	}
+
+	ret = pci_set_power_state(pci_dev, PCI_D0);
+	if (ret) {
+		MHI_ERR("Failed to set PCI_D0 state, ret:%d\n", ret);
+		return ret;
+	}
+
+	ret = pci_enable_device(pci_dev);
+	if (ret) {
+		MHI_ERR("Failed to enable device, ret:%d\n", ret);
+		return ret;
+	}
+
+	ret = pci_load_and_free_saved_state(pci_dev, &arch_info->pcie_state);
+	if (ret)
+		MHI_LOG("Failed to load saved cfg state\n");
+
+	pci_restore_state(pci_dev);
+	pci_set_master(pci_dev);
+
+	return 0;
+}
+
+int mhi_arch_link_resume(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+	struct arch_info *arch_info = mhi_dev->arch_info;
+	struct pci_dev *pci_dev = mhi_dev->pci_dev;
+	struct mhi_link_info *cur_info = &arch_info->current_link_info;
+	struct mhi_link_info *updated_info = &mhi_cntrl->mhi_link_info;
+	int ret = 0;
+
+	MHI_LOG("Entered\n");
+
+	switch (mhi_dev->suspend_mode) {
+	case MHI_DEFAULT_SUSPEND:
+		ret = __mhi_arch_link_resume(mhi_cntrl);
+		break;
+	case MHI_FAST_LINK_OFF:
+	case MHI_ACTIVE_STATE:
+	case MHI_FAST_LINK_ON:
+		break;
+	}
+
+	if (ret) {
+		MHI_ERR("Link training failed, ret:%d\n", ret);
+		return ret;
+	}
+
+	/* BW request from device doesn't match current link speed */
+	if (cur_info->target_link_speed != updated_info->target_link_speed ||
+	    cur_info->target_link_width != updated_info->target_link_width) {
+		ret = mhi_arch_pcie_scale_bw(mhi_cntrl, pci_dev, updated_info);
+		if (!ret)
+			*cur_info = *updated_info;
+	}
+
+	msm_pcie_l1ss_timeout_enable(pci_dev);
+
+	MHI_LOG("Exited\n");
+
+	return 0;
+}
+
+#endif //CONFIG_ARCH_QCOM
diff --git a/drivers/staging/em9190/controllers/mhi_qcom.c b/drivers/staging/em9190/controllers/mhi_qcom.c
new file mode 100644
index 000000000000..bb1fcba22ad7
--- /dev/null
+++ b/drivers/staging/em9190/controllers/mhi_qcom.c
@@ -0,0 +1,1373 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+//#include <asm/arch_timer.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/msi.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/memblock.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/version.h>
+#include "../inc/mhi.h"
+#include "mhi_qcom.h"
+
+/* set PCI_HELPER to 0 to remove PCI helper */
+#define PCI_HELPER	1
+
+#if PCI_HELPER 
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+
+#define PCI_IRQ_LEGACY		(1 << 0) /* allow legacy interrupts */
+#define PCI_IRQ_MSI		(1 << 1) /* allow MSI interrupts */
+#define PCI_IRQ_MSIX		(1 << 2) /* allow MSI-X interrupts */
+#define PCI_IRQ_AFFINITY	(1 << 3) /* auto-assign affinity */
+#define PCI_IRQ_ALL_TYPES \
+	(PCI_IRQ_LEGACY | PCI_IRQ_MSI | PCI_IRQ_MSIX)
+
+struct irq_affinity {
+	int	pre_vectors;
+	int	post_vectors;
+};
+	
+int
+pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
+				   unsigned int max_vecs, unsigned int flags,
+				   const struct irq_affinity *affd)
+{
+	static const struct irq_affinity msi_default_affd;
+	int vecs = -ENOSPC;
+
+	if (flags & PCI_IRQ_AFFINITY) {
+		if (!affd)
+			affd = &msi_default_affd;
+	} else {
+		if (WARN_ON(affd))
+			affd = NULL;
+	}
+
+	if (flags & PCI_IRQ_MSIX) {
+		vecs = pci_enable_msix_range(dev, NULL, min_vecs, max_vecs);
+		if (vecs > 0)
+			return vecs;
+	}
+
+	if (flags & PCI_IRQ_MSI) {
+		vecs = pci_enable_msi_range(dev, min_vecs, max_vecs);
+		if (vecs > 0)
+			return vecs;
+	}
+
+	/* use legacy irq if allowed */
+	if (flags & PCI_IRQ_LEGACY) {
+		if (min_vecs == 1 && dev->irq) {
+			pci_intx(dev, 1);
+			return 1;
+		}
+	}
+
+	return vecs;
+}
+
+static inline int
+pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,
+		      unsigned int max_vecs, unsigned int flags)
+{
+	return pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs, flags,
+					      NULL);
+}
+
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0)
+void pci_free_irq_vectors(struct pci_dev *dev)
+{
+	pci_disable_msix(dev);
+	pci_disable_msi(dev);
+}
+
+#ifndef dev_to_msi_list
+#define dev_to_msi_list(dev)		(&(dev)->msi_list)
+#endif
+
+#ifndef first_msi_entry
+#define first_msi_entry(dev)		\
+	list_first_entry(dev_to_msi_list((dev)), struct msi_desc, list)
+#endif
+
+#ifndef first_pci_msi_entry
+#define first_pci_msi_entry(pdev)	first_msi_entry((pdev))
+#endif
+
+#ifndef for_each_msi_entry
+#define for_each_msi_entry(desc, dev)	\
+	list_for_each_entry((desc), dev_to_msi_list((dev)), list)
+#endif
+
+#ifndef for_each_pci_msi_entry
+#define for_each_pci_msi_entry(desc, pdev)	\
+	for_each_msi_entry((desc), (pdev))
+#endif
+
+int pci_irq_vector(struct pci_dev *dev, unsigned int nr)
+{
+	if (dev->msix_enabled) {
+		struct msi_desc *entry;
+		int i = 0;
+
+		for_each_pci_msi_entry(entry, dev) {
+			if (i == nr)
+				return entry->irq;
+			i++;
+		}
+		WARN_ON_ONCE(1);
+		return -EINVAL;
+	}
+
+	if (dev->msi_enabled) {
+		struct msi_desc *entry = first_pci_msi_entry(dev);
+
+		if (WARN_ON_ONCE(nr >= entry->nvec_used))
+			return -EINVAL;
+	}
+	else {
+		if (WARN_ON_ONCE(nr > 0))
+			return -EINVAL;
+	}
+
+	return dev->irq + nr;
+}
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,12,0)
+void pci_free_irq(struct pci_dev *dev, unsigned int nr, void *dev_id)
+{
+	free_irq(pci_irq_vector(dev, nr), dev_id);
+}
+
+int pci_request_irq(struct pci_dev *dev, unsigned int nr, irq_handler_t handler,
+		irq_handler_t thread_fn, void *dev_id, const char *fmt, ...)
+{
+	va_list ap;
+	int ret;
+	char *devname;
+	unsigned long irqflags = IRQF_SHARED;
+
+	if (!handler)
+		irqflags |= IRQF_ONESHOT;
+
+	va_start(ap, fmt);
+	devname = kvasprintf(GFP_KERNEL, fmt, ap);
+	va_end(ap);
+
+	ret = request_threaded_irq(pci_irq_vector(dev, nr), handler, thread_fn,
+				   irqflags, devname, dev_id);
+	if (ret)
+		kfree(devname);
+	return ret;
+}
+#endif
+
+#endif /* PCI_HELPER */
+
+struct firmware_info
+{
+    unsigned int dev_id;
+    const char *fw_image;
+    const char *edl_image;
+};
+
+static const struct firmware_info firmware_table[] = {
+    {.dev_id = 0x306, .fw_image = "sdx55m/sbl1.mbn"},
+    {.dev_id = 0x305, .fw_image = "sdx50m/sbl1.mbn"},
+    {.dev_id = 0x304, .fw_image = "sbl.mbn", .edl_image = "edl.mbn"},
+    /* default, set to debug.mbn */
+    {.fw_image = "debug.mbn"},
+};
+
+static int debug_mode = 0;
+
+#ifdef CONFIG_DEBUG_FS
+int mhi_debugfs_trigger_m0(void *data, u64 val)
+{
+    struct mhi_controller *mhi_cntrl = data;
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+    MHI_LOG("Trigger M3 Exit\n");
+    pm_runtime_get(&mhi_dev->pci_dev->dev);
+    pm_runtime_put(&mhi_dev->pci_dev->dev);
+
+    return 0;
+}
+DEFINE_SIMPLE_ATTRIBUTE(debugfs_trigger_m0_fops, NULL,
+                        mhi_debugfs_trigger_m0, "%llu\n");
+
+int mhi_debugfs_trigger_m3(void *data, u64 val)
+{
+    struct mhi_controller *mhi_cntrl = data;
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+    MHI_LOG("Trigger M3 Entry\n");
+    pm_runtime_mark_last_busy(&mhi_dev->pci_dev->dev);
+    pm_request_autosuspend(&mhi_dev->pci_dev->dev);
+
+    return 0;
+}
+DEFINE_SIMPLE_ATTRIBUTE(debugfs_trigger_m3_fops, NULL,
+                        mhi_debugfs_trigger_m3, "%llu\n");
+#endif
+
+void mhi_deinit_free_irq(struct mhi_controller *mhi_cntrl);
+
+void mhi_disable_irq(struct mhi_controller *mhi_cntrl)
+{
+	int i;
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    struct pci_dev *pci_dev = mhi_dev->pci_dev;
+
+    /* disable the irq first */
+    MHI_LOG("msi_allocated %d\n", mhi_cntrl->msi_allocated);
+    for (i = 0; i < mhi_cntrl->msi_allocated; i++)
+    {
+        MHI_LOG("disable_irq %d\n", pci_irq_vector(pci_dev, i));
+	    disable_irq(pci_irq_vector(pci_dev, i));
+    }
+}
+
+void mhi_deinit_pci_dev(struct mhi_controller *mhi_cntrl)
+{
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    struct pci_dev *pci_dev = mhi_dev->pci_dev;
+
+    pm_runtime_mark_last_busy(&pci_dev->dev);
+    pm_runtime_dont_use_autosuspend(&pci_dev->dev);
+    pm_runtime_disable(&pci_dev->dev);
+
+    /* reset counter for lpm state changes */
+    mhi_dev->lpm_disable_depth = 0;
+
+    MHI_LOG("pci_dev 0x%px\n", pci_dev);
+
+    mhi_deinit_free_irq(mhi_cntrl);
+
+    pci_free_irq_vectors(pci_dev);
+    kfree(mhi_cntrl->irq);
+    mhi_cntrl->irq = NULL;
+    iounmap(mhi_cntrl->regs);
+    mhi_cntrl->regs = NULL;
+    pci_clear_master(pci_dev);
+    pci_release_region(pci_dev, mhi_dev->resn);
+    pci_disable_device(pci_dev);
+}
+
+/**
+ * @brief Initialize PCI device
+ *
+ * @param[in ]            mhi_cntrl				struct mhi controller
+ */
+static int mhi_init_pci_dev(struct mhi_controller *mhi_cntrl)
+{
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    struct pci_dev *pci_dev = mhi_dev->pci_dev;
+    int ret;
+    resource_size_t len;
+    int i;
+
+    mhi_dev->resn = MHI_PCI_BAR_NUM;
+    ret = pci_assign_resource(pci_dev, mhi_dev->resn);
+    if (ret)
+    {
+        MHI_ERR("Error assign pci resources, ret:%d\n", ret);
+        return ret;
+    }
+
+    ret = pci_enable_device(pci_dev);
+    if (ret)
+    {
+        MHI_ERR("Error enabling device, ret:%d\n", ret);
+        goto error_enable_device;
+    }
+
+    ret = pci_request_region(pci_dev, mhi_dev->resn, "mhi");
+    if (ret)
+    {
+        MHI_ERR("Error pci_request_region, ret:%d\n", ret);
+        goto error_request_region;
+    }
+
+    pci_set_master(pci_dev);
+
+    mhi_cntrl->base_addr = pci_resource_start(pci_dev, mhi_dev->resn);
+    len = pci_resource_len(pci_dev, mhi_dev->resn);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0)
+    mhi_cntrl->regs = ioremap(mhi_cntrl->base_addr, len);
+#else
+    mhi_cntrl->regs = ioremap_nocache(mhi_cntrl->base_addr, len);
+#endif
+
+    if (!mhi_cntrl->regs)
+    {
+        MHI_ERR("Error ioremap region\n");
+        goto error_ioremap;
+    }
+
+    mhi_cntrl->msi_required = 8;
+    MHI_LOG("msi_required %d", mhi_cntrl->msi_required);
+	ret = pci_alloc_irq_vectors(pci_dev, 1,
+		mhi_cntrl->msi_required, PCI_IRQ_MSI | PCI_IRQ_MSIX | PCI_IRQ_AFFINITY);
+	if (IS_ERR_VALUE((ulong)ret) || ret < 1)
+    {
+        MHI_ERR("Failed to enable MSI, ret:%d\n", ret);
+        goto error_req_msi;
+    }
+
+    mhi_cntrl->msi_allocated = ret;
+    MHI_LOG("msi_allocated %d", mhi_cntrl->msi_allocated);
+    mhi_cntrl->irq = kmalloc_array(mhi_cntrl->msi_allocated,
+                                   sizeof(*mhi_cntrl->irq), GFP_KERNEL);
+    if (!mhi_cntrl->irq)
+    {
+        ret = -ENOMEM;
+        goto error_alloc_msi_vec;
+    }
+
+    for (i = 0; i < mhi_cntrl->msi_allocated; i++)
+    {
+        mhi_cntrl->irq[i] = pci_irq_vector(pci_dev, i);
+        MHI_LOG("irq %d: vector %d", i, mhi_cntrl->irq[i]);
+        if (mhi_cntrl->irq[i] < 0)
+        {
+            ret = mhi_cntrl->irq[i];
+            goto error_get_irq_vec;
+        }
+    }
+
+    dev_set_drvdata(&pci_dev->dev, mhi_cntrl);
+
+    /* configure runtime pm */
+    pm_runtime_set_autosuspend_delay(&pci_dev->dev, MHI_RPM_SUSPEND_TMR_MS);
+    pm_runtime_use_autosuspend(&pci_dev->dev);
+    pm_suspend_ignore_children(&pci_dev->dev, true);
+
+    /*
+	 * pci framework will increment usage count (twice) before
+	 * calling local device driver probe function.
+	 * 1st pci.c pci_pm_init() calls pm_runtime_forbid
+	 * 2nd pci-driver.c local_pci_probe calls pm_runtime_get_sync
+	 * Framework expect pci device driver to call
+	 * pm_runtime_put_noidle to decrement usage count after
+	 * successful probe and and call pm_runtime_allow to enable
+	 * runtime suspend.
+	 */
+    pm_runtime_mark_last_busy(&pci_dev->dev);
+    pm_runtime_put_noidle(&pci_dev->dev);
+
+    return 0;
+
+error_get_irq_vec:
+    kfree(mhi_cntrl->irq);
+    mhi_cntrl->irq = NULL;
+
+error_alloc_msi_vec:
+    pci_free_irq_vectors(pci_dev);
+
+error_req_msi:
+    iounmap(mhi_cntrl->regs);
+
+error_ioremap:
+    pci_clear_master(pci_dev);
+
+error_request_region:
+    pci_disable_device(pci_dev);
+
+error_enable_device:
+    pci_release_region(pci_dev, mhi_dev->resn);
+
+    return ret;
+}
+
+#ifdef CONFIG_PM
+/**
+ * @brief Runtime suspend
+ *
+ * @param[in ]            dev				struct device
+ */
+static int mhi_runtime_suspend(struct device *dev)
+{
+    int ret = 0;
+    struct mhi_controller *mhi_cntrl = dev_get_drvdata(dev);
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+    MHI_LOG("Enter\n");
+
+    mutex_lock(&mhi_cntrl->pm_mutex);
+
+    if (!mhi_dev->powered_on)
+    {
+        MHI_LOG("Not fully powered, return success\n");
+        mutex_unlock(&mhi_cntrl->pm_mutex);
+        return 0;
+    }
+
+    ret = mhi_pm_suspend(mhi_cntrl);
+
+    if (ret)
+    {
+        MHI_LOG("Abort due to ret:%d\n", ret);
+        mhi_dev->suspend_mode = MHI_ACTIVE_STATE;
+        goto exit_runtime_suspend;
+    }
+
+    mhi_dev->suspend_mode = MHI_DEFAULT_SUSPEND;
+
+    ret = mhi_arch_link_suspend(mhi_cntrl);
+
+    /* failed suspending link abort mhi suspend */
+    if (ret)
+    {
+        MHI_LOG("Failed to suspend link, abort suspend\n");
+        mhi_pm_resume(mhi_cntrl);
+        mhi_dev->suspend_mode = MHI_ACTIVE_STATE;
+    }
+
+exit_runtime_suspend:
+    mutex_unlock(&mhi_cntrl->pm_mutex);
+    MHI_LOG("Exited with ret:%d\n", ret);
+
+    return ret;
+}
+
+/**
+ * @brief Runtime idle
+ *
+ * @param[in ]            dev				struct device
+ */
+static int mhi_runtime_idle(struct device *dev)
+{
+    MHI_LOG("Entered returning -EBUSY\n");
+
+    /*
+	 * RPM framework during runtime resume always calls
+	 * rpm_idle to see if device ready to suspend.
+	 * If dev.power usage_count count is 0, rpm fw will call
+	 * rpm_idle cb to see if device is ready to suspend.
+	 * if cb return 0, or cb not defined the framework will
+	 * assume device driver is ready to suspend;
+	 * therefore, fw will schedule runtime suspend.
+	 * In MHI power management, MHI host shall go to
+	 * runtime suspend only after entering MHI State M2, even if
+	 * usage count is 0.  Return -EBUSY to disable automatic suspend.
+	 */
+    return -EBUSY;
+}
+
+/**
+ * @brief Runtime resume
+ *
+ * @param[in ]            dev				struct device
+ */
+static int mhi_runtime_resume(struct device *dev)
+{
+    int ret = 0;
+    struct mhi_controller *mhi_cntrl = dev_get_drvdata(dev);
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+    MHI_LOG("Enter\n");
+
+    mutex_lock(&mhi_cntrl->pm_mutex);
+
+    if (!mhi_dev->powered_on)
+    {
+        MHI_LOG("Not fully powered, return success\n");
+        mutex_unlock(&mhi_cntrl->pm_mutex);
+        return 0;
+    }
+
+    /* turn on link */
+    ret = mhi_arch_link_resume(mhi_cntrl);
+    if (ret)
+        goto rpm_resume_exit;
+
+    /* transition to M0 state */
+    if (mhi_dev->suspend_mode == MHI_DEFAULT_SUSPEND)
+        ret = mhi_pm_resume(mhi_cntrl);
+    else
+        ret = mhi_pm_fast_resume(mhi_cntrl, MHI_FAST_LINK_ON);
+
+    mhi_dev->suspend_mode = MHI_ACTIVE_STATE;
+
+rpm_resume_exit:
+    mutex_unlock(&mhi_cntrl->pm_mutex);
+    MHI_LOG("Exited with :%d\n", ret);
+
+    return ret;
+}
+
+static int mhi_qcom_power_up(struct mhi_controller *mhi_cntrl);
+
+/**
+ * @brief System resume
+ *
+ * @param[in ]            dev				struct device
+ */
+static int mhi_system_resume(struct device *dev)
+{
+    struct mhi_controller *mhi_cntrl = dev_get_drvdata(dev);
+    int ret;
+
+    MHI_LOG("Entered\n");
+
+    mhi_runtime_resume(dev);
+
+	/* device will regain power */
+	mhi_cntrl->previous_dev_state = MHI_STATE_MAX;
+	mhi_cntrl->previous_ee = MHI_EE_MAX;
+
+	MHI_LOG("mhi_qcom_power_up\n");
+
+    ret = mhi_qcom_power_up(mhi_cntrl);
+
+	return ret;
+
+}
+
+/**
+ * @brief System suspend
+ *
+ * @param[in ]            dev				struct device
+ */
+int mhi_system_suspend(struct device *dev)
+{
+    struct mhi_controller *mhi_cntrl = dev_get_drvdata(dev);
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    int ret;
+
+    MHI_LOG("Entered\n");
+
+    MHI_LOG("close all channels\n");
+    mhi_close_all_channel(mhi_cntrl);
+
+    mutex_lock(&mhi_cntrl->pm_mutex);
+
+    if (!mhi_dev->powered_on)
+    {
+        MHI_LOG("Not fully powered, return success\n");
+        mutex_unlock(&mhi_cntrl->pm_mutex);
+        return 0;
+    }
+
+#if 0
+    /*
+	 * pci framework always makes a dummy vote to rpm
+	 * framework to resume before calling system suspend
+	 * hence usage count is minimum one
+	 */
+    if (atomic_read(&dev->power.usage_count) > 1)
+    {
+        /*
+		 * clients have requested to keep link on, try
+		 * fast suspend. No need to notify clients since
+		 * we will not be turning off the pcie link
+		 */
+        ret = mhi_pm_fast_suspend(mhi_cntrl, false);
+        mhi_dev->suspend_mode = MHI_FAST_LINK_ON;
+    }
+    else
+    {
+#endif        
+        /* try normal suspend */
+        mhi_dev->suspend_mode = MHI_DEFAULT_SUSPEND;
+        ret = mhi_pm_suspend(mhi_cntrl);
+        
+        MHI_LOG("mhi_pm_suspend ret 0x%x\n", ret);
+
+        /*
+		 * normal suspend failed because we're busy, try
+		 * fast suspend before aborting system suspend.
+		 * this could happens if client has disabled
+		 * device lpm but no active vote for PCIe from
+		 * apps processor
+		 */
+        if (ret == -EBUSY)
+        {
+            ret = mhi_pm_fast_suspend(mhi_cntrl, true);
+            mhi_dev->suspend_mode = MHI_FAST_LINK_ON;
+        }
+//  }
+
+    if (ret)
+    {
+        MHI_LOG("Abort due to ret:%d\n", ret);
+        mhi_dev->suspend_mode = MHI_ACTIVE_STATE;
+        goto exit_system_suspend;
+    }
+
+    ret = mhi_arch_link_suspend(mhi_cntrl);
+
+    /* failed suspending link abort mhi suspend */
+    if (ret)
+    {
+        MHI_LOG("Failed to suspend link, abort suspend\n");
+        if (mhi_dev->suspend_mode == MHI_DEFAULT_SUSPEND)
+            mhi_pm_resume(mhi_cntrl);
+        else
+            mhi_pm_fast_resume(mhi_cntrl, MHI_FAST_LINK_OFF);
+
+        mhi_dev->suspend_mode = MHI_ACTIVE_STATE;
+    }
+
+exit_system_suspend:
+    mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	/* device will lost power */
+	MHI_LOG("mhi_power_down graceful\n");
+    mhi_power_down(mhi_cntrl, true);
+
+    MHI_LOG("Exit with ret:%d\n", ret);
+
+    return ret;
+}
+#endif
+
+/**
+ * @brief Force device to suspend
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ */
+static int mhi_force_suspend(struct mhi_controller *mhi_cntrl)
+{
+    int ret = -EIO;
+    const u32 delayms = 100;
+    struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    int itr = DIV_ROUND_UP(mhi_cntrl->timeout_ms, delayms);
+
+    MHI_LOG("Entered\n");
+
+    mutex_lock(&mhi_cntrl->pm_mutex);
+
+    for (; itr; itr--)
+    {
+        /*
+		 * This function get called soon as device entered mission mode
+		 * so most of the channels are still in disabled state. However,
+		 * sbl channels are active and clients could be trying to close
+		 * channels while we trying to suspend the link. So, we need to
+		 * re-try if MHI is busy
+		 */
+        ret = mhi_pm_suspend(mhi_cntrl);
+        if (!ret || ret != -EBUSY)
+            break;
+
+        MHI_LOG("MHI busy, sleeping and retry\n");
+        msleep(delayms);
+    }
+
+    if (ret)
+        goto exit_force_suspend;
+
+    mhi_dev->suspend_mode = MHI_DEFAULT_SUSPEND;
+    ret = mhi_arch_link_suspend(mhi_cntrl);
+
+exit_force_suspend:
+    MHI_LOG("Force suspend ret with %d\n", ret);
+
+    mutex_unlock(&mhi_cntrl->pm_mutex);
+
+    return ret;
+}
+
+/**
+ * @brief checks if link is down
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static int mhi_link_status(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    struct mhi_dev *mhi_dev = priv;
+    u16 dev_id;
+    int ret;
+
+    /* try reading device id, if dev id don't match, link is down */
+    ret = pci_read_config_word(mhi_dev->pci_dev, PCI_DEVICE_ID, &dev_id);
+
+    return (ret || dev_id != mhi_cntrl->dev_id) ? -EIO : 0;
+}
+
+/**
+ * @brief disable PCIe L1
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static int mhi_lpm_disable(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    struct mhi_dev *mhi_dev = priv;
+    struct pci_dev *pci_dev = mhi_dev->pci_dev;
+    int lnkctl = pci_dev->pcie_cap + PCI_EXP_LNKCTL;
+    u8 val;
+    unsigned long flags;
+    int ret = 0;
+
+    spin_lock_irqsave(&mhi_dev->lpm_lock, flags);
+
+    /* L1 is already disabled */
+    if (mhi_dev->lpm_disable_depth)
+    {
+        mhi_dev->lpm_disable_depth++;
+        goto lpm_disable_exit;
+    }
+
+    ret = pci_read_config_byte(pci_dev, lnkctl, &val);
+    if (ret)
+    {
+        MHI_ERR("Error reading LNKCTL, ret:%d\n", ret);
+        goto lpm_disable_exit;
+    }
+
+    /* L1 is not supported, do not increment lpm_disable_depth */
+    if (unlikely(!(val & PCI_EXP_LNKCTL_ASPM_L1)))
+        goto lpm_disable_exit;
+
+    val &= ~PCI_EXP_LNKCTL_ASPM_L1;
+    ret = pci_write_config_byte(pci_dev, lnkctl, val);
+    if (ret)
+    {
+        MHI_ERR("Error writing LNKCTL to disable LPM, ret:%d\n", ret);
+        goto lpm_disable_exit;
+    }
+
+    mhi_dev->lpm_disable_depth++;
+
+lpm_disable_exit:
+    spin_unlock_irqrestore(&mhi_dev->lpm_lock, flags);
+
+    return ret;
+}
+
+/**
+ * @brief enable PCIe L1
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static int mhi_lpm_enable(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    struct mhi_dev *mhi_dev = priv;
+    struct pci_dev *pci_dev = mhi_dev->pci_dev;
+    int lnkctl = pci_dev->pcie_cap + PCI_EXP_LNKCTL;
+    u8 val;
+    unsigned long flags;
+    int ret = 0;
+
+    spin_lock_irqsave(&mhi_dev->lpm_lock, flags);
+
+    /*
+	 * Exit if L1 is not supported or is already disabled or
+	 * decrementing lpm_disable_depth still keeps it above 0
+	 */
+    if (!mhi_dev->lpm_disable_depth)
+        goto lpm_enable_exit;
+
+    if (mhi_dev->lpm_disable_depth > 1)
+    {
+        mhi_dev->lpm_disable_depth--;
+        goto lpm_enable_exit;
+    }
+
+    ret = pci_read_config_byte(pci_dev, lnkctl, &val);
+    if (ret)
+    {
+        MHI_ERR("Error reading LNKCTL, ret:%d\n", ret);
+        goto lpm_enable_exit;
+    }
+
+    val |= PCI_EXP_LNKCTL_ASPM_L1;
+    ret = pci_write_config_byte(pci_dev, lnkctl, val);
+    if (ret)
+    {
+        MHI_ERR("Error writing LNKCTL to enable LPM, ret:%d\n", ret);
+        goto lpm_enable_exit;
+    }
+
+    mhi_dev->lpm_disable_depth = 0;
+
+lpm_enable_exit:
+    spin_unlock_irqrestore(&mhi_dev->lpm_lock, flags);
+
+    return ret;
+}
+
+/**
+ * @brief power up device 
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ */
+static int mhi_qcom_power_up(struct mhi_controller *mhi_cntrl)
+{
+    enum mhi_dev_state dev_state;
+    const u32 delayus = 10;
+    int itr = DIV_ROUND_UP(mhi_cntrl->timeout_ms * 1000, delayus);
+    int ret;
+
+    MHI_LOG("Entered\n");
+
+	dev_state = mhi_get_mhi_state(mhi_cntrl);
+
+    MHI_LOG("dev_state %d\n", dev_state);
+
+    /*
+	 * It's possible device did not go thru a cold reset before
+	 * power up and still in error state. If device in error state,
+	 * we need to trigger a soft reset before continue with power
+	 * up
+	 */
+    if (dev_state == MHI_STATE_SYS_ERR)
+    {
+        mhi_set_mhi_state(mhi_cntrl, MHI_STATE_RESET);
+        while (itr--)
+        {
+            dev_state = mhi_get_mhi_state(mhi_cntrl);
+            if (dev_state != MHI_STATE_SYS_ERR)
+                break;
+            usleep_range(delayus, delayus << 1);
+        }
+        /* device still in error state, abort power up */
+        if (dev_state == MHI_STATE_SYS_ERR)
+            return -EIO;
+    }
+
+    /* when coming out of SSR, initial ee state is not valid */
+    mhi_cntrl->ee = 0;
+
+    /* clear mission_mode_done */
+	mhi_cntrl->mission_mode_done = false;
+
+    ret = mhi_arch_power_up(mhi_cntrl);
+    if (ret)
+        return ret;
+
+    ret = mhi_async_power_up(mhi_cntrl);
+
+    /* power up create the dentry */
+    if (mhi_cntrl->dentry)
+    {
+#ifdef CONFIG_DEBUG_FS
+        debugfs_create_file("m0", 0444, mhi_cntrl->dentry, mhi_cntrl,
+                            &debugfs_trigger_m0_fops);
+        debugfs_create_file("m3", 0444, mhi_cntrl->dentry, mhi_cntrl,
+                            &debugfs_trigger_m3_fops);
+#endif                            
+    }
+
+    return ret;
+}
+
+/**
+ * @brief runtime resume
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static int mhi_runtime_get(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    struct mhi_dev *mhi_dev = priv;
+    struct device *dev = &mhi_dev->pci_dev->dev;
+
+    return pm_runtime_get(dev);
+}
+
+/**
+ * @brief runtime idle
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static void mhi_runtime_put(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    struct mhi_dev *mhi_dev = priv;
+    struct device *dev = &mhi_dev->pci_dev->dev;
+
+    pm_runtime_put_noidle(dev);
+}
+
+/**
+ * @brief status callback
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ * @param[in ]            reason				enum MHI_CB
+ */
+static void mhi_status_cb(struct mhi_controller *mhi_cntrl,
+                          void *priv,
+                          enum MHI_CB reason)
+{
+    struct mhi_dev *mhi_dev = priv;
+    struct device *dev = &mhi_dev->pci_dev->dev;
+    int ret;
+
+    switch (reason)
+    {
+    case MHI_CB_IDLE:
+        MHI_LOG("Schedule runtime suspend\n");
+        pm_runtime_mark_last_busy(dev);
+        pm_request_autosuspend(dev);
+        break;
+    case MHI_CB_BW_REQ:
+        if (mhi_dev->bw_scale)
+            mhi_dev->bw_scale(mhi_cntrl, mhi_dev);
+        break;
+    case MHI_CB_EE_MISSION_MODE:
+        /*
+		 * we need to force a suspend so device can switch to
+		 * mission mode pcie phy settings.
+		 */
+        pm_runtime_get(dev);
+        ret = mhi_force_suspend(mhi_cntrl);
+        if (!ret)
+            mhi_runtime_resume(dev);
+        pm_runtime_put(dev);
+        break;
+    default:
+        MHI_ERR("Unhandled cb:0x%x\n", reason);
+    }
+}
+
+/**
+ * @brief capture host SoC XO time in ticks
+ *
+ * @param[in ]            mhi_cntrl				struct mhi_controller
+ * @param[in ]            priv					struct mhi_dev
+ */
+static u64 mhi_time_get(struct mhi_controller *mhi_cntrl, void *priv)
+{
+    //return arch_counter_get_cntvct();
+    return get_jiffies_64();
+}
+
+/**
+ * @brief print timeout value (ms) to a buffer
+ *
+ * @param[in ]            dev				struct device
+ * @param[in ]            attr				struct device_attribute
+ * @param[in ]            buf				buffer for the timeout string
+ */
+static ssize_t timeout_ms_show(struct device *dev,
+                               struct device_attribute *attr,
+                               char *buf)
+{
+    struct mhi_device *mhi_dev = to_mhi_device(dev);
+    struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+    /* buffer provided by sysfs has a minimum size of PAGE_SIZE */
+    return snprintf(buf, PAGE_SIZE, "%u\n", mhi_cntrl->timeout_ms);
+}
+
+/**
+ * @brief set timeout value (ms)
+ *
+ * @param[in ]            dev				struct device
+ * @param[in ]            attr				struct device_attribute
+ * @param[in ]            buf				buffer for the timeout string
+ * @param[out ]           count				buffer size
+ */
+static ssize_t timeout_ms_store(struct device *dev,
+                                struct device_attribute *attr,
+                                const char *buf,
+                                size_t count)
+{
+    struct mhi_device *mhi_dev = to_mhi_device(dev);
+    struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+    u32 timeout_ms;
+
+    if (kstrtou32(buf, 0, &timeout_ms) < 0)
+        return -EINVAL;
+
+    mhi_cntrl->timeout_ms = timeout_ms;
+
+    return count;
+}
+static DEVICE_ATTR_RW(timeout_ms);
+
+/**
+ * @brief restore power for the device
+ *
+ * @param[in ]            dev				struct device
+ * @param[in ]            attr				struct device_attribute
+ * @param[in ]            buf				buffer
+ * @param[out ]           count				buffer size
+ */
+static ssize_t power_up_store(struct device *dev,
+                              struct device_attribute *attr,
+                              const char *buf,
+                              size_t count)
+{
+    int ret;
+    struct mhi_device *mhi_dev = to_mhi_device(dev);
+    struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+    ret = mhi_qcom_power_up(mhi_cntrl);
+    if (ret)
+        return ret;
+
+    return count;
+}
+static DEVICE_ATTR_WO(power_up);
+
+static struct attribute *mhi_qcom_attrs[] = {
+    &dev_attr_timeout_ms.attr,
+    &dev_attr_power_up.attr,
+    NULL};
+
+static const struct attribute_group mhi_qcom_group = {
+    .attrs = mhi_qcom_attrs,
+};
+
+#define MHI_MEM_BASE_DEFAULT         0x000000000
+#define MHI_MEM_SIZE_DEFAULT         0x2000000000
+
+/**
+ * @brief register MHI controller for the PCIe device
+ *
+ * @param[in ]            pci_dev				struct pci_dev
+ */
+static struct mhi_controller *mhi_register_controller(struct pci_dev *pci_dev)
+{
+    struct mhi_controller *mhi_cntrl;
+    struct mhi_dev *mhi_dev;
+    struct device_node *of_node = pci_dev->dev.of_node;
+    const struct firmware_info *firmware_info;
+    // bool use_bb;
+    u64 addr_win[2];
+    int ret, i;
+
+    //if (!of_node)
+    //	return ERR_PTR(-ENODEV);
+
+    mhi_cntrl = mhi_alloc_controller(sizeof(*mhi_dev));
+    if (!mhi_cntrl)
+        return ERR_PTR(-ENOMEM);
+
+    mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+    mhi_cntrl->domain = pci_domain_nr(pci_dev->bus);
+    mhi_cntrl->dev_id = pci_dev->device;
+    mhi_cntrl->bus = pci_dev->bus->number;
+    mhi_cntrl->slot = PCI_SLOT(pci_dev->devfn);
+
+#if 0
+	ret = of_property_read_u32(of_node, "qcom,smmu-cfg",
+				   &mhi_dev->smmu_cfg);
+	if (ret)
+		goto error_register;
+
+	use_bb = of_property_read_bool(of_node, "mhi,use-bb");
+
+	/*
+	 * if s1 translation enabled or using bounce buffer pull iova addr
+	 * from dt
+	 */
+	if (use_bb || (mhi_dev->smmu_cfg & MHI_SMMU_ATTACH &&
+		       !(mhi_dev->smmu_cfg & MHI_SMMU_S1_BYPASS))) {
+		ret = of_property_count_elems_of_size(of_node, "qcom,addr-win",
+						      sizeof(addr_win));
+		if (ret != 1)
+			goto error_register;
+		ret = of_property_read_u64_array(of_node, "qcom,addr-win",
+						 addr_win, 2);
+		if (ret)
+			goto error_register;
+	} else {
+		addr_win[0] = memblock_start_of_DRAM();
+		addr_win[1] = memblock_end_of_DRAM();
+	}
+#endif
+
+#if 1
+    addr_win[0] = MHI_MEM_BASE_DEFAULT;
+    addr_win[1] = MHI_MEM_SIZE_DEFAULT;
+#else
+    addr_win[0] = memblock_start_of_DRAM();
+    addr_win[1] = memblock_end_of_DRAM();
+#endif
+
+    mhi_dev->iova_start = addr_win[0];
+    mhi_dev->iova_stop = addr_win[1];
+
+    /*
+	 * If S1 is enabled, set MHI_CTRL start address to 0 so we can use low
+	 * level mapping api to map buffers outside of smmu domain
+	 */
+    if (mhi_dev->smmu_cfg & MHI_SMMU_ATTACH &&
+        !(mhi_dev->smmu_cfg & MHI_SMMU_S1_BYPASS))
+        mhi_cntrl->iova_start = 0;
+    else
+        mhi_cntrl->iova_start = addr_win[0];
+
+    mhi_cntrl->iova_stop = mhi_dev->iova_stop;
+    mhi_cntrl->of_node = of_node;
+
+    mhi_dev->pci_dev = pci_dev;
+    spin_lock_init(&mhi_dev->lpm_lock);
+
+    /* setup power management apis */
+    mhi_cntrl->status_cb = mhi_status_cb;
+    mhi_cntrl->runtime_get = mhi_runtime_get;
+    mhi_cntrl->runtime_put = mhi_runtime_put;
+    mhi_cntrl->link_status = mhi_link_status;
+
+    mhi_cntrl->lpm_disable = mhi_lpm_disable;
+    mhi_cntrl->lpm_enable = mhi_lpm_enable;
+    mhi_cntrl->time_get = mhi_time_get;
+    mhi_cntrl->remote_timer_freq = 19200000;
+    mhi_cntrl->local_timer_freq = 19200000;
+
+    ret = of_register_mhi_controller(mhi_cntrl);
+    if (ret)
+        goto error_register;
+
+    for (i = 0; i < ARRAY_SIZE(firmware_table); i++)
+    {
+        firmware_info = firmware_table + i;
+
+        /* debug mode always use default */
+        if (!debug_mode && mhi_cntrl->dev_id == firmware_info->dev_id)
+            break;
+    }
+
+    mhi_cntrl->fw_image = firmware_info->fw_image;
+    mhi_cntrl->edl_image = firmware_info->edl_image;
+
+    ret = sysfs_create_group(&mhi_cntrl->mhi_dev->dev.kobj,
+                             &mhi_qcom_group);
+    if (ret) {
+        goto error_register;
+    }
+    
+	atomic_set(&mhi_cntrl->dev_wake, 0);
+	atomic_set(&mhi_cntrl->alloc_size, 0);
+	atomic_set(&mhi_cntrl->pending_pkts, 0);
+
+	mhi_cntrl->previous_dev_state = MHI_STATE_MAX;
+	mhi_cntrl->previous_ee = MHI_EE_MAX;
+
+    return mhi_cntrl;
+
+error_register:
+    mhi_free_controller(mhi_cntrl);
+
+    return ERR_PTR(-EINVAL);
+}
+
+/**
+ * @brief unregister MHI controller for the PCIe device
+ *
+ * @param[in ]            pci_dev				struct pci_dev
+ */
+static void mhi_unregister_controller(struct pci_dev *pci_dev)
+{
+	struct mhi_controller *mhi_cntrl = pci_get_drvdata(pci_dev);
+
+	mhi_free_controller(mhi_cntrl);
+}
+
+int mhi_deinit(void);
+
+/**
+ * @brief remove PCIe device
+ *
+ * @param[in ]            pci_dev				struct pci_dev
+ */
+static void mhi_pci_remove(struct pci_dev *pci_dev)
+{
+	struct mhi_controller *mhi_cntrl = pci_get_drvdata(pci_dev);
+
+    MHI_DEFAULT("enter\n");
+
+	MHI_LOG("mhi_power_down graceful\n");
+    mhi_power_down(mhi_cntrl, true);
+	
+    MHI_LOG("mhi_deinit_pci_dev\n");
+	mhi_deinit_pci_dev(mhi_cntrl);
+
+	MHI_LOG("mhi_arch_iommu_deinit\n");
+	mhi_arch_iommu_deinit(mhi_cntrl);
+
+	MHI_LOG("mhi_arch_pcie_deinit\n");
+	mhi_arch_pcie_deinit(mhi_cntrl);
+
+	MHI_LOG("mhi_unregister_mhi_controller\n");
+    mhi_unregister_mhi_controller(mhi_cntrl);
+
+    MHI_LOG("buffer size %d\n", atomic_read(&mhi_cntrl->alloc_size));
+
+	MHI_LOG("mhi_unregister_controller\n");
+	mhi_unregister_controller(pci_dev);
+
+}
+
+int mhi_init(void);
+
+/**
+ * @brief probe PCIe device
+ *
+ * @param[in ]            pci_dev				struct pci_dev
+ * @param[in ]            device_id				struct pci_device_id
+ */
+int mhi_pci_probe(struct pci_dev *pci_dev,
+                  const struct pci_device_id *device_id)
+{
+    struct mhi_controller *mhi_cntrl;
+    u32 domain = pci_domain_nr(pci_dev->bus);
+    u32 bus = pci_dev->bus->number;
+    u32 dev_id = pci_dev->device;
+    u32 slot = PCI_SLOT(pci_dev->devfn);
+    struct mhi_dev *mhi_dev;
+    int ret;
+
+    MHI_DEFAULT("enter\n");
+
+    /* see if we already registered */
+    mhi_cntrl = mhi_bdf_to_controller(domain, bus, slot, dev_id);
+    if (!mhi_cntrl) {
+        mhi_cntrl = mhi_register_controller(pci_dev);
+    }
+
+    if (IS_ERR(mhi_cntrl)) {
+        return PTR_ERR(mhi_cntrl);
+    }
+    
+	pci_set_drvdata(pci_dev, mhi_cntrl);
+
+    mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+    mhi_dev->powered_on = true;
+
+    ret = mhi_arch_pcie_init(mhi_cntrl);
+    if (ret)
+        goto error_arch_pcie_init;
+
+    ret = mhi_arch_iommu_init(mhi_cntrl);
+    if (ret)
+        goto error_iommu_init;
+
+    ret = mhi_init_pci_dev(mhi_cntrl);
+    if (ret)
+        goto error_init_pci;
+
+    /* start power up sequence */
+    if (!debug_mode)
+    {
+        ret = mhi_qcom_power_up(mhi_cntrl);
+        if (ret)
+            goto error_power_up;
+    }
+
+    pm_runtime_mark_last_busy(&pci_dev->dev);
+
+    MHI_DEFAULT("Return successful\n");
+
+    return 0;
+
+error_power_up:
+    mhi_deinit_pci_dev(mhi_cntrl);
+
+error_init_pci:
+    mhi_arch_iommu_deinit(mhi_cntrl);
+
+error_iommu_init:
+    mhi_arch_pcie_deinit(mhi_cntrl);
+
+error_arch_pcie_init:
+    mhi_unregister_mhi_controller(mhi_cntrl);
+
+    MHI_DEFAULT("failed!\n");
+
+    return ret;
+}
+
+#ifdef CONFIG_PM
+static const struct dev_pm_ops pm_ops = {
+    SET_RUNTIME_PM_OPS(mhi_runtime_suspend,
+                       mhi_runtime_resume,
+                       mhi_runtime_idle)
+        SET_SYSTEM_SLEEP_PM_OPS(mhi_system_suspend, mhi_system_resume)};
+
+#define MHI_PM_OPS	(&pm_ops)
+#else 
+#define MHI_PM_OPS	NULL
+#endif
+
+static struct pci_device_id mhi_pcie_device_id[] = {
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0300)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0301)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0302)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0303)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0304)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0305)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, 0x0306)},
+    {PCI_DEVICE(MHI_PCIE_VENDOR_ID, MHI_PCIE_DEBUG_ID)},
+    {PCI_DEVICE(MHI_SIERRA_PCIE_VENDOR_ID, 0x0200)},
+    {0},
+};
+MODULE_DEVICE_TABLE(pci, mhi_pcie_device_id);
+
+static struct pci_driver mhi_pcie_driver = {
+    .name = "mhictrl",
+    .id_table = mhi_pcie_device_id,
+    .probe = mhi_pci_probe,
+	.remove = mhi_pci_remove,
+    .driver = {
+        .pm = MHI_PM_OPS}};
+
+/**
+ * @brief Module initialization
+ *
+ */
+static int __init
+mhi_pci_init(void)
+{
+	mhi_init();
+
+	return pci_register_driver(&mhi_pcie_driver);
+}
+
+/**
+ * @brief Module exit
+ *
+ */
+static void __exit
+mhi_pci_exit(void)
+{
+	pci_unregister_driver(&mhi_pcie_driver);
+
+	mhi_deinit();
+}
+
+module_init(mhi_pci_init);
+module_exit(mhi_pci_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("MHI_CORE");
+MODULE_DESCRIPTION("MHI Host Driver");
+MODULE_VERSION("1.10.2103.1");
diff --git a/drivers/staging/em9190/controllers/mhi_qcom.h b/drivers/staging/em9190/controllers/mhi_qcom.h
new file mode 100644
index 000000000000..0cd9cc5c0681
--- /dev/null
+++ b/drivers/staging/em9190/controllers/mhi_qcom.h
@@ -0,0 +1,125 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+#ifndef _MHI_QCOM_
+#define _MHI_QCOM_
+
+/* iova cfg bitmask */
+#define MHI_SMMU_ATTACH BIT(0)
+#define MHI_SMMU_S1_BYPASS BIT(1)
+#define MHI_SMMU_FAST BIT(2)
+#define MHI_SMMU_ATOMIC BIT(3)
+#define MHI_SMMU_FORCE_COHERENT BIT(4)
+
+#define MHI_PCIE_VENDOR_ID (0x17cb)
+#define MHI_PCIE_DEBUG_ID (0xffff)
+
+#define MHI_SIERRA_PCIE_VENDOR_ID (0x18d7)
+
+/* runtime suspend timer */
+#define MHI_RPM_SUSPEND_TMR_MS (250)
+#define MHI_PCI_BAR_NUM (0)
+
+/* timesync time calculations */
+#define REMOTE_TICKS_TO_US(x) (div_u64((x) * 100ULL, \
+			       div_u64(mhi_cntrl->remote_timer_freq, 10000ULL)))
+#define REMOTE_TICKS_TO_SEC(x) (div_u64((x), \
+				mhi_cntrl->remote_timer_freq))
+#define REMOTE_TIME_REMAINDER_US(x) (REMOTE_TICKS_TO_US((x)) % \
+					(REMOTE_TICKS_TO_SEC((x)) * 1000000ULL))
+
+extern const char * const mhi_ee_str[MHI_EE_MAX];
+#define TO_MHI_EXEC_STR(ee) (ee >= MHI_EE_MAX ? "INVALID_EE" : mhi_ee_str[ee])
+
+enum mhi_suspend_mode {
+	MHI_ACTIVE_STATE,
+	MHI_DEFAULT_SUSPEND,
+	MHI_FAST_LINK_OFF,
+	MHI_FAST_LINK_ON,
+};
+
+#define MHI_IS_SUSPENDED(mode) (mode)
+
+struct mhi_dev {
+	struct pci_dev *pci_dev;
+	bool drv_supported;
+	u32 smmu_cfg;
+	int resn;
+	void *arch_info;
+	bool powered_on;
+	dma_addr_t iova_start;
+	dma_addr_t iova_stop;
+	enum mhi_suspend_mode suspend_mode;
+
+	/* if set, soc support dynamic bw scaling */
+	void (*bw_scale)(struct mhi_controller *mhi_cntrl,
+			 struct mhi_dev *mhi_dev);
+	unsigned int lpm_disable_depth;
+	/* lock to toggle low power modes */
+	spinlock_t lpm_lock;
+};
+
+void mhi_deinit_pci_dev(struct mhi_controller *mhi_cntrl);
+int mhi_pci_probe(struct pci_dev *pci_dev,
+		  const struct pci_device_id *device_id);
+
+#if 0 // def CONFIG_ARCH_QCOM
+
+int mhi_arch_power_up(struct mhi_controller *mhi_cntrl);
+int mhi_arch_pcie_init(struct mhi_controller *mhi_cntrl);
+void mhi_arch_pcie_deinit(struct mhi_controller *mhi_cntrl);
+int mhi_arch_iommu_init(struct mhi_controller *mhi_cntrl);
+void mhi_arch_iommu_deinit(struct mhi_controller *mhi_cntrl);
+int mhi_arch_link_suspend(struct mhi_controller *mhi_cntrl);
+int mhi_arch_link_resume(struct mhi_controller *mhi_cntrl);
+
+#else
+
+static inline int mhi_arch_iommu_init(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_dev *mhi_dev = mhi_controller_get_devdata(mhi_cntrl);
+
+	mhi_cntrl->dev = &mhi_dev->pci_dev->dev;
+
+	return dma_set_mask_and_coherent(mhi_cntrl->dev, DMA_BIT_MASK(64));
+}
+
+static inline void mhi_arch_iommu_deinit(struct mhi_controller *mhi_cntrl)
+{
+}
+
+static inline int mhi_arch_pcie_init(struct mhi_controller *mhi_cntrl)
+{
+	return 0;
+}
+
+static inline void mhi_arch_pcie_deinit(struct mhi_controller *mhi_cntrl)
+{
+}
+
+static inline int mhi_arch_link_suspend(struct mhi_controller *mhi_cntrl)
+{
+	return 0;
+}
+
+static inline int mhi_arch_link_resume(struct mhi_controller *mhi_cntrl)
+{
+	return 0;
+}
+
+static inline int mhi_arch_power_up(struct mhi_controller *mhi_cntrl)
+{
+	return 0;
+}
+
+#endif
+
+#endif /* _MHI_QCOM_ */
diff --git a/drivers/staging/em9190/core/Makefile b/drivers/staging/em9190/core/Makefile
new file mode 100644
index 000000000000..aae8e36dfb4c
--- /dev/null
+++ b/drivers/staging/em9190/core/Makefile
@@ -0,0 +1,25 @@
+# Comment/uncomment the following line to disable/enable debugging
+#DEBUG = y
+
+# Add your debugging flag (or not) to CFLAGS
+ifeq ($(DEBUG),y)
+  DEBFLAGS = -O2 -g -DMHINET_DEBUG # "-O" is needed to expand inlines
+else
+  DEBFLAGS = -O2
+endif
+
+ccflags-$(DEBUG):=$(DEBFLAGS)
+
+obj-m	:= mhicore.o
+mhicore-objs := mhi_init.o mhi_main.o mhi_pm.o mhi_boot.o mhi_dtr.o
+
+KERNELDIR ?= /lib/modules/$(shell uname -r)/build
+PWD       := $(shell pwd)
+
+all:
+	$(MAKE) -C $(KERNELDIR) M=$(PWD)
+
+clean:
+	rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions *.o.ur-safe *.symvers *.order .cache.mk
+
+
diff --git a/drivers/staging/em9190/core/mhi_boot.c b/drivers/staging/em9190/core/mhi_boot.c
new file mode 100644
index 000000000000..bd0183919541
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_boot.c
@@ -0,0 +1,608 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/firmware.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include "../inc/mhi.h"
+#include "mhi_internal.h"
+
+ /**
+  * @brief setup rddm vector table for rddm transfer and program rxvec
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            img_info				struct image_info
+  */
+void mhi_rddm_prepare(struct mhi_controller *mhi_cntrl,
+			     struct image_info *img_info)
+{
+	struct mhi_buf *mhi_buf = img_info->mhi_buf;
+	struct bhi_vec_entry *bhi_vec = img_info->bhi_vec;
+	void __iomem *base = mhi_cntrl->bhie;
+	u32 sequence_id;
+	int i = 0;
+
+	for (i = 0; i < img_info->entries - 1; i++, mhi_buf++, bhi_vec++) {
+		MHI_VERB("Setting vector:%pad size:%zu\n",
+			 &mhi_buf->dma_addr, mhi_buf->len);
+		bhi_vec->dma_addr = mhi_buf->dma_addr;
+		bhi_vec->size = mhi_buf->len;
+	}
+
+	MHI_LOG("BHIe programming for RDDM\n");
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_RXVECADDR_HIGH_OFFS,
+		      upper_32_bits(mhi_buf->dma_addr));
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_RXVECADDR_LOW_OFFS,
+		      lower_32_bits(mhi_buf->dma_addr));
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_RXVECSIZE_OFFS, mhi_buf->len);
+	sequence_id = prandom_u32() & BHIE_RXVECSTATUS_SEQNUM_BMSK;
+
+	if (unlikely(!sequence_id))
+		sequence_id = 1;
+
+	mhi_write_reg_field(mhi_cntrl, base, BHIE_RXVECDB_OFFS,
+			    BHIE_RXVECDB_SEQNUM_BMSK, BHIE_RXVECDB_SEQNUM_SHFT,
+			    sequence_id);
+
+	MHI_LOG("address:%pad len:0x%lx sequence:%u\n",
+		&mhi_buf->dma_addr, (unsigned long)mhi_buf->len, sequence_id);
+}
+
+/**
+  * @brief collect rddm during kernel panic
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  */
+static int __mhi_download_rddm_in_panic(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+	u32 rx_status;
+	enum mhi_ee ee;
+	const u32 delayus = 2000;
+	u32 retry = (mhi_cntrl->timeout_ms * 1000) / delayus;
+	const u32 rddm_timeout_us = 200000;
+	int rddm_retry = rddm_timeout_us / delayus; /* time to enter rddm */
+	void __iomem *base = mhi_cntrl->bhie;
+
+	MHI_LOG("Entered with pm_state:%s dev_state:%s ee:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	/*
+	 * This should only be executing during a kernel panic, we expect all
+	 * other cores to shutdown while we're collecting rddm buffer. After
+	 * returning from this function, we expect device to reset.
+	 *
+	 * Normaly, we would read/write pm_state only after grabbing
+	 * pm_lock, since we're in a panic, skipping it. Also there is no
+	 * gurantee this state change would take effect since
+	 * we're setting it w/o grabbing pmlock, it's best effort
+	 */
+	mhi_cntrl->pm_state = MHI_PM_LD_ERR_FATAL_DETECT;
+	/* update should take the effect immediately */
+	smp_wmb();
+
+	/*
+	 * Make sure device is not already in RDDM.
+	 * In case device asserts and a kernel panic follows, device will
+	 * already be in RDDM. Do not trigger SYS ERR again and proceed with
+	 * waiting for image download completion.
+	 */
+	ee = mhi_get_exec_env(mhi_cntrl);
+	if (ee != MHI_EE_RDDM) {
+
+		MHI_LOG("Trigger device into RDDM mode using SYSERR\n");
+		mhi_set_mhi_state(mhi_cntrl, MHI_STATE_SYS_ERR);
+
+		MHI_LOG("Waiting for device to enter RDDM\n");
+		while (rddm_retry--) {
+			ee = mhi_get_exec_env(mhi_cntrl);
+			if (ee == MHI_EE_RDDM)
+				break;
+
+			udelay(delayus);
+		}
+
+		if (rddm_retry <= 0) {
+			/* Hardware reset; force device to enter rddm */
+			MHI_LOG(
+				"Did not enter RDDM, do a host req. reset\n");
+			mhi_write_reg(mhi_cntrl, mhi_cntrl->regs,
+				      MHI_SOC_RESET_REQ_OFFSET,
+				      MHI_SOC_RESET_REQ);
+			udelay(delayus);
+		}
+
+		ee = mhi_get_exec_env(mhi_cntrl);
+	}
+
+	MHI_LOG("Waiting for image download completion, current EE:%s\n",
+		TO_MHI_EXEC_STR(ee));
+	while (retry--) {
+		ret = mhi_read_reg_field(mhi_cntrl, base, BHIE_RXVECSTATUS_OFFS,
+					 BHIE_RXVECSTATUS_STATUS_BMSK,
+					 BHIE_RXVECSTATUS_STATUS_SHFT,
+					 &rx_status);
+		if (ret)
+			return -EIO;
+
+		if (rx_status == BHIE_RXVECSTATUS_STATUS_XFER_COMPL) {
+			MHI_LOG("RDDM successfully collected\n");
+			return 0;
+		}
+
+		udelay(delayus);
+	}
+
+	ee = mhi_get_exec_env(mhi_cntrl);
+	ret = mhi_read_reg(mhi_cntrl, base, BHIE_RXVECSTATUS_OFFS, &rx_status);
+
+	MHI_ERR("Did not complete RDDM transfer\n");
+	MHI_ERR("Current EE:%s\n", TO_MHI_EXEC_STR(ee));
+	MHI_ERR("RXVEC_STATUS:0x%x, ret:%d\n", rx_status, ret);
+
+	return -EIO;
+}
+
+/**
+  * @brief download ramdump image from device
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            in_panic				true for panic
+  */
+int mhi_download_rddm_img(struct mhi_controller *mhi_cntrl, bool in_panic)
+{
+	void __iomem *base = mhi_cntrl->bhie;
+	u32 rx_status;
+
+	if (in_panic)
+		return __mhi_download_rddm_in_panic(mhi_cntrl);
+
+	MHI_LOG("Waiting for image download completion\n");
+
+	/* waiting for image download completion */
+	wait_event_timeout(mhi_cntrl->state_event,
+			   mhi_read_reg_field(mhi_cntrl, base,
+					      BHIE_RXVECSTATUS_OFFS,
+					      BHIE_RXVECSTATUS_STATUS_BMSK,
+					      BHIE_RXVECSTATUS_STATUS_SHFT,
+					      &rx_status) || rx_status,
+			   msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	return (rx_status == BHIE_RXVECSTATUS_STATUS_XFER_COMPL) ? 0 : -EIO;
+}
+EXPORT_SYMBOL(mhi_download_rddm_img);
+
+/**
+  * @brief load FW AMSS
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            mhi_buf				struct mhi_buf
+  */
+static int mhi_fw_load_amss(struct mhi_controller *mhi_cntrl,
+			    const struct mhi_buf *mhi_buf)
+{
+	void __iomem *base = mhi_cntrl->bhie;
+	rwlock_t *pm_lock = &mhi_cntrl->pm_lock;
+	u32 tx_status;
+
+	read_lock_bh(pm_lock);
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		read_unlock_bh(pm_lock);
+		return -EIO;
+	}
+
+	MHI_LOG("Starting BHIe Programming\n");
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_TXVECADDR_HIGH_OFFS,
+		      upper_32_bits(mhi_buf->dma_addr));
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_TXVECADDR_LOW_OFFS,
+		      lower_32_bits(mhi_buf->dma_addr));
+
+	mhi_write_reg(mhi_cntrl, base, BHIE_TXVECSIZE_OFFS, mhi_buf->len);
+
+	mhi_cntrl->sequence_id = prandom_u32() & BHIE_TXVECSTATUS_SEQNUM_BMSK;
+	mhi_write_reg_field(mhi_cntrl, base, BHIE_TXVECDB_OFFS,
+			    BHIE_TXVECDB_SEQNUM_BMSK, BHIE_TXVECDB_SEQNUM_SHFT,
+			    mhi_cntrl->sequence_id);
+	read_unlock_bh(pm_lock);
+
+	MHI_LOG("Upper:0x%x Lower:0x%x len:%lu sequence:%u\n",
+		upper_32_bits(mhi_buf->dma_addr),
+		lower_32_bits(mhi_buf->dma_addr),
+		(unsigned long)mhi_buf->len, mhi_cntrl->sequence_id);
+	MHI_LOG("Waiting for image transfer completion\n");
+
+	/* waiting for image download completion */
+	wait_event_timeout(mhi_cntrl->state_event,
+			   MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state) ||
+			   mhi_read_reg_field(mhi_cntrl, base,
+					      BHIE_TXVECSTATUS_OFFS,
+					      BHIE_TXVECSTATUS_STATUS_BMSK,
+					      BHIE_TXVECSTATUS_STATUS_SHFT,
+					      &tx_status) || tx_status,
+			   msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	return (tx_status == BHIE_TXVECSTATUS_STATUS_XFER_COMPL) ? 0 : -EIO;
+}
+
+/**
+  * @brief load FW SBL
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            dma_addr				DMA address
+  * @param[in ]            size					size
+  */
+static int mhi_fw_load_sbl(struct mhi_controller *mhi_cntrl,
+			   dma_addr_t dma_addr,
+			   size_t size)
+{
+	u32 tx_status, val;
+	int i, ret;
+	void __iomem *base = mhi_cntrl->bhi;
+	rwlock_t *pm_lock = &mhi_cntrl->pm_lock;
+	struct {
+		char *name;
+		u32 offset;
+	} error_reg[] = {
+		{ "ERROR_CODE", BHI_ERRCODE },
+		{ "ERROR_DBG1", BHI_ERRDBG1 },
+		{ "ERROR_DBG2", BHI_ERRDBG2 },
+		{ "ERROR_DBG3", BHI_ERRDBG3 },
+		{ NULL },
+	};
+
+	MHI_LOG("Starting BHI programming\n");
+
+	/* program start sbl download via  bhi protocol */
+	read_lock_bh(pm_lock);
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		read_unlock_bh(pm_lock);
+		goto invalid_pm_state;
+	}
+
+	mhi_write_reg(mhi_cntrl, base, BHI_STATUS, 0);
+	mhi_write_reg(mhi_cntrl, base, BHI_IMGADDR_HIGH,
+		      upper_32_bits(dma_addr));
+	mhi_write_reg(mhi_cntrl, base, BHI_IMGADDR_LOW,
+		      lower_32_bits(dma_addr));
+	mhi_write_reg(mhi_cntrl, base, BHI_IMGSIZE, size);
+	mhi_cntrl->session_id = prandom_u32() & BHI_TXDB_SEQNUM_BMSK;
+	mhi_write_reg(mhi_cntrl, base, BHI_IMGTXDB, mhi_cntrl->session_id);
+	read_unlock_bh(pm_lock);
+
+	MHI_LOG("Waiting for image transfer completion\n");
+
+	/* waiting for image download completion */
+	wait_event_timeout(mhi_cntrl->state_event,
+			   MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state) ||
+			   mhi_read_reg_field(mhi_cntrl, base, BHI_STATUS,
+					      BHI_STATUS_MASK, BHI_STATUS_SHIFT,
+					      &tx_status) || tx_status,
+			   msecs_to_jiffies(mhi_cntrl->timeout_ms));
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		goto invalid_pm_state;
+
+	if (tx_status == BHI_STATUS_ERROR) {
+		MHI_ERR("Image transfer failed\n");
+		read_lock_bh(pm_lock);
+		if (MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+			for (i = 0; error_reg[i].name; i++) {
+				ret = mhi_read_reg(mhi_cntrl, base,
+						   error_reg[i].offset, &val);
+				if (ret)
+					break;
+				MHI_ERR("reg:%s value:0x%x\n",
+					error_reg[i].name, val);
+			}
+		}
+		read_unlock_bh(pm_lock);
+		goto invalid_pm_state;
+	}
+
+	return (tx_status == BHI_STATUS_SUCCESS) ? 0 : -ETIMEDOUT;
+
+invalid_pm_state:
+
+	return -EIO;
+}
+
+/**
+  * @brief free BHIE table
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            image_info			struct image_info
+  */
+void mhi_free_bhie_table(struct mhi_controller *mhi_cntrl,
+			 struct image_info *image_info)
+{
+	int i;
+	struct mhi_buf *mhi_buf = image_info->mhi_buf;
+
+	for (i = 0; i < image_info->entries; i++, mhi_buf++)
+		mhi_free_coherent(mhi_cntrl, mhi_buf->len, mhi_buf->buf,
+				  mhi_buf->dma_addr);
+
+	kfree(image_info->mhi_buf);
+	kfree(image_info);
+}
+
+/**
+  * @brief allocate BHIE table
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            image_info			struct image_info
+  * @param[in ]            alloc_size			size
+  */
+int mhi_alloc_bhie_table(struct mhi_controller *mhi_cntrl,
+			 struct image_info **image_info,
+			 size_t alloc_size)
+{
+	size_t seg_size = mhi_cntrl->seg_len;
+	/* requier additional entry for vec table */
+	int segments = DIV_ROUND_UP(alloc_size, seg_size) + 1;
+	int i;
+	struct image_info *img_info;
+	struct mhi_buf *mhi_buf;
+
+	MHI_LOG("Allocating bytes:%zu seg_size:%zu total_seg:%u\n",
+		alloc_size, seg_size, segments);
+
+	img_info = kzalloc(sizeof(*img_info), GFP_KERNEL);
+	if (!img_info)
+		return -ENOMEM;
+
+	/* allocate memory for entries */
+	img_info->mhi_buf = kcalloc(segments, sizeof(*img_info->mhi_buf),
+				    GFP_KERNEL);
+	if (!img_info->mhi_buf)
+		goto error_alloc_mhi_buf;
+
+	/* allocate and populate vector table */
+	mhi_buf = img_info->mhi_buf;
+	for (i = 0; i < segments; i++, mhi_buf++) {
+		size_t vec_size = seg_size;
+
+		/* last entry is for vector table */
+		if (i == segments - 1)
+			vec_size = sizeof(struct bhi_vec_entry) * i;
+
+		mhi_buf->len = vec_size;
+		mhi_buf->buf = mhi_alloc_coherent(mhi_cntrl, vec_size,
+					&mhi_buf->dma_addr, GFP_KERNEL);
+		if (!mhi_buf->buf)
+			goto error_alloc_segment;
+
+		MHI_LOG("Entry:%d Address:0x%lx size:%lu\n", i,
+			(unsigned long)mhi_buf->dma_addr, (unsigned long)mhi_buf->len);
+	}
+
+	img_info->bhi_vec = img_info->mhi_buf[segments - 1].buf;
+	img_info->entries = segments;
+	*image_info = img_info;
+
+	MHI_LOG("Successfully allocated bhi vec table\n");
+
+	return 0;
+
+error_alloc_segment:
+	for (--i, --mhi_buf; i >= 0; i--, mhi_buf--)
+		mhi_free_coherent(mhi_cntrl, mhi_buf->len, mhi_buf->buf,
+				  mhi_buf->dma_addr);
+
+error_alloc_mhi_buf:
+	kfree(img_info);
+
+	return -ENOMEM;
+}
+
+/**
+  * @brief copy FW image
+  *
+  * @param[in ]            mhi_cntrl			struct mhi_controller
+  * @param[in ]            firmware				struct firmware
+  * @param[in ]            img_info				struct image_info
+  */
+static void mhi_firmware_copy(struct mhi_controller *mhi_cntrl,
+			      const struct firmware *firmware,
+			      struct image_info *img_info)
+{
+	size_t remainder = firmware->size;
+	size_t to_cpy;
+	const u8 *buf = firmware->data;
+	int i = 0;
+	struct mhi_buf *mhi_buf = img_info->mhi_buf;
+	struct bhi_vec_entry *bhi_vec = img_info->bhi_vec;
+
+	while (remainder) {
+		MHI_ASSERT(i >= img_info->entries, "malformed vector table");
+
+		to_cpy = min(remainder, mhi_buf->len);
+		memcpy(mhi_buf->buf, buf, to_cpy);
+		bhi_vec->dma_addr = mhi_buf->dma_addr;
+		bhi_vec->size = to_cpy;
+
+		MHI_VERB("Setting Vector:0x%llx size: %llu\n",
+			 bhi_vec->dma_addr, bhi_vec->size);
+		buf += to_cpy;
+		remainder -= to_cpy;
+		i++;
+		bhi_vec++;
+		mhi_buf++;
+	}
+}
+
+/**
+  * @brief FW downloading worker
+  *
+  * @param[in ]            work					struct work_struct
+  */
+void mhi_fw_load_worker(struct work_struct *work)
+{
+	int ret;
+	struct mhi_controller *mhi_cntrl;
+	const char *fw_name;
+	const struct firmware *firmware;
+	struct image_info *image_info;
+	void *buf;
+	dma_addr_t dma_addr;
+	size_t size;
+
+	mhi_cntrl = container_of(work, struct mhi_controller, fw_worker);
+
+	MHI_LOG("Waiting for device to enter PBL from EE:%s\n",
+		TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 MHI_IN_PBL(mhi_cntrl->ee) ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("MHI is not in valid state\n");
+		return;
+	}
+
+	MHI_LOG("Device current EE:%s\n", TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	/* if device in pthru, do reset to ready state transition */
+	if (mhi_cntrl->ee == MHI_EE_PTHRU)
+		goto fw_load_ee_pthru;
+
+	fw_name = (mhi_cntrl->ee == MHI_EE_EDL) ?
+		mhi_cntrl->edl_image : mhi_cntrl->fw_image;
+
+	if (!fw_name || (mhi_cntrl->fbc_download && (!mhi_cntrl->sbl_size ||
+						     !mhi_cntrl->seg_len))) {
+		MHI_ERR("No firmware image defined or !sbl_size || !seg_len\n");
+		return;
+	}
+
+	ret = request_firmware(&firmware, fw_name, mhi_cntrl->dev);
+	if (ret) {
+		MHI_ERR("Error loading firmware, ret:%d\n", ret);
+		return;
+	}
+
+	size = (mhi_cntrl->fbc_download) ? mhi_cntrl->sbl_size : firmware->size;
+
+	/* the sbl size provided is maximum size, not necessarily image size */
+	if (size > firmware->size)
+		size = firmware->size;
+
+	buf = mhi_alloc_coherent(mhi_cntrl, size, &dma_addr, GFP_KERNEL);
+	if (!buf) {
+		MHI_ERR("Could not allocate memory for image\n");
+		release_firmware(firmware);
+		return;
+	}
+
+	/* load sbl image */
+	memcpy(buf, firmware->data, size);
+	ret = mhi_fw_load_sbl(mhi_cntrl, dma_addr, size);
+	mhi_free_coherent(mhi_cntrl, size, buf, dma_addr);
+
+	if (!mhi_cntrl->fbc_download || ret || mhi_cntrl->ee == MHI_EE_EDL)
+		release_firmware(firmware);
+
+	/* error or in edl, we're done */
+	if (ret || mhi_cntrl->ee == MHI_EE_EDL)
+		return;
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	mhi_cntrl->dev_state = MHI_STATE_RESET;
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/*
+	 * if we're doing fbc, populate vector tables while
+	 * device transitioning into MHI READY state
+	 */
+	if (mhi_cntrl->fbc_download) {
+		ret = mhi_alloc_bhie_table(mhi_cntrl, &mhi_cntrl->fbc_image,
+					   firmware->size);
+		if (ret) {
+			MHI_ERR("Error alloc size of %zu\n", firmware->size);
+			goto error_alloc_fw_table;
+		}
+
+		MHI_LOG("Copying firmware image into vector table\n");
+
+		/* load the firmware into BHIE vec table */
+		mhi_firmware_copy(mhi_cntrl, firmware, mhi_cntrl->fbc_image);
+	}
+
+fw_load_ee_pthru:
+	/* transitioning into MHI RESET->READY state */
+	ret = mhi_ready_state_transition(mhi_cntrl);
+
+	MHI_LOG("To Reset->Ready PM_STATE:%s MHI_STATE:%s EE:%s, ret:%d\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		TO_MHI_EXEC_STR(mhi_cntrl->ee), ret);
+
+	if (!mhi_cntrl->fbc_download)
+		return;
+
+	if (ret) {
+		MHI_ERR("Did not transition to READY state\n");
+		goto error_read;
+	}
+
+	/* wait for SBL event */
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->ee == MHI_EE_SBL ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("MHI did not enter BHIE\n");
+		goto error_read;
+	}
+
+	/* start full firmware image download */
+	image_info = mhi_cntrl->fbc_image;
+	ret = mhi_fw_load_amss(mhi_cntrl,
+			       /* last entry is vec table */
+			       &image_info->mhi_buf[image_info->entries - 1]);
+
+	MHI_LOG("amss fw_load, ret:%d\n", ret);
+
+	release_firmware(firmware);
+
+	return;
+
+error_read:
+	mhi_free_bhie_table(mhi_cntrl, mhi_cntrl->fbc_image);
+	mhi_cntrl->fbc_image = NULL;
+
+error_alloc_fw_table:
+	release_firmware(firmware);
+}
diff --git a/drivers/staging/em9190/core/mhi_dtr.c b/drivers/staging/em9190/core/mhi_dtr.c
new file mode 100644
index 000000000000..26a65f9525ae
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_dtr.c
@@ -0,0 +1,249 @@
+/* Copyright (c) 2018, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/termios.h>
+#include <linux/wait.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+
+#include "../inc/mhi.h"
+#include "mhi_internal.h"
+
+struct __packed dtr_ctrl_msg {
+	u32 preamble;
+	u32 msg_id;
+	u32 dest_id;
+	u32 size;
+	u32 msg;
+};
+
+#define CTRL_MAGIC (0x4C525443)
+#define CTRL_MSG_DTR BIT(0)
+#define CTRL_MSG_RTS BIT(1)
+#define CTRL_MSG_DCD BIT(0)
+#define CTRL_MSG_DSR BIT(1)
+#define CTRL_MSG_RI BIT(3)
+#define CTRL_HOST_STATE (0x10)
+#define CTRL_DEVICE_STATE (0x11)
+#define CTRL_GET_CHID(dtr) (dtr->dest_id & 0xFF)
+
+static int mhi_dtr_tiocmset(struct mhi_controller *mhi_cntrl,
+			    struct mhi_device *mhi_dev,
+			    u32 tiocm)
+{
+	struct dtr_ctrl_msg *dtr_msg = NULL;
+	struct mhi_chan *dtr_chan = mhi_cntrl->dtr_dev->ul_chan;
+	spinlock_t *res_lock = &mhi_dev->dev.devres_lock;
+	u32 cur_tiocm;
+	int ret = 0;
+
+	cur_tiocm = mhi_dev->tiocm & ~(TIOCM_CD | TIOCM_DSR | TIOCM_RI);
+
+	tiocm &= (TIOCM_DTR | TIOCM_RTS);
+
+	/* state did not changed */
+	if (cur_tiocm == tiocm)
+		return 0;
+
+	mutex_lock(&dtr_chan->mutex);
+
+	dtr_msg = kzalloc(sizeof(*dtr_msg), GFP_KERNEL);
+	if (!dtr_msg) {
+		ret = -ENOMEM;
+		goto tiocm_exit;
+	}
+
+	dtr_msg->preamble = CTRL_MAGIC;
+	dtr_msg->msg_id = CTRL_HOST_STATE;
+	dtr_msg->dest_id = mhi_dev->ul_chan_id;
+	dtr_msg->size = sizeof(u32);
+	if (tiocm & TIOCM_DTR)
+		dtr_msg->msg |= CTRL_MSG_DTR;
+	if (tiocm & TIOCM_RTS)
+		dtr_msg->msg |= CTRL_MSG_RTS;
+
+	reinit_completion(&dtr_chan->completion);
+	ret = mhi_queue_transfer(mhi_cntrl->dtr_dev, DMA_TO_DEVICE, dtr_msg,
+				 sizeof(*dtr_msg), MHI_EOT);
+	if (ret)
+		goto tiocm_exit;
+
+	ret = wait_for_completion_timeout(&dtr_chan->completion,
+				msecs_to_jiffies(mhi_cntrl->timeout_ms));
+	if (!ret) {
+		MHI_ERR("Failed to receive transfer callback\n");
+		ret = -EIO;
+		goto tiocm_exit;
+	}
+
+	ret = 0;
+	spin_lock_irq(res_lock);
+	mhi_dev->tiocm &= ~(TIOCM_DTR | TIOCM_RTS);
+	mhi_dev->tiocm |= tiocm;
+	spin_unlock_irq(res_lock);
+
+tiocm_exit:
+	kfree(dtr_msg);
+	mutex_unlock(&dtr_chan->mutex);
+
+	return ret;
+}
+
+long mhi_ioctl(struct mhi_device *mhi_dev, unsigned int cmd, unsigned long arg)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	int ret;
+
+	/* ioctl not supported by this controller */
+	if (!mhi_cntrl->dtr_dev)
+		return -EIO;
+
+	switch (cmd) {
+	case TIOCMGET:
+		return mhi_dev->tiocm;
+	case TIOCMSET:
+	{
+		u32 tiocm;
+
+		ret = get_user(tiocm, (u32 *)arg);
+		if (ret)
+			return ret;
+
+		return mhi_dtr_tiocmset(mhi_cntrl, mhi_dev, tiocm);
+	}
+	default:
+		break;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(mhi_ioctl);
+
+static void mhi_dtr_dl_xfer_cb(struct mhi_device *mhi_dev,
+			       struct mhi_result *mhi_result)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct dtr_ctrl_msg *dtr_msg = mhi_result->buf_addr;
+	u32 chan;
+	spinlock_t *res_lock;
+
+	if (mhi_result->bytes_xferd != sizeof(*dtr_msg)) {
+		MHI_ERR("Unexpected length %zu received\n",
+			mhi_result->bytes_xferd);
+		return;
+	}
+
+	MHI_VERB("preamble:0x%x msg_id:%u dest_id:%u msg:0x%x\n",
+		 dtr_msg->preamble, dtr_msg->msg_id, dtr_msg->dest_id,
+		 dtr_msg->msg);
+
+	chan = CTRL_GET_CHID(dtr_msg);
+	if (chan >= mhi_cntrl->max_chan)
+		return;
+
+	mhi_dev = mhi_cntrl->mhi_chan[chan].mhi_dev;
+	if (!mhi_dev)
+		return;
+
+	res_lock = &mhi_dev->dev.devres_lock;
+	spin_lock_irq(res_lock);
+	mhi_dev->tiocm &= ~(TIOCM_CD | TIOCM_DSR | TIOCM_RI);
+
+	if (dtr_msg->msg & CTRL_MSG_DCD)
+		mhi_dev->tiocm |= TIOCM_CD;
+
+	if (dtr_msg->msg & CTRL_MSG_DSR)
+		mhi_dev->tiocm |= TIOCM_DSR;
+
+	if (dtr_msg->msg & CTRL_MSG_RI)
+		mhi_dev->tiocm |= TIOCM_RI;
+	spin_unlock_irq(res_lock);
+}
+
+static void mhi_dtr_ul_xfer_cb(struct mhi_device *mhi_dev,
+			       struct mhi_result *mhi_result)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *dtr_chan = mhi_cntrl->dtr_dev->ul_chan;
+
+	MHI_VERB("Received with status:%d\n", mhi_result->transaction_status);
+	if (!mhi_result->transaction_status)
+		complete(&dtr_chan->completion);
+}
+
+static void mhi_dtr_remove(struct mhi_device *mhi_dev)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+	mhi_cntrl->dtr_dev = NULL;
+}
+
+static int mhi_dtr_probe(struct mhi_device *mhi_dev,
+			 const struct mhi_device_id *id)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	int ret;
+
+	MHI_LOG("Enter for DTR control channel\n");
+
+	ret = mhi_prepare_for_transfer(mhi_dev);
+	if (!ret)
+		mhi_cntrl->dtr_dev = mhi_dev;
+
+	MHI_LOG("Exit with ret:%d\n", ret);
+
+	return ret;
+}
+
+static const struct mhi_device_id mhi_dtr_table[] = {
+	{ .chan = "IP_CTRL" },
+	{},
+};
+
+static struct mhi_driver mhi_dtr_driver = {
+	.id_table = mhi_dtr_table,
+	.remove = mhi_dtr_remove,
+	.probe = mhi_dtr_probe,
+	.ul_xfer_cb = mhi_dtr_ul_xfer_cb,
+	.dl_xfer_cb = mhi_dtr_dl_xfer_cb,
+	.driver = {
+		.name = "MHI_DTR",
+		.owner = THIS_MODULE,
+	}
+};
+
+int mhi_dtr_deinit(void)
+{
+    mhi_driver_unregister(&mhi_dtr_driver);
+
+    return 0;
+}
+
+//int __init mhi_dtr_init(void)
+int mhi_dtr_init(void)
+{
+	return mhi_driver_register(&mhi_dtr_driver);
+}
diff --git a/drivers/staging/em9190/core/mhi_init.c b/drivers/staging/em9190/core/mhi_init.c
new file mode 100644
index 000000000000..35a6f97110a5
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_init.c
@@ -0,0 +1,2404 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/pci.h>
+#include <linux/vmalloc.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+#include "../inc/mhi.h"
+#include "mhi_internal.h"
+
+bool debug = false;
+
+module_param(debug, bool, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH);
+MODULE_PARM_DESC(debug,"enable/disable driver logging");
+
+int debug_level = MHI_MSG_LVL_INFO;
+module_param(debug_level, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH);
+MODULE_PARM_DESC(debug_level,"driver logging level");
+
+#define MHI_WRITE_REG_64(_BASE, _OFFSET, _VAL)                                \
+        writeq(_VAL, (volatile void *)((u8 *)_BASE + _OFFSET))
+
+const char * const mhi_ee_str[MHI_EE_MAX] = {
+	[MHI_EE_PBL] = "PBL",
+	[MHI_EE_SBL] = "SBL",
+	[MHI_EE_AMSS] = "AMSS",
+	[MHI_EE_RDDM] = "RDDM",
+	[MHI_EE_WFW] = "WFW",
+	[MHI_EE_PTHRU] = "PASS THRU",
+	[MHI_EE_EDL] = "EDL",
+	[MHI_EE_DISABLE_TRANSITION] = "DISABLE",
+	[MHI_EE_NOT_SUPPORTED] = "NOT SUPPORTED",
+};
+
+const char * const mhi_state_tran_str[MHI_ST_TRANSITION_MAX] = {
+	[MHI_ST_TRANSITION_PBL] = "PBL",
+	[MHI_ST_TRANSITION_READY] = "READY",
+	[MHI_ST_TRANSITION_SBL] = "SBL",
+	[MHI_ST_TRANSITION_MISSION_MODE] = "MISSION MODE",
+};
+
+const char * const mhi_state_str[MHI_STATE_MAX] = {
+	[MHI_STATE_RESET] = "RESET",
+	[MHI_STATE_READY] = "READY",
+	[MHI_STATE_M0] = "M0",
+	[MHI_STATE_M1] = "M1",
+	[MHI_STATE_M2] = "M2",
+	[MHI_STATE_M3] = "M3",
+	[MHI_STATE_M3_FAST] = "M3_FAST",
+	[MHI_STATE_BHI] = "BHI",
+	[MHI_STATE_SYS_ERR] = "SYS_ERR",
+};
+
+static const char * const mhi_pm_state_str[] = {
+	[MHI_PM_BIT_DISABLE] = "DISABLE",
+	[MHI_PM_BIT_POR] = "POR",
+	[MHI_PM_BIT_M0] = "M0",
+	[MHI_PM_BIT_M2] = "M2",
+	[MHI_PM_BIT_M3_ENTER] = "M?->M3",
+	[MHI_PM_BIT_M3] = "M3",
+	[MHI_PM_BIT_M3_EXIT] = "M3->M0",
+	[MHI_PM_BIT_FW_DL_ERR] = "FW DL Error",
+	[MHI_PM_BIT_SYS_ERR_DETECT] = "SYS_ERR Detect",
+	[MHI_PM_BIT_SYS_ERR_PROCESS] = "SYS_ERR Process",
+	[MHI_PM_BIT_SHUTDOWN_PROCESS] = "SHUTDOWN Process",
+	[MHI_PM_BIT_LD_ERR_FATAL_DETECT] = "LD or Error Fatal Detect",
+};
+
+struct mhi_bus mhi_bus;
+
+/**
+ * @brief map power state to a string
+ *
+ * @param[in ]            state					power state
+ */
+const char *to_mhi_pm_state_str(enum MHI_PM_STATE state)
+{
+	int index = find_last_bit((unsigned long *)&state, 32);
+
+	if (index >= ARRAY_SIZE(mhi_pm_state_str))
+		return "Invalid State";
+
+	return mhi_pm_state_str[index];
+}
+
+/**
+ * @brief print bus vote to a buffer
+ *
+ * @param[in ]            dev 					device
+ * @param[in ]            attr					device attribute
+ * @param[in,out]         buf					buffer
+ */
+static ssize_t bus_vote_show(struct device *dev,
+			     struct device_attribute *attr,
+			     char *buf)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&mhi_dev->bus_vote));
+}
+
+/**
+ * @brief save bus vote to a buffer
+ *
+ * @param[in ]            dev 					device
+ * @param[in ]            attr					device attribute
+ * @param[in ]            buf					buffer
+ * @param[in ]            count					buffer size
+ */
+static ssize_t bus_vote_store(struct device *dev,
+			      struct device_attribute *attr,
+			      const char *buf,
+			      size_t count)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	int ret = -EINVAL;
+
+	if (sysfs_streq(buf, "get")) {
+		ret = mhi_device_get_sync(mhi_dev, MHI_VOTE_BUS);
+	} else if (sysfs_streq(buf, "put")) {
+		mhi_device_put(mhi_dev, MHI_VOTE_BUS);
+		ret = 0;
+	}
+
+	return ret ? ret : count;
+}
+static DEVICE_ATTR_RW(bus_vote);
+
+/**
+ * @brief print device vote to a buffer
+ *
+ * @param[in ]            dev 				device
+ * @param[in ]            attr 				device attribute
+ * @param[in,out ]        buf 				buffer
+ */
+static ssize_t device_vote_show(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&mhi_dev->dev_vote));
+}
+
+/**
+ * @brief save device vote
+ *
+ * @param[in ]            dev 				device
+ * @param[in ]            attr 				device attribute
+ * @param[in ]			  buf 				buffer
+ * @param[in ]			  count				buffer size
+ */
+static ssize_t device_vote_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf,
+				 size_t count)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	int ret = -EINVAL;
+
+	if (sysfs_streq(buf, "get")) {
+		ret = mhi_device_get_sync(mhi_dev, MHI_VOTE_DEVICE);
+	} else if (sysfs_streq(buf, "put")) {
+		mhi_device_put(mhi_dev, MHI_VOTE_DEVICE);
+		ret = 0;
+	}
+
+	return ret ? ret : count;
+}
+static DEVICE_ATTR_RW(device_vote);
+
+static struct attribute *mhi_vote_attrs[] = {
+	&dev_attr_bus_vote.attr,
+	&dev_attr_device_vote.attr,
+	NULL,
+};
+
+static const struct attribute_group mhi_vote_group = {
+	.attrs = mhi_vote_attrs,
+};
+
+/**
+ * @brief create vote sysfs
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+int mhi_create_vote_sysfs(struct mhi_controller *mhi_cntrl)
+{
+	return sysfs_create_group(&mhi_cntrl->mhi_dev->dev.kobj,
+				  &mhi_vote_group);
+}
+
+/**
+ * @brief destroy vote sysfs
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+void mhi_destroy_vote_sysfs(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_device *mhi_dev = mhi_cntrl->mhi_dev;
+
+	sysfs_remove_group(&mhi_dev->dev.kobj, &mhi_vote_group);
+
+	/* relinquish any pending votes for device */
+	while (atomic_read(&mhi_dev->dev_vote))
+		mhi_device_put(mhi_dev, MHI_VOTE_DEVICE);
+
+	/* remove pending votes for the bus */
+	while (atomic_read(&mhi_dev->bus_vote))
+		mhi_device_put(mhi_dev, MHI_VOTE_BUS);
+}
+
+#if 0
+/* MHI protocol require transfer ring to be aligned to ring length */
+static int mhi_alloc_aligned_ring(struct mhi_controller *mhi_cntrl,
+				  struct mhi_ring *ring,
+				  u64 len)
+{
+    MHI_LOG("ring 0x%p len %lld", ring, len);
+
+	ring->alloc_size = len + (len - 1);
+    MHI_LOG("alloc_size %ld", ring->alloc_size);
+	ring->pre_aligned = mhi_alloc_coherent(mhi_cntrl, ring->alloc_size,
+					       &ring->dma_handle, GFP_KERNEL);
+	if (!ring->pre_aligned)
+		return -ENOMEM;
+
+    MHI_LOG("pre_aligned 0x%p", ring->pre_aligned);
+
+	ring->iommu_base = (ring->dma_handle + (len - 1)) & ~(len - 1);
+    MHI_LOG("dma_handle 0x%llx iommu_base 0x%llx", ring->dma_handle, ring->iommu_base);
+
+	ring->base = ring->pre_aligned + (ring->iommu_base - ring->dma_handle);
+    MHI_LOG("base 0x%p", ring->base);
+
+	return 0;
+}
+
+void mhi_deinit_free_irq(struct mhi_controller *mhi_cntrl)
+{
+	int i;
+	struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		if (mhi_event->offload_ev)
+			continue;
+
+		free_irq(mhi_cntrl->irq[mhi_event->msi], mhi_event);
+	}
+
+	free_irq(mhi_cntrl->irq[0], mhi_cntrl);
+}
+#endif
+
+void mhi_disable_irq(struct mhi_controller *mhi_cntrl);
+
+/**
+ * @brief uninitialize assigned irq
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+void mhi_deinit_free_irq(struct mhi_controller *mhi_cntrl)
+{
+	int i;
+	struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+
+	if (mhi_cntrl->msi_active) {
+
+		/* disable the irq first */
+		mhi_disable_irq(mhi_cntrl);
+
+		/* free irq before vector */
+		if (mhi_cntrl->msi_allocated > 1) {
+			MHI_LOG("free_irq %d\n", mhi_cntrl->irq[0]);
+			free_irq(mhi_cntrl->irq[0], mhi_cntrl);
+
+			for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++)
+			{
+				if (mhi_event->offload_ev)
+					continue;
+
+				MHI_LOG("free_irq %d\n", mhi_cntrl->irq[mhi_event->msi]);
+				free_irq(mhi_cntrl->irq[mhi_event->msi], mhi_event);
+			}
+		}
+		else if (mhi_cntrl->msi_allocated == 1) {
+			MHI_LOG("free_irq %d\n", mhi_cntrl->irq[0]);
+			free_irq(mhi_cntrl->irq[0], mhi_cntrl);
+		}
+
+		mhi_cntrl->msi_active = false;
+	}
+}
+
+/**
+ * @brief initialize assigned irq
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+int mhi_init_irq_setup(struct mhi_controller *mhi_cntrl)
+{
+	int i = 0;
+	int ret;
+	struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+
+	MHI_LOG("request_threaded_irq: allocated %d\n", mhi_cntrl->msi_allocated);
+
+	if (mhi_cntrl->msi_allocated > 1) {
+
+		/* for BHI INTVEC msi */
+		ret = request_threaded_irq(mhi_cntrl->irq[0], mhi_intvec_handlr,
+			mhi_intvec_threaded_handlr,
+			IRQF_ONESHOT | IRQF_NO_SUSPEND,
+			"mhi", mhi_cntrl);
+		if (ret)
+			return ret;
+
+		for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+			if (mhi_event->offload_ev)
+				continue;
+
+			MHI_LOG("request_irq: irq %d vector %d\n", mhi_event->msi, mhi_cntrl->irq[mhi_event->msi]);
+
+			ret = request_irq(mhi_cntrl->irq[mhi_event->msi],
+				mhi_msi_handlr, IRQF_SHARED | IRQF_NO_SUSPEND,
+				"mhi", mhi_event);
+			if (ret) {
+				MHI_ERR("Error requesting irq:%d for ev:%d\n",
+					mhi_cntrl->irq[mhi_event->msi], i);
+				goto error_request;
+			}
+		}
+	}
+	else {
+		MHI_LOG("shared interrupt!\n");
+
+		ret = request_irq(mhi_cntrl->irq[0],
+			mhi_msi_handlr_all, IRQF_SHARED,  //| IRQF_NO_SUSPEND,
+			"mhi", mhi_cntrl);
+		if (ret) {
+			MHI_ERR("Error requesting irq:%d for ev:%d\n",
+				mhi_cntrl->irq[0], 0);
+			goto error_request;
+		}
+	}
+
+	mhi_cntrl->msi_active = true;
+
+	MHI_LOG("request_threaded_irq success");
+
+	return 0;
+
+error_request:
+	for (--i, --mhi_event; i >= 0; i--, mhi_event--) {
+		if (mhi_event->offload_ev)
+			continue;
+
+		free_irq(mhi_cntrl->irq[mhi_event->msi], mhi_event);
+	}
+	free_irq(mhi_cntrl->irq[0], mhi_cntrl);
+
+	return ret;
+}
+
+/**
+ * @brief uninitialize device context
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+void mhi_deinit_dev_ctxt(struct mhi_controller *mhi_cntrl)
+{
+    mhi_deinit_ctrl_seq(mhi_cntrl);
+}
+
+#ifdef CONFIG_DEBUG_FS
+
+/**
+ * @brief initialize debugfs for mhi states
+ *
+ * @param[in ]            inode 				inode
+ * @param[in ]            fp 					file
+ */
+static int mhi_init_debugfs_mhi_states_open(struct inode *inode,
+					    struct file *fp)
+{
+	return single_open(fp, mhi_debugfs_mhi_states_show, inode->i_private);
+}
+
+/**
+ * @brief initialize debugfs for mhi event
+ *
+ * @param[in ]            inode 				inode
+ * @param[in ]            fp 					file
+ */
+static int mhi_init_debugfs_mhi_event_open(struct inode *inode, struct file *fp)
+{
+	return single_open(fp, mhi_debugfs_mhi_event_show, inode->i_private);
+}
+
+/**
+ * @brief initialize debugfs for mhi channel
+ *
+ * @param[in ]            inode 				inode
+ * @param[in ]            fp 					file
+ */
+static int mhi_init_debugfs_mhi_chan_open(struct inode *inode, struct file *fp)
+{
+	return single_open(fp, mhi_debugfs_mhi_chan_show, inode->i_private);
+}
+
+static const struct file_operations debugfs_state_ops = {
+	.open = mhi_init_debugfs_mhi_states_open,
+	.release = single_release,
+	.read = seq_read,
+};
+
+static const struct file_operations debugfs_ev_ops = {
+	.open = mhi_init_debugfs_mhi_event_open,
+	.release = single_release,
+	.read = seq_read,
+};
+
+static const struct file_operations debugfs_chan_ops = {
+	.open = mhi_init_debugfs_mhi_chan_open,
+	.release = single_release,
+	.read = seq_read,
+};
+
+DEFINE_SIMPLE_ATTRIBUTE(debugfs_trigger_reset_fops, NULL,
+			mhi_debugfs_trigger_reset, "%llu\n");
+
+/**
+ * @brief initialize debugfs
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+void mhi_init_debugfs(struct mhi_controller *mhi_cntrl)
+{
+	struct dentry *dentry;
+	char node[32];
+
+	if (!mhi_cntrl->parent)
+		return;
+
+	snprintf(node, sizeof(node), "%04x_%02u:%02u.%02u",
+		 mhi_cntrl->dev_id, mhi_cntrl->domain, mhi_cntrl->bus,
+		 mhi_cntrl->slot);
+
+	dentry = debugfs_create_dir(node, mhi_cntrl->parent);
+	if (IS_ERR_OR_NULL(dentry))
+		return;
+
+	debugfs_create_file("states", 0444, dentry, mhi_cntrl,
+			    &debugfs_state_ops);
+	debugfs_create_file("events", 0444, dentry, mhi_cntrl,
+			    &debugfs_ev_ops);
+	debugfs_create_file("chan", 0444, dentry, mhi_cntrl, &debugfs_chan_ops);
+	debugfs_create_file("reset", 0444, dentry, mhi_cntrl,
+			    &debugfs_trigger_reset_fops);
+	mhi_cntrl->dentry = dentry;
+}
+
+/**
+ * @brief uninitialize debugfs
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+void mhi_deinit_debugfs(struct mhi_controller *mhi_cntrl)
+{
+	if (mhi_cntrl->dentry) {
+		debugfs_remove_recursive(mhi_cntrl->dentry);
+		mhi_cntrl->dentry = NULL;
+	}
+}
+#else 
+void mhi_init_debugfs(struct mhi_controller *mhi_cntrl)
+{}
+
+void mhi_deinit_debugfs(struct mhi_controller *mhi_cntrl)
+{}
+#endif /* CONFIG_DEBUG_FS */
+
+/**
+ * @brief init device context
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_init_dev_ctxt(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_ctxt *mhi_ctxt;
+	struct mhi_chan_ctxt *chan_ctxt;
+	struct mhi_event_ctxt *er_ctxt;
+	struct mhi_cmd_ctxt *cmd_ctxt;
+	struct mhi_chan *mhi_chan;
+	struct mhi_event *mhi_event;
+	struct mhi_cmd *mhi_cmd;
+	int i;
+
+	//atomic_set(&mhi_cntrl->dev_wake, 0);
+	//atomic_set(&mhi_cntrl->alloc_size, 0);
+	//atomic_set(&mhi_cntrl->pending_pkts, 0);
+
+	mhi_ctxt = kzalloc(sizeof(*mhi_ctxt), GFP_KERNEL);
+	if (!mhi_ctxt)
+		return -ENOMEM;
+
+	/* setup channel ctxt */
+#if 0    
+	mhi_ctxt->chan_ctxt = mhi_alloc_coherent(mhi_cntrl,
+			sizeof(*mhi_ctxt->chan_ctxt) * mhi_cntrl->max_chan,
+			&mhi_ctxt->chan_ctxt_addr, GFP_KERNEL);
+	if (!mhi_ctxt->chan_ctxt)
+		goto error_alloc_chan_ctxt;
+#endif
+    mhi_ctxt->chan_ctxt_addr = ((void *)mhi_cntrl->ctrl_seg->xfer_ring - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+
+	MHI_LOG("chan_ctxt_addr 0xx%llx", (unsigned long long)mhi_ctxt->chan_ctxt_addr);
+
+    mhi_ctxt->chan_ctxt = mhi_cntrl->ctrl_seg->xfer_ring;
+
+	mhi_chan = mhi_cntrl->mhi_chan;
+	chan_ctxt = mhi_ctxt->chan_ctxt;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, chan_ctxt++, mhi_chan++) {
+		/* If it's offload channel skip this step */
+		if (mhi_chan->offload_ch)
+			continue;
+
+		chan_ctxt->chstate = MHI_CH_STATE_DISABLED;
+		chan_ctxt->brstmode = mhi_chan->db_cfg.brstmode;
+		chan_ctxt->pollcfg = mhi_chan->db_cfg.pollcfg;
+		chan_ctxt->chtype = mhi_chan->type;
+		chan_ctxt->erindex = mhi_chan->er_index;
+
+        MHI_LOG("i %d erindex %d", i, mhi_chan->er_index);
+
+		mhi_chan->ch_state = MHI_CH_STATE_DISABLED;
+		mhi_chan->tre_ring.db_addr = &chan_ctxt->wp;
+	}
+
+	/* setup event context */
+#if 0
+	mhi_ctxt->er_ctxt = mhi_alloc_coherent(mhi_cntrl,
+			sizeof(*mhi_ctxt->er_ctxt) * mhi_cntrl->total_ev_rings,
+			&mhi_ctxt->er_ctxt_addr, GFP_KERNEL);
+#endif
+    mhi_ctxt->er_ctxt_addr = ((void *)mhi_cntrl->ctrl_seg->evt_ring - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+
+	MHI_LOG("er_ctxt_addr 0xx%llx", (unsigned long long)mhi_ctxt->er_ctxt_addr);
+
+    mhi_ctxt->er_ctxt = mhi_cntrl->ctrl_seg->evt_ring;
+
+//	if (!mhi_ctxt->er_ctxt)
+//		goto error_alloc_er_ctxt;
+
+	er_ctxt = mhi_ctxt->er_ctxt;
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, er_ctxt++,
+		     mhi_event++) {
+		struct mhi_ring *ring = &mhi_event->ring;
+
+		/* it's a satellite ev, we do not touch it */
+		if (mhi_event->offload_ev)
+			continue;
+
+		er_ctxt->intmodc = 0;
+		er_ctxt->intmodt = mhi_event->intmod;
+		er_ctxt->ertype = MHI_ER_TYPE_VALID;
+
+	    MHI_LOG("msi_allocated %d", mhi_cntrl->msi_allocated);
+        if (mhi_cntrl->msi_allocated == 1) 
+            mhi_event->msi = 0;     // all msi should set to 0
+
+		er_ctxt->msivec = mhi_event->msi;
+		mhi_event->db_cfg.db_mode = true;
+
+		ring->el_size = sizeof(struct mhi_tre);
+		ring->len = ring->el_size * ring->elements;
+		MHI_LOG("ev_ring %d elements %ld", i, (long)ring->elements);
+
+        MHI_LOG("ring 0x%p len %ld", ring, (long)ring->len);
+
+	    ring->alloc_size = ring->len;
+        MHI_LOG("alloc_size %ld", (long)ring->alloc_size);
+	    
+        ring->pre_aligned = &mhi_cntrl->ctrl_seg->evt_ring_element[i][0];
+
+        MHI_LOG("pre_aligned 0x%p", ring->pre_aligned);
+
+	    ring->iommu_base = (ring->pre_aligned - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+        MHI_LOG("dma_handle 0x%llx iommu_base 0x%llx", (unsigned long long)ring->dma_handle, (unsigned long long)ring->iommu_base);
+
+	    ring->base = ring->pre_aligned;
+        MHI_LOG("base 0x%p", ring->base);
+
+		MHI_LOG("ring %d base 0x%p", i, ring->base);
+
+		ring->rp = ring->wp = ring->base;
+		er_ctxt->rbase = ring->iommu_base;
+		er_ctxt->rp = er_ctxt->wp = er_ctxt->rbase;
+		er_ctxt->rlen = ring->len;
+		ring->ctxt_wp = &er_ctxt->wp;
+
+		MHI_LOG("ring %d ctxt_wp 0x%p", i, ring->ctxt_wp);
+	}
+
+	/* setup cmd context */
+#if 0    
+	mhi_ctxt->cmd_ctxt = mhi_alloc_coherent(mhi_cntrl,
+				sizeof(*mhi_ctxt->cmd_ctxt) * NR_OF_CMD_RINGS,
+				&mhi_ctxt->cmd_ctxt_addr, GFP_KERNEL);
+	if (!mhi_ctxt->cmd_ctxt)
+		goto error_alloc_er;
+#endif
+
+    mhi_ctxt->cmd_ctxt_addr = ((void *)mhi_cntrl->ctrl_seg->cmd_ring - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+
+	MHI_LOG("cmd_ctxt_addr 0xx%llx", (unsigned long long)mhi_ctxt->cmd_ctxt_addr);
+
+    mhi_ctxt->cmd_ctxt = &mhi_cntrl->ctrl_seg->cmd_ring[0];
+
+	mhi_cmd = mhi_cntrl->mhi_cmd;
+	cmd_ctxt = mhi_ctxt->cmd_ctxt;
+	for (i = 0; i < NR_OF_CMD_RINGS; i++, mhi_cmd++, cmd_ctxt++) {
+		struct mhi_ring *ring = &mhi_cmd->ring;
+
+		ring->el_size = sizeof(struct mhi_tre);
+		ring->elements = CMD_EL_PER_RING;
+		ring->len = ring->el_size * ring->elements;
+
+        MHI_LOG("ring 0x%p len %ld", ring, (long)ring->len);
+
+	    ring->alloc_size = ring->len;
+        MHI_LOG("alloc_size %ld", (long)ring->alloc_size);
+
+        ring->pre_aligned = &mhi_cntrl->ctrl_seg->cmd_ring_element[0];
+
+        MHI_LOG("pre_aligned 0x%p", ring->pre_aligned);
+
+	    ring->iommu_base = (ring->pre_aligned - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+        MHI_LOG("dma_handle 0x%llx iommu_base 0x%llx", (unsigned long long)ring->dma_handle, (unsigned long long)ring->iommu_base);
+
+	    ring->base = ring->pre_aligned;
+        MHI_LOG("base 0x%p", ring->base);
+
+		ring->rp = ring->wp = ring->base;
+		cmd_ctxt->rbase = ring->iommu_base;
+		cmd_ctxt->rp = cmd_ctxt->wp = cmd_ctxt->rbase;
+		cmd_ctxt->rlen = ring->len;
+		ring->ctxt_wp = &cmd_ctxt->wp;
+	}
+
+	mhi_cntrl->mhi_ctxt = mhi_ctxt;
+
+	return 0;
+}
+
+/**
+ * @brief get time sync event configuration
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+static int mhi_get_tsync_er_cfg(struct mhi_controller *mhi_cntrl)
+{
+	int i;
+	struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+
+	/* find event ring with timesync support */
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++)
+		if (mhi_event->data_type == MHI_ER_TSYNC_ELEMENT_TYPE)
+			return mhi_event->er_index;
+
+	return -ENOENT;
+}
+
+/**
+ * @brief initialize time sync
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_init_timesync(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_timesync *mhi_tsync;
+	u32 time_offset, db_offset;
+	int ret;
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		ret = -EIO;
+		goto exit_timesync;
+	}
+
+	ret = mhi_get_capability_offset(mhi_cntrl, TIMESYNC_CAP_ID,
+					&time_offset);
+	if (ret) {
+		MHI_LOG("No timesync capability found\n");
+		goto exit_timesync;
+	}
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	if (!mhi_cntrl->time_get || !mhi_cntrl->lpm_disable ||
+	     !mhi_cntrl->lpm_enable)
+		return -EINVAL;
+
+	/* register method supported */
+	mhi_tsync = kzalloc(sizeof(*mhi_tsync), GFP_KERNEL);
+	if (!mhi_tsync)
+		return -ENOMEM;
+
+	spin_lock_init(&mhi_tsync->lock);
+	mutex_init(&mhi_tsync->lpm_mutex);
+	INIT_LIST_HEAD(&mhi_tsync->head);
+	init_completion(&mhi_tsync->completion);
+
+	/* save time_offset for obtaining time */
+	MHI_LOG("TIME OFFS:0x%x\n", time_offset);
+	mhi_tsync->time_reg = mhi_cntrl->regs + time_offset
+			      + TIMESYNC_TIME_LOW_OFFSET;
+
+	mhi_cntrl->mhi_tsync = mhi_tsync;
+
+	ret = mhi_create_timesync_sysfs(mhi_cntrl);
+	if (unlikely(ret)) {
+		/* kernel method still work */
+		MHI_ERR("Failed to create timesync sysfs nodes\n");
+	}
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		ret = -EIO;
+		goto exit_timesync;
+	}
+
+	/* get DB offset if supported, else return */
+	ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->regs,
+			   time_offset + TIMESYNC_DB_OFFSET, &db_offset);
+	if (ret || !db_offset) {
+		ret = 0;
+		goto exit_timesync;
+	}
+
+	MHI_LOG("TIMESYNC_DB OFFS:0x%x\n", db_offset);
+	mhi_tsync->db = mhi_cntrl->regs + db_offset;
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	/* get time-sync event ring configuration */
+	ret = mhi_get_tsync_er_cfg(mhi_cntrl);
+	if (ret < 0) {
+		MHI_LOG("Could not find timesync event ring\n");
+		return ret;
+	}
+
+	mhi_tsync->er_index = ret;
+
+	ret = mhi_send_cmd(mhi_cntrl, NULL, MHI_CMD_TIMSYNC_CFG);
+	if (ret) {
+		MHI_ERR("Failed to send time sync cfg cmd\n");
+		return ret;
+	}
+
+	ret = wait_for_completion_timeout(&mhi_tsync->completion,
+			msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || mhi_tsync->ccs != MHI_EV_CC_SUCCESS) {
+		MHI_ERR("Failed to get time cfg cmd completion\n");
+		return -EIO;
+	}
+
+	return 0;
+
+exit_timesync:
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	return ret;
+}
+
+/**
+ * @brief initialize mhi mmio
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_init_mmio(struct mhi_controller *mhi_cntrl)
+{
+	u32 val;
+	int i, ret;
+	struct mhi_chan *mhi_chan;
+	struct mhi_event *mhi_event;
+	void __iomem *base = mhi_cntrl->regs;
+
+	MHI_LOG("Initializing MMIO\n");
+
+    MHI_LOG("base 0x%px\n", base);
+
+	ret = mhi_read_reg(mhi_cntrl, base, MHICFG, &val);
+	if (ret)
+		return -EIO;
+
+	MHI_LOG("MHICFG:0x%x\n", val);
+
+	/* NER */
+	val = (val & ~MHICFG_NER_MASK) | (mhi_cntrl->total_ev_rings << MHICFG_NER_SHIFT);
+
+	/* NHWER */
+	val = (val & ~MHICFG_NHWER_MASK) | (NUM_MHI_HW_EVT_RINGS << MHICFG_NHWER_SHIFT);
+
+	mhi_write_reg(mhi_cntrl, base, MHICFG, val);
+
+	MHI_LOG("MHICFG write:0x%x\n", val);
+
+	/* set up DB register for all the chan rings */
+	ret = mhi_read_reg_field(mhi_cntrl, base, CHDBOFF, CHDBOFF_CHDBOFF_MASK,
+				 CHDBOFF_CHDBOFF_SHIFT, &val);
+	if (ret)
+		return -EIO;
+
+	MHI_LOG("CHDBOFF:0x%x\n", val);
+
+	/* setup wake db */
+	mhi_cntrl->wake_db = base + val + (8 * MHI_DEV_WAKE_DB);
+	mhi_write_reg(mhi_cntrl, mhi_cntrl->wake_db, 4, 0);
+	mhi_write_reg(mhi_cntrl, mhi_cntrl->wake_db, 0, 0);
+	mhi_cntrl->wake_set = false;
+
+	/* setup channel db addresses */
+	mhi_chan = mhi_cntrl->mhi_chan;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, val += 8, mhi_chan++) {
+		mhi_chan->tre_ring.db_addr = base + val;
+	    
+        MHI_LOG("chan %d ChanDb 0x%px\n", i, mhi_chan->tre_ring.db_addr);
+    }
+
+	/* setup event ring db addresses */
+	ret = mhi_read_reg_field(mhi_cntrl, base, ERDBOFF, ERDBOFF_ERDBOFF_MASK,
+				 ERDBOFF_ERDBOFF_SHIFT, &val);
+	if (ret)
+		return -EIO;
+
+	MHI_LOG("ERDBOFF:0x%x\n", val);
+
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, val += 8, mhi_event++) {
+		if (mhi_event->offload_ev)
+			continue;
+
+		mhi_event->ring.db_addr = base + val;
+        MHI_LOG("evt %d EvtDb 0x%px\n", i, mhi_event->ring.db_addr);
+	}
+
+	/* set up DB register for primary CMD rings */
+	mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING].ring.db_addr = base + CRDB_LOWER;
+
+	MHI_LOG("CmdDb 0x%px\n",
+           mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING].ring.db_addr);
+
+	MHI_LOG("Programming all MMIO values.\n");
+
+	MHI_WRITE_REG_64(base, CCABAP, mhi_cntrl->mhi_ctxt->chan_ctxt_addr);
+	MHI_WRITE_REG_64(base, ECABAP, mhi_cntrl->mhi_ctxt->er_ctxt_addr);
+	MHI_WRITE_REG_64(base, CRCBAP, mhi_cntrl->mhi_ctxt->cmd_ctxt_addr);
+	MHI_WRITE_REG_64(base, MHICTRLBASE, mhi_cntrl->mem_props.phys_aligned);
+	MHI_WRITE_REG_64(base, MHICTRLLIMIT, mhi_cntrl->mem_props.phys_aligned + mhi_cntrl->mem_props.size);
+	MHI_WRITE_REG_64(base, MHIDATABASE, mhi_cntrl->iova_start);
+	MHI_WRITE_REG_64(base, MHIDATALIMIT, mhi_cntrl->iova_stop);
+
+	return 0;
+}
+
+/**
+ * @brief uninitialize mhi channel context
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            mhi_chan 					mhi channel
+ */
+void mhi_deinit_chan_ctxt(struct mhi_controller *mhi_cntrl,
+			  struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring;
+	struct mhi_ring *tre_ring;
+	struct mhi_chan_ctxt *chan_ctxt;
+
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+	chan_ctxt = &mhi_cntrl->mhi_ctxt->chan_ctxt[mhi_chan->chan];
+
+	//MHI_LOG("mhi_free_coherent\n");
+	//mhi_free_coherent(mhi_cntrl, tre_ring->alloc_size,
+	//		  tre_ring->pre_aligned, tre_ring->dma_handle);
+	MHI_LOG("vfree: buf_ring->base 0x%px\n", buf_ring->base);
+	vfree(buf_ring->base);
+
+	buf_ring->base = tre_ring->base = NULL;
+	chan_ctxt->rbase = 0;
+}
+
+/**
+ * @brief initialize mhi channel context
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            mhi_chan 					mhi channel
+ */
+int mhi_init_chan_ctxt(struct mhi_controller *mhi_cntrl,
+		       struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring;
+	struct mhi_ring *tre_ring;
+	struct mhi_chan_ctxt *chan_ctxt;
+	//int ret;
+    
+    MHI_LOG("mhi_cntrl 0x%p mhi_chan 0x%p chan %d\n", mhi_cntrl, mhi_chan, mhi_chan->chan);
+
+    buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+	tre_ring->el_size = sizeof(struct mhi_tre);
+	tre_ring->len = tre_ring->el_size * tre_ring->elements;
+	chan_ctxt = &mhi_cntrl->mhi_ctxt->chan_ctxt[mhi_chan->chan];
+
+#if 0
+	ret = mhi_alloc_aligned_ring(mhi_cntrl, tre_ring, tre_ring->len);
+	if (ret)
+		return -ENOMEM;
+#endif
+
+    MHI_LOG("tre_ring 0x%p len %ld", tre_ring, (long)tre_ring->len);
+
+	tre_ring->alloc_size = tre_ring->len;
+    MHI_LOG("alloc_size %ld", (long)tre_ring->alloc_size);
+	    
+    tre_ring->pre_aligned = &mhi_cntrl->ctrl_seg->xfer_ring_element[mhi_chan->tre_index][0];
+
+    MHI_LOG("pre_aligned 0x%p", tre_ring->pre_aligned);
+
+	tre_ring->iommu_base = (tre_ring->pre_aligned - (void *)mhi_cntrl->ctrl_seg) + mhi_cntrl->mem_props.phys_aligned;
+    MHI_LOG("dma_handle 0x%llx iommu_base 0x%llx", (unsigned long long)tre_ring->dma_handle, (unsigned long long)tre_ring->iommu_base);
+
+	tre_ring->base = tre_ring->pre_aligned;
+    MHI_LOG("base 0x%p", tre_ring->base);
+
+	buf_ring->el_size = sizeof(struct mhi_buf_info);
+	buf_ring->len = buf_ring->el_size * buf_ring->elements;
+	buf_ring->base = vzalloc(buf_ring->len);
+
+	if (!buf_ring->base) {
+//		mhi_free_coherent(mhi_cntrl, tre_ring->alloc_size,
+//				  tre_ring->pre_aligned, tre_ring->dma_handle);
+		return -ENOMEM;
+	}
+
+	chan_ctxt->chstate = MHI_CH_STATE_ENABLED;
+	chan_ctxt->rbase = tre_ring->iommu_base;
+	chan_ctxt->rp = chan_ctxt->wp = chan_ctxt->rbase;
+	chan_ctxt->rlen = tre_ring->len;
+	tre_ring->ctxt_wp = &chan_ctxt->wp;
+
+	tre_ring->rp = tre_ring->wp = tre_ring->base;
+	buf_ring->rp = buf_ring->wp = buf_ring->base;
+	mhi_chan->db_cfg.db_mode = 1;
+
+	/* update to all cores */
+	smp_wmb();
+
+	return 0;
+}
+
+int mhi_device_configure(struct mhi_device *mhi_dev,
+			 enum dma_data_direction dir,
+			 struct mhi_buf *cfg_tbl,
+			 int elements)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan;
+	struct mhi_event_ctxt *er_ctxt;
+	struct mhi_chan_ctxt *ch_ctxt;
+	int er_index, chan;
+
+	switch (dir) {
+	case DMA_TO_DEVICE:
+		mhi_chan = mhi_dev->ul_chan;
+		break;
+	case DMA_BIDIRECTIONAL:
+	case DMA_FROM_DEVICE:
+	case DMA_NONE:
+		mhi_chan = mhi_dev->dl_chan;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	er_index = mhi_chan->er_index;
+	chan = mhi_chan->chan;
+
+	for (; elements > 0; elements--, cfg_tbl++) {
+		/* update event context array */
+		if (!strcmp(cfg_tbl->name, "ECA")) {
+			er_ctxt = &mhi_cntrl->mhi_ctxt->er_ctxt[er_index];
+			if (sizeof(*er_ctxt) != cfg_tbl->len) {
+				MHI_ERR(
+					"Invalid ECA size, expected:%zu actual%zu\n",
+					sizeof(*er_ctxt), cfg_tbl->len);
+				return -EINVAL;
+			}
+			memcpy((void *)er_ctxt, cfg_tbl->buf, sizeof(*er_ctxt));
+			continue;
+		}
+
+		/* update channel context array */
+		if (!strcmp(cfg_tbl->name, "CCA")) {
+			ch_ctxt = &mhi_cntrl->mhi_ctxt->chan_ctxt[chan];
+			if (cfg_tbl->len != sizeof(*ch_ctxt)) {
+				MHI_ERR(
+					"Invalid CCA size, expected:%zu actual:%zu\n",
+					sizeof(*ch_ctxt), cfg_tbl->len);
+				return -EINVAL;
+			}
+			memcpy((void *)ch_ctxt, cfg_tbl->buf, sizeof(*ch_ctxt));
+			continue;
+		}
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#if 0
+static int of_parse_ev_cfg(struct mhi_controller *mhi_cntrl,
+			   struct device_node *of_node)
+{
+	int i, ret, num = 0;
+	struct mhi_event *mhi_event;
+	struct device_node *child;
+
+	of_node = of_find_node_by_name(of_node, "mhi_events");
+	if (!of_node)
+		return -EINVAL;
+
+	for_each_available_child_of_node(of_node, child) {
+		if (!strcmp(child->name, "mhi_event"))
+			num++;
+	}
+
+	if (!num)
+		return -EINVAL;
+
+	mhi_cntrl->total_ev_rings = num;
+	mhi_cntrl->mhi_event = kcalloc(num, sizeof(*mhi_cntrl->mhi_event),
+				       GFP_KERNEL);
+	if (!mhi_cntrl->mhi_event)
+		return -ENOMEM;
+
+	/* populate ev ring */
+	mhi_event = mhi_cntrl->mhi_event;
+	i = 0;
+	for_each_available_child_of_node(of_node, child) {
+		if (strcmp(child->name, "mhi_event"))
+			continue;
+
+		mhi_event->er_index = i++;
+		ret = of_property_read_u32(child, "mhi,num-elements",
+					   (u32 *)&mhi_event->ring.elements);
+		if (ret)
+			goto error_ev_cfg;
+
+		ret = of_property_read_u32(child, "mhi,intmod",
+					   &mhi_event->intmod);
+		if (ret)
+			goto error_ev_cfg;
+
+		ret = of_property_read_u32(child, "mhi,msi",
+					   &mhi_event->msi);
+		if (ret)
+			goto error_ev_cfg;
+
+		ret = of_property_read_u32(child, "mhi,chan",
+					   &mhi_event->chan);
+		if (!ret) {
+			if (mhi_event->chan >= mhi_cntrl->max_chan)
+				goto error_ev_cfg;
+			/* this event ring has a dedicated channel */
+			mhi_event->mhi_chan =
+				&mhi_cntrl->mhi_chan[mhi_event->chan];
+		}
+
+		ret = of_property_read_u32(child, "mhi,priority",
+					   &mhi_event->priority);
+		if (ret)
+			goto error_ev_cfg;
+
+		ret = of_property_read_u32(child, "mhi,brstmode",
+					   &mhi_event->db_cfg.brstmode);
+		if (ret || MHI_INVALID_BRSTMODE(mhi_event->db_cfg.brstmode))
+			goto error_ev_cfg;
+
+		mhi_event->db_cfg.process_db =
+			(mhi_event->db_cfg.brstmode == MHI_BRSTMODE_ENABLE) ?
+			mhi_db_brstmode : mhi_db_brstmode_disable;
+
+		ret = of_property_read_u32(child, "mhi,data-type",
+					   &mhi_event->data_type);
+		if (ret)
+			mhi_event->data_type = MHI_ER_DATA_ELEMENT_TYPE;
+
+		if (mhi_event->data_type > MHI_ER_DATA_TYPE_MAX)
+			goto error_ev_cfg;
+
+		switch (mhi_event->data_type) {
+		case MHI_ER_DATA_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_data_event_ring;
+			break;
+		case MHI_ER_CTRL_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_ctrl_ev_ring;
+			break;
+		case MHI_ER_TSYNC_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_tsync_event_ring;
+			break;
+		}
+
+		mhi_event->hw_ring = of_property_read_bool(child, "mhi,hw-ev");
+		if (mhi_event->hw_ring)
+			mhi_cntrl->hw_ev_rings++;
+		else
+			mhi_cntrl->sw_ev_rings++;
+		mhi_event->cl_manage = of_property_read_bool(child,
+							"mhi,client-manage");
+		mhi_event->offload_ev = of_property_read_bool(child,
+							      "mhi,offload");
+		mhi_event++;
+	}
+
+	/* we need msi for each event ring + additional one for BHI */
+	mhi_cntrl->msi_required = mhi_cntrl->total_ev_rings + 1;
+
+	return 0;
+
+error_ev_cfg:
+
+	kfree(mhi_cntrl->mhi_event);
+	return -EINVAL;
+}
+#endif
+
+struct mhi_event_properties {
+    u32 num_elements;   /* number of elements event ring support */
+    u32 intmod;         /* interrupt moderation time in ms */
+    u32 msi;            /* MSI associated with this event ring */
+    u32 chan;           /* dedicated channel number */
+    u32 priority;       /* event ring priority */
+    u32 brstmode;       /* event doorbell mode configuration */
+    u32 data_type;      /* type of data this event ring will process */
+    bool hw_ev;         /* event ring associated with hardware channels */
+    bool client_manage; /* client manages the event ring */
+    bool offload;       /* event ring associated with offload channel */
+};
+
+static struct mhi_event_properties event_config[] = {
+/*    num                        intmod  msi chan    pri brs type    hw_ev   c_m off  */
+    { NUM_MHI_EVT_RING_ELEMENTS,  1,      1,  0,      1,  2,  1,      0,      0,  0},
+    { NUM_MHI_EVT_RING_ELEMENTS,  1,      2,  100,    1,  3,  0,      1,      0,  0},
+    { NUM_MHI_EVT_RING_ELEMENTS,  1,      3,  101,    1,  3,  0,      1,      0,  0},
+//    { 240,  1,      3,  101,    1,  2,  0,      1,      0,  0},
+//    { 240,  1,      4,  0,      1,  2,  1,      0,      0,  0},
+};
+
+/**
+ * @brief get mhi event configuration
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            of_node 					device node
+ */
+static int of_parse_ev_cfg(struct mhi_controller *mhi_cntrl,
+			   struct device_node *of_node)
+{
+	int i, num = 0;
+	struct mhi_event *mhi_event;
+
+    num = sizeof(event_config)/sizeof(struct mhi_event_properties);
+	mhi_cntrl->total_ev_rings = num;
+	mhi_cntrl->mhi_event = kcalloc(num, sizeof(*mhi_cntrl->mhi_event),
+				       GFP_KERNEL);
+	if (!mhi_cntrl->mhi_event)
+		return -ENOMEM;
+
+	/* populate ev ring */
+	mhi_event = mhi_cntrl->mhi_event;
+    for (i = 0; i < num; i++) {
+
+		mhi_event->er_index = i;
+		
+        mhi_event->ring.elements = event_config[i].num_elements;
+        mhi_event->intmod = event_config[i].intmod;
+        mhi_event->msi = event_config[i].msi;
+        
+        mhi_event->chan = event_config[i].chan;
+
+		if (mhi_event->chan >= mhi_cntrl->max_chan)
+			goto error_ev_cfg;
+
+		/* this event ring has a dedicated channel */
+		mhi_event->mhi_chan =
+			&mhi_cntrl->mhi_chan[mhi_event->chan];
+
+        mhi_event->priority = event_config[i].priority;
+        mhi_event->db_cfg.brstmode = event_config[i].brstmode;
+		if (MHI_INVALID_BRSTMODE(mhi_event->db_cfg.brstmode))
+			goto error_ev_cfg;
+
+		mhi_event->db_cfg.process_db =
+			(mhi_event->db_cfg.brstmode == MHI_BRSTMODE_ENABLE) ?
+			mhi_db_brstmode : mhi_db_brstmode_disable;
+
+        mhi_event->data_type = event_config[i].data_type;
+
+		if (mhi_event->data_type > MHI_ER_DATA_TYPE_MAX)
+			goto error_ev_cfg;
+
+		switch (mhi_event->data_type) {
+		case MHI_ER_DATA_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_data_event_ring;
+			break;
+		case MHI_ER_CTRL_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_ctrl_ev_ring;
+			break;
+		case MHI_ER_TSYNC_ELEMENT_TYPE:
+			mhi_event->process_event = mhi_process_tsync_event_ring;
+			break;
+		}
+
+		mhi_event->hw_ring = event_config[i].hw_ev;
+		if (mhi_event->hw_ring)
+			mhi_cntrl->hw_ev_rings++;
+		else
+			mhi_cntrl->sw_ev_rings++;
+
+		mhi_event->cl_manage = event_config[i].client_manage;
+		mhi_event->offload_ev = event_config[i].offload;
+		mhi_event++;
+	}
+
+	/* we need msi for each event ring + additional one for BHI */
+	mhi_cntrl->msi_required = mhi_cntrl->total_ev_rings + 1;
+
+	return 0;
+
+error_ev_cfg:
+
+	kfree(mhi_cntrl->mhi_event);
+	return -EINVAL;
+}
+
+#if 0
+static int of_parse_ch_cfg(struct mhi_controller *mhi_cntrl,
+			   struct device_node *of_node)
+{
+	int ret;
+	struct device_node *child;
+	u32 chan;
+
+	ret = of_property_read_u32(of_node, "mhi,max-channels",
+				   &mhi_cntrl->max_chan);
+	if (ret)
+		return ret;
+
+	of_node = of_find_node_by_name(of_node, "mhi_channels");
+	if (!of_node)
+		return -EINVAL;
+
+	mhi_cntrl->mhi_chan = vzalloc(mhi_cntrl->max_chan *
+				      sizeof(*mhi_cntrl->mhi_chan));
+	if (!mhi_cntrl->mhi_chan)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&mhi_cntrl->lpm_chans);
+
+	/* populate channel configurations */
+	for_each_available_child_of_node(of_node, child) {
+		struct mhi_chan *mhi_chan;
+
+		if (strcmp(child->name, "mhi_chan"))
+			continue;
+
+		ret = of_property_read_u32(child, "reg", &chan);
+		if (ret || chan >= mhi_cntrl->max_chan)
+			goto error_chan_cfg;
+
+		mhi_chan = &mhi_cntrl->mhi_chan[chan];
+
+		ret = of_property_read_string(child, "label",
+					      &mhi_chan->name);
+		if (ret)
+			goto error_chan_cfg;
+
+		mhi_chan->chan = chan;
+
+		ret = of_property_read_u32(child, "mhi,num-elements",
+					   (u32 *)&mhi_chan->tre_ring.elements);
+		if (!ret && !mhi_chan->tre_ring.elements)
+			goto error_chan_cfg;
+
+		/*
+		 * For some channels, local ring len should be bigger than
+		 * transfer ring len due to internal logical channels in device.
+		 * So host can queue much more buffers than transfer ring len.
+		 * Example, RSC channels should have a larger local channel
+		 * than transfer ring length.
+		 */
+		ret = of_property_read_u32(child, "mhi,local-elements",
+					   (u32 *)&mhi_chan->buf_ring.elements);
+		if (ret)
+			mhi_chan->buf_ring.elements =
+				mhi_chan->tre_ring.elements;
+
+		ret = of_property_read_u32(child, "mhi,event-ring",
+					   &mhi_chan->er_index);
+		if (ret)
+			goto error_chan_cfg;
+
+		ret = of_property_read_u32(child, "mhi,chan-dir",
+					   &mhi_chan->dir);
+		if (ret)
+			goto error_chan_cfg;
+
+		/*
+		 * For most channels, chtype is identical to channel directions,
+		 * if not defined, assign ch direction to chtype
+		 */
+		ret = of_property_read_u32(child, "mhi,chan-type",
+					   &mhi_chan->type);
+		if (ret)
+			mhi_chan->type = (enum mhi_ch_type)mhi_chan->dir;
+
+		ret = of_property_read_u32(child, "mhi,ee", &mhi_chan->ee_mask);
+		if (ret)
+			goto error_chan_cfg;
+
+		of_property_read_u32(child, "mhi,pollcfg",
+				     &mhi_chan->db_cfg.pollcfg);
+
+		ret = of_property_read_u32(child, "mhi,data-type",
+					   &mhi_chan->xfer_type);
+		if (ret)
+			goto error_chan_cfg;
+
+		switch (mhi_chan->xfer_type) {
+		case MHI_XFER_BUFFER:
+			mhi_chan->gen_tre = mhi_gen_tre;
+			mhi_chan->queue_xfer = mhi_queue_buf;
+			break;
+		case MHI_XFER_SKB:
+			mhi_chan->queue_xfer = mhi_queue_skb;
+			break;
+		case MHI_XFER_SCLIST:
+			mhi_chan->gen_tre = mhi_gen_tre;
+			mhi_chan->queue_xfer = mhi_queue_sclist;
+			break;
+		case MHI_XFER_NOP:
+			mhi_chan->queue_xfer = mhi_queue_nop;
+			break;
+		case MHI_XFER_DMA:
+		case MHI_XFER_RSC_DMA:
+			mhi_chan->queue_xfer = mhi_queue_dma;
+			break;
+		default:
+			goto error_chan_cfg;
+		}
+
+		mhi_chan->lpm_notify = of_property_read_bool(child,
+							     "mhi,lpm-notify");
+		mhi_chan->offload_ch = of_property_read_bool(child,
+							"mhi,offload-chan");
+		mhi_chan->db_cfg.reset_req = of_property_read_bool(child,
+							"mhi,db-mode-switch");
+		mhi_chan->pre_alloc = of_property_read_bool(child,
+							    "mhi,auto-queue");
+		mhi_chan->auto_start = of_property_read_bool(child,
+							     "mhi,auto-start");
+		mhi_chan->wake_capable = of_property_read_bool(child,
+							"mhi,wake-capable");
+
+		if (mhi_chan->pre_alloc &&
+		    (mhi_chan->dir != DMA_FROM_DEVICE ||
+		     mhi_chan->xfer_type != MHI_XFER_BUFFER))
+			goto error_chan_cfg;
+
+		/* bi-dir and dirctionless channels must be a offload chan */
+		if ((mhi_chan->dir == DMA_BIDIRECTIONAL ||
+		     mhi_of_parse_ch_cfgchan->dir == DMA_NONE) && !mhi_chan->offload_ch)
+			goto error_chan_cfg;
+
+		/* if mhi host allocate the buffers then client cannot queue */
+		if (mhi_chan->pre_alloc)
+			mhi_chan->queue_xfer = mhi_queue_nop;
+
+		if (!mhi_chan->offload_ch) {
+			ret = of_property_read_u32(child, "mhi,doorbell-mode",
+						   &mhi_chan->db_cfg.brstmode);
+			if (ret ||
+			    MHI_INVALID_BRSTMODE(mhi_chan->db_cfg.brstmode))
+				goto error_chan_cfg;
+
+			mhi_chan->db_cfg.process_db =
+				(mhi_chan->db_cfg.brstmode ==
+				 MHI_BRSTMODE_ENABLE) ?
+				mhi_db_brstmode : mhi_db_brstmode_disable;
+		}
+
+		mhi_chan->configured = true;
+
+		if (mhi_chan->lpm_notify)
+			list_add_tail(&mhi_chan->node, &mhi_cntrl->lpm_chans);
+	}
+
+	return 0;
+
+error_chan_cfg:
+	vfree(mhi_cntrl->mhi_chan);
+
+	return -EINVAL;
+}
+#endif
+
+static struct mhi_chan_properties chan_config[] = {
+/*    reg   label       num_el  ev_ring dir type db_mode ee     auto_q  auto_start */
+
+//    { 0,    "LOOPBACK", 32,     1,      2,  0,  2,      0x14}, 
+//    { 1,    "LOOPBACK", 32,     1,      1,  0,  2,      0x14}, 
+    /* only add SAHARA devices for SBL */
+    { 2,    "SAHARA",     32,     0,      1,  0,  2,      0x2},    // MHI_CLIENT_SAHARA_OUT = 4, MHI_EE_SBL
+    { 3,    "SAHARA",     256,     0,      2,  0,  2,      0x2},    // MHI_CLIENT_SAHARA_IN = 5, MHI_EE_SBL
+    { 4,    "DIAG",     32,     0,      1,  0,  2,      0x14},    // MHI_CLIENT_DIAG_OUT = 4,
+    { 5,    "DIAG",     32,     0,      2,  0,  2,      0x14},    // MHI_CLIENT_DIAG_IN = 5, 
+	{ 12,   "MBIM",     128,    0,      1,  0,  2,      0x14},    // MHI_CLIENT_MBIM_OUT = 12,
+	{ 13,   "MBIM",     128,    0,      2,  0,  2,      0x14},    // MHI_CLIENT_MBIM_IN = 13,
+	{ 14,   "QMI0",     32,     0,      1,  0,  2,      0x14},    // MHI_CLIENT_QMI_OUT = 14,
+	{ 15,   "QMI0",     32,     0,      2,  0,  2,      0x14},    // MHI_CLIENT_QMI_IN = 15,
+	{ 32,   "DUN",      32,     0,      1,  0,  2,      0x14},    // MHI_CLIENT_DUN_OUT = 32,
+	{ 33,   "DUN",      32,     0,      2,  0,  2,      0x14},    // MHI_CLIENT_DUN_IN = 33,
+#ifdef ADB_SUPPORT
+	{ 36,   "ADB",      128,     0,      1,  0,  2,      0x14},    // MHI_CLIENT_ADB_OUT = 36,
+	{ 37,   "ADB",      128,     0,      2,  0,  2,      0x14},    // MHI_CLIENT_ADB_IN = 37,
+#endif	
+//    { 20,   "IPCR",     32,     1,      1,  0,  2,      0x14,   false,  true},
+//    { 20,   "IPCR",     32,     1,      2,  0,  2,      0x14,   true,   true},
+    { 100,  "IP_HW0",   512,    1,      1,  6,  3,      0x14,   true,   true},      // MHI_CLIENT_IP_HW_0_OUT = 100,
+    { 101,  "IP_HW0",   512,    2,      2,  4,  3,      0x14,   true,   true},      // MHI_CLIENT_IP_HW_0_IN = 101,
+};
+
+/**
+ * @brief get mhi channel configuration
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            of_node 					device node
+ */
+static int of_parse_ch_cfg(struct mhi_controller *mhi_cntrl,
+			   struct device_node *of_node)
+{
+	u32 chan, i;
+
+    mhi_cntrl->max_chan = 128;
+
+	mhi_cntrl->mhi_chan = vzalloc(mhi_cntrl->max_chan *
+				      sizeof(*mhi_cntrl->mhi_chan));
+	if (!mhi_cntrl->mhi_chan)
+		return -ENOMEM;
+
+	BUILD_BUG_ON(sizeof(chan_config)/sizeof(struct mhi_chan_properties) >= NUM_MHI_XFER_RINGS);
+
+	INIT_LIST_HEAD(&mhi_cntrl->lpm_chans);
+
+	/* populate channel configurations */
+	for (i = 0; i < sizeof(chan_config)/sizeof(struct mhi_chan_properties); i++) {
+        struct mhi_chan *mhi_chan;
+
+        chan = chan_config[i].reg;
+
+		mhi_chan = &mhi_cntrl->mhi_chan[chan];
+
+		mhi_chan->chan = chan;
+
+        mhi_chan->name = (const char *)chan_config[i].label;
+
+        mhi_chan->tre_ring.elements = chan_config[i].num_elements;
+
+		if (!mhi_chan->tre_ring.elements)
+			goto error_chan_cfg;
+
+		/*
+		 * For some channels, local ring len should be bigger than
+		 * transfer ring len due to internal logical channels in device.
+		 * So host can queue much more buffers than transfer ring len.
+		 * Example, RSC channels should have a larger local channel
+		 * than transfer ring length.
+		 */
+		mhi_chan->buf_ring.elements = mhi_chan->tre_ring.elements;
+
+        mhi_chan->er_index = chan_config[i].event_ring;
+
+		mhi_chan->tre_index = i;
+
+        mhi_chan->dir = chan_config[i].chan_dir;
+
+		/*
+		 * For most channels, chtype is identical to channel directions,
+		 * if not defined, assign ch direction to chtype
+		 */
+		mhi_chan->type = chan_config[i].chan_dir;
+
+		mhi_chan->ee_mask = chan_config[i].ee;
+        
+        mhi_chan->xfer_type = chan_config[i].data_type;
+
+		switch (mhi_chan->xfer_type) {
+		case MHI_XFER_BUFFER:
+			mhi_chan->gen_tre = mhi_gen_tre;
+			mhi_chan->queue_xfer = mhi_queue_buf;
+			break;
+		case MHI_XFER_SKB:
+			mhi_chan->queue_xfer = mhi_queue_skb;
+			break;
+		case MHI_XFER_SCLIST:
+			mhi_chan->gen_tre = mhi_gen_tre;
+			mhi_chan->queue_xfer = mhi_queue_sclist;
+			break;
+		case MHI_XFER_NOP:
+			mhi_chan->queue_xfer = mhi_queue_nop;
+			break;
+		case MHI_XFER_DMA:
+		case MHI_XFER_RSC_DMA:
+			mhi_chan->queue_xfer = mhi_queue_dma;
+			break;
+		case MHI_XFER_QMAP:
+			mhi_chan->queue_xfer = mhi_queue_qmap;
+			mhi_chan->queue_full = mhi_qmap_full;
+			break;
+		default:
+
+			goto error_chan_cfg;
+		}
+
+		mhi_chan->lpm_notify = 0;
+		mhi_chan->offload_ch = 0;
+		mhi_chan->db_cfg.reset_req = 0;
+		mhi_chan->pre_alloc = 0;
+		//mhi_chan->auto_start = 0;
+		mhi_chan->wake_capable = 0;
+
+		if (mhi_chan->pre_alloc &&
+		    (mhi_chan->dir != DMA_FROM_DEVICE ||
+		     mhi_chan->xfer_type != MHI_XFER_BUFFER))
+			goto error_chan_cfg;
+
+		/* bi-dir and dirctionless channels must be a offload chan */
+#if 0
+		if ((mhi_chan->dir == DMA_BIDIRECTIONAL ||
+		     mhi_of_parse_ch_cfgchan->dir == DMA_NONE) && !mhi_chan->offload_ch)
+			goto error_chan_cfg;
+#endif
+		/* if mhi host allocate the buffers then client cannot queue */
+		if (mhi_chan->pre_alloc)
+			mhi_chan->queue_xfer = mhi_queue_nop;
+
+		if (!mhi_chan->offload_ch) {
+            mhi_chan->db_cfg.brstmode = chan_config[i].doorbell_mode;
+
+			if (MHI_INVALID_BRSTMODE(mhi_chan->db_cfg.brstmode))
+				goto error_chan_cfg;
+
+			mhi_chan->db_cfg.process_db =
+				(mhi_chan->db_cfg.brstmode ==
+				 MHI_BRSTMODE_ENABLE) ?
+				mhi_db_brstmode : mhi_db_brstmode_disable;
+		}
+
+		mhi_chan->configured = true;
+
+		if (mhi_chan->lpm_notify)
+			list_add_tail(&mhi_chan->node, &mhi_cntrl->lpm_chans);
+	}
+
+	return 0;
+
+error_chan_cfg:
+	vfree(mhi_cntrl->mhi_chan);
+
+	return -EINVAL;
+}
+
+/**
+ * @brief get mhi configuration
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            of_node 					device node
+ */
+static int of_parse_dt(struct mhi_controller *mhi_cntrl,
+		       struct device_node *of_node)
+{
+	int ret;
+	enum mhi_ee i;
+	u32 *ee;
+	u32 bhie_offset = 0;
+
+	/* parse MHI channel configuration */
+	ret = of_parse_ch_cfg(mhi_cntrl, of_node);
+	if (ret)
+		return ret;
+
+	/* parse MHI event configuration */
+	ret = of_parse_ev_cfg(mhi_cntrl, of_node);
+	if (ret)
+		goto error_ev_cfg;
+
+	mhi_cntrl->timeout_ms = MHI_TIMEOUT_MS;
+
+	mhi_cntrl->buffer_len = MHI_MAX_MTU;
+
+	/* by default host allowed to ring DB both M0 and M2 state */
+	mhi_cntrl->db_access = MHI_PM_M0 | MHI_PM_M2;
+
+	/* parse the device ee table */
+	for (i = MHI_EE_PBL, ee = mhi_cntrl->ee_table; i < MHI_EE_MAX;
+	     i++, ee++) {
+		/* setup the default ee before checking for override */
+		*ee = i;
+
+#if 0
+		ret = of_property_match_string(of_node, "mhi,ee-names",
+					       mhi_ee_str[i]);
+		if (ret < 0)
+			continue;
+
+		of_property_read_u32_index(of_node, "mhi,ee", ret, ee);
+#endif        
+	}
+
+	mhi_cntrl->bhie = mhi_cntrl->regs + bhie_offset;
+
+	return 0;
+
+error_ev_cfg:
+	vfree(mhi_cntrl->mhi_chan);
+
+	return ret;
+}
+
+/**
+ * @brief register mhi controller
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int of_register_mhi_controller(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+	int i;
+	struct mhi_event *mhi_event;
+	struct mhi_chan *mhi_chan;
+	struct mhi_cmd *mhi_cmd;
+	struct mhi_device *mhi_dev;
+	u32 soc_info;
+
+	//if (!mhi_cntrl->of_node)
+	//	return -EINVAL;
+
+	if (!mhi_cntrl->runtime_get || !mhi_cntrl->runtime_put)
+		return -EINVAL;
+
+	if (!mhi_cntrl->status_cb || !mhi_cntrl->link_status)
+		return -EINVAL;
+
+	ret = of_parse_dt(mhi_cntrl, mhi_cntrl->of_node);
+	if (ret)
+		return -EINVAL;
+
+	mhi_cntrl->mhi_cmd = kcalloc(NR_OF_CMD_RINGS,
+				     sizeof(*mhi_cntrl->mhi_cmd), GFP_KERNEL);
+	if (!mhi_cntrl->mhi_cmd) {
+		ret = -ENOMEM;
+		goto error_alloc_cmd;
+	}
+
+	INIT_LIST_HEAD(&mhi_cntrl->transition_list);
+	mutex_init(&mhi_cntrl->pm_mutex);
+	rwlock_init(&mhi_cntrl->pm_lock);
+	spin_lock_init(&mhi_cntrl->transition_lock);
+	spin_lock_init(&mhi_cntrl->wlock);
+	INIT_WORK(&mhi_cntrl->st_worker, mhi_pm_st_worker);
+	INIT_WORK(&mhi_cntrl->fw_worker, mhi_fw_load_worker);
+	INIT_WORK(&mhi_cntrl->syserr_worker, mhi_pm_sys_err_worker);
+	INIT_WORK(&mhi_cntrl->intvec_worker, mhi_intvec_worker);
+	tasklet_init(&mhi_cntrl->intvec_task, mhi_intvec_task,
+		(ulong)mhi_cntrl);
+	init_waitqueue_head(&mhi_cntrl->state_event);
+	mutex_init(&mhi_cntrl->dev_mutex);
+
+	mhi_cmd = mhi_cntrl->mhi_cmd;
+	for (i = 0; i < NR_OF_CMD_RINGS; i++, mhi_cmd++)
+		spin_lock_init(&mhi_cmd->lock);
+
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		if (mhi_event->offload_ev)
+			continue;
+
+		mhi_event->mhi_cntrl = mhi_cntrl;
+		spin_lock_init(&mhi_event->lock);
+		if (mhi_event->data_type == MHI_ER_CTRL_ELEMENT_TYPE)
+			tasklet_init(&mhi_event->task, mhi_ctrl_ev_task,
+				     (ulong)mhi_event);
+		else
+			tasklet_init(&mhi_event->task, mhi_ev_task,
+				     (ulong)mhi_event);
+	}
+
+	mhi_chan = mhi_cntrl->mhi_chan;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {
+		mutex_init(&mhi_chan->mutex);
+		init_completion(&mhi_chan->completion);
+		rwlock_init(&mhi_chan->lock);
+        spin_lock_init(&mhi_chan->skb_write_lock);
+        spin_lock_init(&mhi_chan->dma_write_lock);
+        spin_lock_init(&mhi_chan->qmap_write_lock);
+		spin_lock_init(&mhi_chan->tre_lock);
+	}
+    
+    mhi_cntrl->bounce_buf = false;
+
+	if (mhi_cntrl->bounce_buf) {
+		mhi_cntrl->map_single = mhi_map_single_use_bb;
+		mhi_cntrl->unmap_single = mhi_unmap_single_use_bb;
+	} else {
+		mhi_cntrl->map_single = mhi_map_single_no_bb;
+		mhi_cntrl->unmap_single = mhi_unmap_single_no_bb;
+	}
+
+	/* read the device info if possible */
+	if (mhi_cntrl->regs) {
+		ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->regs,
+				   SOC_HW_VERSION_OFFS, &soc_info);
+		if (ret)
+			goto error_alloc_dev;
+
+		mhi_cntrl->family_number =
+			(soc_info & SOC_HW_VERSION_FAM_NUM_BMSK) >>
+			SOC_HW_VERSION_FAM_NUM_SHFT;
+		mhi_cntrl->device_number =
+			(soc_info & SOC_HW_VERSION_DEV_NUM_BMSK) >>
+			SOC_HW_VERSION_DEV_NUM_SHFT;
+		mhi_cntrl->major_version =
+			(soc_info & SOC_HW_VERSION_MAJOR_VER_BMSK) >>
+			SOC_HW_VERSION_MAJOR_VER_SHFT;
+		mhi_cntrl->minor_version =
+			(soc_info & SOC_HW_VERSION_MINOR_VER_BMSK) >>
+			SOC_HW_VERSION_MINOR_VER_SHFT;
+	}
+
+	/* register controller with mhi_bus */
+	mhi_dev = mhi_alloc_device(mhi_cntrl);
+	if (!mhi_dev) {
+		ret = -ENOMEM;
+		goto error_alloc_dev;
+	}
+
+	mhi_dev->dev_type = MHI_CONTROLLER_TYPE;
+	mhi_dev->mhi_cntrl = mhi_cntrl;
+	dev_set_name(&mhi_dev->dev, "%04x_%02u.%02u.%02u", mhi_dev->dev_id,
+		     mhi_dev->domain, mhi_dev->bus, mhi_dev->slot);
+
+	/* init wake source */
+	device_init_wakeup(&mhi_dev->dev, true);
+
+	ret = device_add(&mhi_dev->dev);
+	if (ret)
+		goto error_add_dev;
+
+	mhi_cntrl->mhi_dev = mhi_dev;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+#ifdef CONFIG_DEBUG_FS
+	mhi_cntrl->parent = debugfs_lookup(mhi_bus_type.name, NULL);
+#endif 	
+#endif
+
+	/* adding it to this list only for debug purpose */
+	mutex_lock(&mhi_bus.lock);
+	list_add_tail(&mhi_cntrl->node, &mhi_bus.controller_list);
+	mutex_unlock(&mhi_bus.lock);
+
+	return 0;
+
+error_add_dev:
+	mhi_dealloc_device(mhi_cntrl, mhi_dev);
+
+error_alloc_dev:
+	kfree(mhi_cntrl->mhi_cmd);
+
+error_alloc_cmd:
+	vfree(mhi_cntrl->mhi_chan);
+	kfree(mhi_cntrl->mhi_event);
+
+	return ret;
+};
+EXPORT_SYMBOL(of_register_mhi_controller);
+
+/**
+ * @brief unregister mhi controller
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+void mhi_unregister_mhi_controller(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_device *mhi_dev = mhi_cntrl->mhi_dev;
+
+    if (mhi_cntrl->mhi_cmd) {
+	    MHI_LOG("mhi_cmd\n");
+	    kfree(mhi_cntrl->mhi_cmd);
+        mhi_cntrl->mhi_cmd = NULL;
+    }
+
+    if (mhi_cntrl->mhi_event) {
+	    MHI_LOG("mhi_event\n");
+	    kfree(mhi_cntrl->mhi_event);
+        mhi_cntrl->mhi_event = NULL;
+    }
+
+    if (mhi_cntrl->mhi_chan) {
+	    MHI_LOG("mhi_chan\n");
+	    vfree(mhi_cntrl->mhi_chan);
+        mhi_cntrl->mhi_chan = NULL;
+    }
+
+	MHI_LOG("mhi_tsync 0x%px", mhi_cntrl->mhi_tsync);
+    if (mhi_cntrl->mhi_tsync) {
+	    MHI_LOG("mhi_tsync\n");
+	    kfree(mhi_cntrl->mhi_tsync);
+        mhi_cntrl->mhi_tsync = NULL;
+    }
+	
+	mhi_cntrl->timesync_done = false;
+
+	device_del(&mhi_dev->dev);
+	put_device(&mhi_dev->dev);
+
+	mutex_lock(&mhi_bus.lock);
+	list_del(&mhi_cntrl->node);
+	mutex_unlock(&mhi_bus.lock);
+}
+EXPORT_SYMBOL(mhi_unregister_mhi_controller);
+
+/**
+ * @brief set ptr to control private data
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            priv 						private data
+ */
+static inline void mhi_controller_set_devdata(struct mhi_controller *mhi_cntrl,
+					 void *priv)
+{
+	mhi_cntrl->priv_data = priv;
+}
+
+/**
+ * @brief allocate mhi controller to register
+ *
+ * @param[in ]            size 				buffer size
+ */
+struct mhi_controller *mhi_alloc_controller(size_t size)
+{
+	struct mhi_controller *mhi_cntrl;
+
+	mhi_cntrl = kzalloc(size + sizeof(*mhi_cntrl), GFP_KERNEL);
+
+	if (mhi_cntrl && size)
+		mhi_controller_set_devdata(mhi_cntrl, mhi_cntrl + 1);
+
+	return mhi_cntrl;
+}
+EXPORT_SYMBOL(mhi_alloc_controller);
+
+/**
+ * @brief prepare device power
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+int mhi_prepare_for_power_up(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+	u32 bhie_off;
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+
+    ret = mhi_init_ctrl_seq(mhi_cntrl);
+	if (ret) {
+		MHI_ERR("Error with init ctrl seq\n");
+		goto error_dev_ctxt;
+	}
+
+	ret = mhi_init_dev_ctxt(mhi_cntrl);
+	if (ret) {
+		MHI_ERR("Error with init dev_ctxt\n");
+		goto error_dev_ctxt;
+	}
+
+	/*
+	 * allocate rddm table if specified, this table is for debug purpose
+	 * so we'll ignore erros
+	 */
+	if (mhi_cntrl->rddm_size) {
+		mhi_alloc_bhie_table(mhi_cntrl, &mhi_cntrl->rddm_image,
+				     mhi_cntrl->rddm_size);
+
+		/*
+		 * This controller supports rddm, we need to manually clear
+		 * BHIE RX registers since por values are undefined.
+		 */
+		if (!mhi_cntrl->bhie) {
+			ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->regs, BHIEOFF,
+					   &bhie_off);
+			if (ret) {
+				MHI_ERR("Error getting bhie offset\n");
+				goto bhie_error;
+			}
+
+			mhi_cntrl->bhie = mhi_cntrl->regs + bhie_off;
+		}
+
+		memset_io(mhi_cntrl->bhie + BHIE_RXVECADDR_LOW_OFFS, 0,
+			  BHIE_RXVECSTATUS_OFFS - BHIE_RXVECADDR_LOW_OFFS + 4);
+
+		if (mhi_cntrl->rddm_image)
+			mhi_rddm_prepare(mhi_cntrl, mhi_cntrl->rddm_image);
+	}
+
+	mhi_cntrl->pre_init = true;
+
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	return 0;
+
+bhie_error:
+	if (mhi_cntrl->rddm_image) {
+		mhi_free_bhie_table(mhi_cntrl, mhi_cntrl->rddm_image);
+		mhi_cntrl->rddm_image = NULL;
+	}
+
+error_dev_ctxt:
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_prepare_for_power_up);
+
+/**
+ * @brief device power down cleanup
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+void mhi_unprepare_after_power_down(struct mhi_controller *mhi_cntrl)
+{
+	if (mhi_cntrl->fbc_image) {
+		mhi_free_bhie_table(mhi_cntrl, mhi_cntrl->fbc_image);
+		mhi_cntrl->fbc_image = NULL;
+	}
+
+	if (mhi_cntrl->rddm_image) {
+		mhi_free_bhie_table(mhi_cntrl, mhi_cntrl->rddm_image);
+		mhi_cntrl->rddm_image = NULL;
+	}
+
+	mhi_deinit_dev_ctxt(mhi_cntrl);
+	mhi_cntrl->pre_init = false;
+}
+EXPORT_SYMBOL(mhi_unprepare_after_power_down);
+
+/**
+ * @brief match device to driver
+ *
+ * @param[in ]            dev 					mhi device
+ * @param[in ]            drv 					driver
+ */
+static int mhi_match(struct device *dev, struct device_driver *drv)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	struct mhi_driver *mhi_drv = to_mhi_driver(drv);
+	const struct mhi_device_id *id;
+
+	/* if controller type there is no client driver associated with it */
+	if (mhi_dev->dev_type == MHI_CONTROLLER_TYPE)
+		return 0;
+
+	for (id = mhi_drv->id_table; id->chan[0]; id++)
+		if (!strcmp(mhi_dev->chan_name, id->chan)) {
+			mhi_dev->id = id;
+			return 1;
+		}
+
+	return 0;
+};
+
+/**
+ * @brief release device
+ *
+ * @param[in ]            dev 					mhi device
+ */
+static void mhi_release_device(struct device *dev)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+
+	if (mhi_dev->ul_chan)
+		mhi_dev->ul_chan->mhi_dev = NULL;
+
+	if (mhi_dev->dl_chan)
+		mhi_dev->dl_chan->mhi_dev = NULL;
+
+	kfree(mhi_dev);
+}
+
+struct bus_type mhi_bus_type = {
+	.name = "mhi",
+	.dev_name = "mhi",
+	.match = mhi_match,
+};
+
+/**
+ * @brief probe device
+ *
+ * @param[in ]            dev 					mhi device
+ */
+static int mhi_driver_probe(struct device *dev)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct device_driver *drv = dev->driver;
+	struct mhi_driver *mhi_drv = to_mhi_driver(drv);
+	struct mhi_event *mhi_event;
+	struct mhi_chan *ul_chan = mhi_dev->ul_chan;
+	struct mhi_chan *dl_chan = mhi_dev->dl_chan;
+	bool auto_start = false;
+	int ret;
+
+	/* bring device out of lpm */
+	ret = mhi_device_get_sync(mhi_dev, MHI_VOTE_DEVICE);
+	if (ret)
+		return ret;
+
+	ret = -EINVAL;
+	if (ul_chan) {
+		/* lpm notification require status_cb */
+		if (ul_chan->lpm_notify && !mhi_drv->status_cb)
+			goto exit_probe;
+
+		if (!ul_chan->offload_ch && !mhi_drv->ul_xfer_cb)
+			goto exit_probe;
+
+		ul_chan->xfer_cb = mhi_drv->ul_xfer_cb;
+		mhi_dev->status_cb = mhi_drv->status_cb;
+		auto_start = ul_chan->auto_start;
+	}
+
+	if (dl_chan) {
+		if (dl_chan->lpm_notify && !mhi_drv->status_cb)
+			goto exit_probe;
+
+		if (!dl_chan->offload_ch && !mhi_drv->dl_xfer_cb)
+			goto exit_probe;
+
+		mhi_event = &mhi_cntrl->mhi_event[dl_chan->er_index];
+
+		/*
+		 * if this channal event ring manage by client, then
+		 * status_cb must be defined so we can send the async
+		 * cb whenever there are pending data
+		 */
+		if (mhi_event->cl_manage && !mhi_drv->status_cb)
+			goto exit_probe;
+
+		dl_chan->xfer_cb = mhi_drv->dl_xfer_cb;
+
+		/* ul & dl uses same status cb */
+		mhi_dev->status_cb = mhi_drv->status_cb;
+		auto_start = (auto_start || dl_chan->auto_start);
+	}
+
+	ret = mhi_drv->probe(mhi_dev, mhi_dev->id);
+
+	if (!ret && auto_start)
+		mhi_prepare_for_transfer(mhi_dev);
+
+exit_probe:
+	mhi_device_put(mhi_dev, MHI_VOTE_DEVICE);
+
+	return ret;
+}
+
+/**
+ * @brief remove driver
+ *
+ * @param[in ]            dev 					mhi device
+ */
+static int mhi_driver_remove(struct device *dev)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	struct mhi_driver *mhi_drv = to_mhi_driver(dev->driver);
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan;
+	enum MHI_CH_STATE ch_state[] = {
+		MHI_CH_STATE_DISABLED,
+		MHI_CH_STATE_DISABLED
+	};
+	int dir;
+
+	/* control device has no work to do */
+	if (mhi_dev->dev_type == MHI_CONTROLLER_TYPE)
+		return 0;
+
+	MHI_LOG("Removing device for chan:%s\n", mhi_dev->chan_name);
+
+	/* reset both channels */
+	for (dir = 0; dir < 2; dir++) {
+		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+
+		if (!mhi_chan)
+			continue;
+
+		/* wake all threads waiting for completion */
+		write_lock_irq(&mhi_chan->lock);
+		mhi_chan->ccs = MHI_EV_CC_INVALID;
+		complete_all(&mhi_chan->completion);
+		write_unlock_irq(&mhi_chan->lock);
+
+		/* move channel state to disable, no more processing */
+		mutex_lock(&mhi_chan->mutex);
+		write_lock_irq(&mhi_chan->lock);
+		ch_state[dir] = mhi_chan->ch_state;
+		mhi_chan->ch_state = MHI_CH_STATE_SUSPENDED;
+		write_unlock_irq(&mhi_chan->lock);
+
+		/* reset the channel */
+		if (!mhi_chan->offload_ch)
+			mhi_reset_chan(mhi_cntrl, mhi_chan);
+
+		mutex_unlock(&mhi_chan->mutex);
+	}
+
+	/* destroy the device */
+	mhi_drv->remove(mhi_dev);
+
+	/* de_init channel if it was enabled */
+	for (dir = 0; dir < 2; dir++) {
+		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+
+		if (!mhi_chan)
+			continue;
+
+		mutex_lock(&mhi_chan->mutex);
+
+		if (ch_state[dir] == MHI_CH_STATE_ENABLED &&
+		    !mhi_chan->offload_ch)
+			mhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);
+
+		mhi_chan->ch_state = MHI_CH_STATE_DISABLED;
+
+		mutex_unlock(&mhi_chan->mutex);
+	}
+
+
+	if (mhi_cntrl->tsync_dev == mhi_dev)
+		mhi_cntrl->tsync_dev = NULL;
+
+	/* relinquish any pending votes for device */
+	while (atomic_read(&mhi_dev->dev_vote))
+		mhi_device_put(mhi_dev, MHI_VOTE_DEVICE);
+
+	/* remove pending votes for the bus */
+	while (atomic_read(&mhi_dev->bus_vote))
+		mhi_device_put(mhi_dev, MHI_VOTE_BUS);
+
+	return 0;
+}
+
+/**
+ * @brief register driver
+ *
+ * @param[in ]            mhi_drv 				mhi driver
+ */
+int mhi_driver_register(struct mhi_driver *mhi_drv)
+{
+	struct device_driver *driver = &mhi_drv->driver;
+
+	if (!mhi_drv->probe || !mhi_drv->remove)
+		return -EINVAL;
+
+	driver->bus = &mhi_bus_type;
+
+	if (!driver->bus->p) 
+		return -EINVAL;
+
+	driver->probe = mhi_driver_probe;
+	driver->remove = mhi_driver_remove;
+	return driver_register(driver);
+}
+EXPORT_SYMBOL(mhi_driver_register);
+
+/**
+ * @brief unregister driver
+ *
+ * @param[in ]            mhi_drv 				mhi driver
+ */
+void mhi_driver_unregister(struct mhi_driver *mhi_drv)
+{
+	driver_unregister(&mhi_drv->driver);
+}
+EXPORT_SYMBOL(mhi_driver_unregister);
+
+/**
+ * @brief allocate buffer for device
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+struct mhi_device *mhi_alloc_device(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_device *mhi_dev = kzalloc(sizeof(*mhi_dev), GFP_KERNEL);
+	struct device *dev;
+
+	if (!mhi_dev)
+		return NULL;
+
+	dev = &mhi_dev->dev;
+	device_initialize(dev);
+	dev->bus = &mhi_bus_type;
+	dev->release = mhi_release_device;
+	dev->parent = mhi_cntrl->dev;
+	mhi_dev->mhi_cntrl = mhi_cntrl;
+	mhi_dev->dev_id = mhi_cntrl->dev_id;
+	mhi_dev->domain = mhi_cntrl->domain;
+	mhi_dev->bus = mhi_cntrl->bus;
+	mhi_dev->slot = mhi_cntrl->slot;
+	mhi_dev->mtu = MHI_MAX_MTU;
+	atomic_set(&mhi_dev->dev_vote, 0);
+	atomic_set(&mhi_dev->bus_vote, 0);
+
+	return mhi_dev;
+}
+
+/**
+ * @brief allocate DMA capable buffer
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            alignment 			alignment for the buffer
+ */
+void
+mhi_malloc_contiguous_memory(struct mhi_controller *mhi_cntrl, u64 alignment)
+{
+	struct mhi_mem_props *mem_props = &mhi_cntrl->mem_props;
+	dma_addr_t bam_PA = 0x0LL;
+
+	mem_props->size = sizeof(struct mhi_ctrl_seg);
+	mem_props->virt_unaligned = 0;
+	mem_props->virt_aligned = 0;
+	mem_props->phys_unaligned = 0;
+	mem_props->phys_aligned = 0;
+	mem_props->alignment = alignment;
+    mem_props->handle = 0;
+
+	mem_props->virt_unaligned =
+	    (size_t) mhi_alloc_coherent(mhi_cntrl,
+				     (mem_props->size + (size_t) alignment),
+				     &bam_PA, GFP_KERNEL);
+
+	if (mem_props->virt_unaligned == 0) {
+		MHI_ERR("ERROR: Failed allocating uncahced contiguous memory!!");
+		return;
+    }
+
+    mem_props->alloc_size = mem_props->size + (size_t) alignment;
+
+    MHI_LOG("%llu + %llu allocated. MHI Ctrl VA: 0x%llx!",
+            mem_props->size, alignment, mem_props->virt_unaligned);
+
+    mem_props->phys_unaligned = bam_PA;
+    mem_props->handle = bam_PA;
+
+    mem_props->phys_aligned =
+        ((mem_props->phys_unaligned + alignment) & ~alignment);
+
+    mem_props->virt_aligned =
+        ((mem_props->virt_unaligned + alignment) & ~alignment);
+
+    MHI_LOG("MHI Control PA-A: 0x%llx, VA-A: 0x%llx!!",
+            mem_props->phys_aligned, mem_props->virt_aligned);
+
+    return;
+}
+
+#define MHI_ALIGN_4K_OFFSET          0xFFF
+
+/**
+ * @brief initialize the control segment
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+int mhi_init_ctrl_seq(struct mhi_controller *mhi_cntrl)
+{
+	mhi_malloc_contiguous_memory(mhi_cntrl, MHI_ALIGN_4K_OFFSET);
+
+	MHI_LOG(
+	       "MHI ctrl: VA-A: 0x%llx, PA-A: 0x%llx, Size: %lld bytes\n",
+			mhi_cntrl->mem_props.virt_aligned,
+			mhi_cntrl->mem_props.phys_aligned,
+			mhi_cntrl->mem_props.size);
+
+	if (mhi_cntrl->mem_props.virt_unaligned == 0)
+		return -ENOMEM;
+
+    mhi_cntrl->ctrl_seg = (struct mhi_ctrl_seg *)(size_t)mhi_cntrl->mem_props.virt_aligned;
+
+	MHI_LOG("ctrl_seg 0x%p", mhi_cntrl->ctrl_seg);
+
+	return 0;
+}
+
+/**
+ * @brief uninitialize the control segment
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+int mhi_deinit_ctrl_seq(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_mem_props *mem_props = &mhi_cntrl->mem_props;
+
+    if (mhi_cntrl->ctrl_seg) {
+	    
+        MHI_LOG("free ctrl_seg 0x%p size %llu", mhi_cntrl->ctrl_seg, mem_props->alloc_size);
+        
+        mhi_free_coherent(mhi_cntrl,
+				     mem_props->alloc_size,
+				     (void *)(size_t)mem_props->virt_unaligned,
+				     mem_props->handle);
+
+        mhi_cntrl->ctrl_seg = NULL;    
+    }
+
+    return 0;
+}
+
+/**
+ * @brief uninitialize mhi core
+ */
+int mhi_deinit(void)
+{
+	bus_unregister(&mhi_bus_type);
+		
+	mhi_dtr_deinit();
+
+	return 0;
+}
+
+/**
+ * @brief initialize mhi core
+ */
+int mhi_init(void)
+{
+	int ret = 0;
+
+	mutex_init(&mhi_bus.lock);
+	INIT_LIST_HEAD(&mhi_bus.controller_list);
+
+#ifdef CONFIG_DEBUG_FS
+	/* parent directory */
+	debugfs_create_dir(mhi_bus_type.name, NULL);
+#endif
+
+	ret = bus_register(&mhi_bus_type);
+
+	if (!ret)
+		mhi_dtr_init();
+
+	return ret;
+}
diff --git a/drivers/staging/em9190/core/mhi_internal.h b/drivers/staging/em9190/core/mhi_internal.h
new file mode 100644
index 000000000000..ccd4468dbdd0
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_internal.h
@@ -0,0 +1,966 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _MHI_INT_H
+#define _MHI_INT_H
+
+extern struct bus_type mhi_bus_type;
+
+/* MHI mmio register mapping */
+#define PCI_INVALID_READ(val) (val == U32_MAX)
+
+#define MHIREGLEN (0x0)
+#define MHIREGLEN_MHIREGLEN_MASK (0xFFFFFFFF)
+#define MHIREGLEN_MHIREGLEN_SHIFT (0)
+
+#define MHIVER (0x8)
+#define MHIVER_MHIVER_MASK (0xFFFFFFFF)
+#define MHIVER_MHIVER_SHIFT (0)
+
+#define MHICFG (0x10)
+#define MHICFG_NHWER_MASK (0xFF000000)
+#define MHICFG_NHWER_SHIFT (24)
+#define MHICFG_NER_MASK (0xFF0000)
+#define MHICFG_NER_SHIFT (16)
+#define MHICFG_NHWCH_MASK (0xFF00)
+#define MHICFG_NHWCH_SHIFT (8)
+#define MHICFG_NCH_MASK (0xFF)
+#define MHICFG_NCH_SHIFT (0)
+
+#define CHDBOFF (0x18)
+#define CHDBOFF_CHDBOFF_MASK (0xFFFFFFFF)
+#define CHDBOFF_CHDBOFF_SHIFT (0)
+
+#define ERDBOFF (0x20)
+#define ERDBOFF_ERDBOFF_MASK (0xFFFFFFFF)
+#define ERDBOFF_ERDBOFF_SHIFT (0)
+
+#define BHIOFF (0x28)
+#define BHIOFF_BHIOFF_MASK (0xFFFFFFFF)
+#define BHIOFF_BHIOFF_SHIFT (0)
+
+#define BHIEOFF (0x2C)
+#define BHIEOFF_BHIEOFF_MASK (0xFFFFFFFF)
+#define BHIEOFF_BHIEOFF_SHIFT (0)
+
+#define DEBUGOFF (0x30)
+#define DEBUGOFF_DEBUGOFF_MASK (0xFFFFFFFF)
+#define DEBUGOFF_DEBUGOFF_SHIFT (0)
+
+#define MHICTRL (0x38)
+#define MHICTRL_MHISTATE_MASK (0x0000FF00)
+#define MHICTRL_MHISTATE_SHIFT (8)
+#define MHICTRL_RESET_MASK (0x2)
+#define MHICTRL_RESET_SHIFT (1)
+
+#define MHISTATUS (0x48)
+#define MHISTATUS_MHISTATE_MASK (0x0000FF00)
+#define MHISTATUS_MHISTATE_SHIFT (8)
+#define MHISTATUS_SYSERR_MASK (0x4)
+#define MHISTATUS_SYSERR_SHIFT (2)
+#define MHISTATUS_READY_MASK (0x1)
+#define MHISTATUS_READY_SHIFT (0)
+
+#define CCABAP_LOWER (0x58)
+#define CCABAP_LOWER_CCABAP_LOWER_MASK (0xFFFFFFFF)
+#define CCABAP_LOWER_CCABAP_LOWER_SHIFT (0)
+
+#define CCABAP_HIGHER (0x5C)
+#define CCABAP_HIGHER_CCABAP_HIGHER_MASK (0xFFFFFFFF)
+#define CCABAP_HIGHER_CCABAP_HIGHER_SHIFT (0)
+
+#define ECABAP_LOWER (0x60)
+#define ECABAP_LOWER_ECABAP_LOWER_MASK (0xFFFFFFFF)
+#define ECABAP_LOWER_ECABAP_LOWER_SHIFT (0)
+
+#define ECABAP_HIGHER (0x64)
+#define ECABAP_HIGHER_ECABAP_HIGHER_MASK (0xFFFFFFFF)
+#define ECABAP_HIGHER_ECABAP_HIGHER_SHIFT (0)
+
+#define CRCBAP_LOWER (0x68)
+#define CRCBAP_LOWER_CRCBAP_LOWER_MASK (0xFFFFFFFF)
+#define CRCBAP_LOWER_CRCBAP_LOWER_SHIFT (0)
+
+#define CRCBAP_HIGHER (0x6C)
+#define CRCBAP_HIGHER_CRCBAP_HIGHER_MASK (0xFFFFFFFF)
+#define CRCBAP_HIGHER_CRCBAP_HIGHER_SHIFT (0)
+
+#define CRDB_LOWER (0x70)
+#define CRDB_LOWER_CRDB_LOWER_MASK (0xFFFFFFFF)
+#define CRDB_LOWER_CRDB_LOWER_SHIFT (0)
+
+#define CRDB_HIGHER (0x74)
+#define CRDB_HIGHER_CRDB_HIGHER_MASK (0xFFFFFFFF)
+#define CRDB_HIGHER_CRDB_HIGHER_SHIFT (0)
+
+#define MHICTRLBASE_LOWER (0x80)
+#define MHICTRLBASE_LOWER_MHICTRLBASE_LOWER_MASK (0xFFFFFFFF)
+#define MHICTRLBASE_LOWER_MHICTRLBASE_LOWER_SHIFT (0)
+
+#define MHICTRLBASE_HIGHER (0x84)
+#define MHICTRLBASE_HIGHER_MHICTRLBASE_HIGHER_MASK (0xFFFFFFFF)
+#define MHICTRLBASE_HIGHER_MHICTRLBASE_HIGHER_SHIFT (0)
+
+#define MHICTRLLIMIT_LOWER (0x88)
+#define MHICTRLLIMIT_LOWER_MHICTRLLIMIT_LOWER_MASK (0xFFFFFFFF)
+#define MHICTRLLIMIT_LOWER_MHICTRLLIMIT_LOWER_SHIFT (0)
+
+#define MHICTRLLIMIT_HIGHER (0x8C)
+#define MHICTRLLIMIT_HIGHER_MHICTRLLIMIT_HIGHER_MASK (0xFFFFFFFF)
+#define MHICTRLLIMIT_HIGHER_MHICTRLLIMIT_HIGHER_SHIFT (0)
+
+#define MHIDATABASE_LOWER (0x98)
+#define MHIDATABASE_LOWER_MHIDATABASE_LOWER_MASK (0xFFFFFFFF)
+#define MHIDATABASE_LOWER_MHIDATABASE_LOWER_SHIFT (0)
+
+#define MHIDATABASE_HIGHER (0x9C)
+#define MHIDATABASE_HIGHER_MHIDATABASE_HIGHER_MASK (0xFFFFFFFF)
+#define MHIDATABASE_HIGHER_MHIDATABASE_HIGHER_SHIFT (0)
+
+#define MHIDATALIMIT_LOWER (0xA0)
+#define MHIDATALIMIT_LOWER_MHIDATALIMIT_LOWER_MASK (0xFFFFFFFF)
+#define MHIDATALIMIT_LOWER_MHIDATALIMIT_LOWER_SHIFT (0)
+
+#define MHIDATALIMIT_HIGHER (0xA4)
+#define MHIDATALIMIT_HIGHER_MHIDATALIMIT_HIGHER_MASK (0xFFFFFFFF)
+#define MHIDATALIMIT_HIGHER_MHIDATALIMIT_HIGHER_SHIFT (0)
+
+#define CCABAP                            0x58
+#define ECABAP                            0x60
+#define CRCBAP                            0x68
+#define CRDB                              0x70
+
+#define MHICTRLBASE                       0x80
+#define MHICTRLLIMIT                      0x88
+#define MHIDATABASE                       0x98
+#define MHIDATALIMIT                      0xA0
+
+/* Host request register */
+#define MHI_SOC_RESET_REQ_OFFSET (0xB0)
+#define MHI_SOC_RESET_REQ BIT(0)
+
+/* MHI misc capability registers */
+#define MISC_OFFSET (0x24)
+#define MISC_CAP_MASK (0xFFFFFFFF)
+#define MISC_CAP_SHIFT (0)
+
+#define CAP_CAPID_MASK (0xFF000000)
+#define CAP_CAPID_SHIFT (24)
+#define CAP_NEXT_CAP_MASK (0x00FFF000)
+#define CAP_NEXT_CAP_SHIFT (12)
+
+/* MHI Timesync offsets */
+#define TIMESYNC_CFG_OFFSET (0x00)
+#define TIMESYNC_CFG_CAPID_MASK (CAP_CAPID_MASK)
+#define TIMESYNC_CFG_CAPID_SHIFT (CAP_CAPID_SHIFT)
+#define TIMESYNC_CFG_NEXT_OFF_MASK (CAP_NEXT_CAP_MASK)
+#define TIMESYNC_CFG_NEXT_OFF_SHIFT (CAP_NEXT_CAP_SHIFT)
+#define TIMESYNC_CFG_NUMCMD_MASK (0xFF)
+#define TIMESYNC_CFG_NUMCMD_SHIFT (0)
+#define TIMESYNC_DB_OFFSET (0x4)
+#define TIMESYNC_TIME_LOW_OFFSET (0x8)
+#define TIMESYNC_TIME_HIGH_OFFSET (0xC)
+
+#define TIMESYNC_CAP_ID (2)
+
+/* MHI BHI offfsets */
+#define BHI_BHIVERSION_MINOR (0x00)
+#define BHI_BHIVERSION_MAJOR (0x04)
+#define BHI_IMGADDR_LOW (0x08)
+#define BHI_IMGADDR_HIGH (0x0C)
+#define BHI_IMGSIZE (0x10)
+#define BHI_RSVD1 (0x14)
+#define BHI_IMGTXDB (0x18)
+#define BHI_TXDB_SEQNUM_BMSK (0x3FFFFFFF)
+#define BHI_TXDB_SEQNUM_SHFT (0)
+#define BHI_RSVD2 (0x1C)
+#define BHI_INTVEC (0x20)
+#define BHI_RSVD3 (0x24)
+#define BHI_EXECENV (0x28)
+#define BHI_STATUS (0x2C)
+#define BHI_ERRCODE (0x30)
+#define BHI_ERRDBG1 (0x34)
+#define BHI_ERRDBG2 (0x38)
+#define BHI_ERRDBG3 (0x3C)
+#define BHI_SERIALNU (0x40)
+#define BHI_SBLANTIROLLVER (0x44)
+#define BHI_NUMSEG (0x48)
+#define BHI_MSMHWID(n) (0x4C + (0x4 * n))
+#define BHI_OEMPKHASH(n) (0x64 + (0x4 * n))
+#define BHI_RSVD5 (0xC4)
+#define BHI_STATUS_MASK (0xC0000000)
+#define BHI_STATUS_SHIFT (30)
+#define BHI_STATUS_ERROR (3)
+#define BHI_STATUS_SUCCESS (2)
+#define BHI_STATUS_RESET (0)
+
+/* MHI BHIE offsets */
+#define BHIE_MSMSOCID_OFFS (0x0000)
+#define BHIE_TXVECADDR_LOW_OFFS (0x002C)
+#define BHIE_TXVECADDR_HIGH_OFFS (0x0030)
+#define BHIE_TXVECSIZE_OFFS (0x0034)
+#define BHIE_TXVECDB_OFFS (0x003C)
+#define BHIE_TXVECDB_SEQNUM_BMSK (0x3FFFFFFF)
+#define BHIE_TXVECDB_SEQNUM_SHFT (0)
+#define BHIE_TXVECSTATUS_OFFS (0x0044)
+#define BHIE_TXVECSTATUS_SEQNUM_BMSK (0x3FFFFFFF)
+#define BHIE_TXVECSTATUS_SEQNUM_SHFT (0)
+#define BHIE_TXVECSTATUS_STATUS_BMSK (0xC0000000)
+#define BHIE_TXVECSTATUS_STATUS_SHFT (30)
+#define BHIE_TXVECSTATUS_STATUS_RESET (0x00)
+#define BHIE_TXVECSTATUS_STATUS_XFER_COMPL (0x02)
+#define BHIE_TXVECSTATUS_STATUS_ERROR (0x03)
+#define BHIE_RXVECADDR_LOW_OFFS (0x0060)
+#define BHIE_RXVECADDR_HIGH_OFFS (0x0064)
+#define BHIE_RXVECSIZE_OFFS (0x0068)
+#define BHIE_RXVECDB_OFFS (0x0070)
+#define BHIE_RXVECDB_SEQNUM_BMSK (0x3FFFFFFF)
+#define BHIE_RXVECDB_SEQNUM_SHFT (0)
+#define BHIE_RXVECSTATUS_OFFS (0x0078)
+#define BHIE_RXVECSTATUS_SEQNUM_BMSK (0x3FFFFFFF)
+#define BHIE_RXVECSTATUS_SEQNUM_SHFT (0)
+#define BHIE_RXVECSTATUS_STATUS_BMSK (0xC0000000)
+#define BHIE_RXVECSTATUS_STATUS_SHFT (30)
+#define BHIE_RXVECSTATUS_STATUS_RESET (0x00)
+#define BHIE_RXVECSTATUS_STATUS_XFER_COMPL (0x02)
+#define BHIE_RXVECSTATUS_STATUS_ERROR (0x03)
+
+#define SOC_HW_VERSION_OFFS (0x224)
+#define SOC_HW_VERSION_FAM_NUM_BMSK (0xF0000000)
+#define SOC_HW_VERSION_FAM_NUM_SHFT (28)
+#define SOC_HW_VERSION_DEV_NUM_BMSK (0x0FFF0000)
+#define SOC_HW_VERSION_DEV_NUM_SHFT (16)
+#define SOC_HW_VERSION_MAJOR_VER_BMSK (0x0000FF00)
+#define SOC_HW_VERSION_MAJOR_VER_SHFT (8)
+#define SOC_HW_VERSION_MINOR_VER_BMSK (0x000000FF)
+#define SOC_HW_VERSION_MINOR_VER_SHFT (0)
+
+/* timesync time calculations */
+#define LOCAL_TICKS_TO_US(x) (div_u64((x) * 100ULL, \
+				div_u64(mhi_cntrl->local_timer_freq, 10000ULL)))
+#define REMOTE_TICKS_TO_US(x) (div_u64((x) * 100ULL, \
+			       div_u64(mhi_cntrl->remote_timer_freq, 10000ULL)))
+
+#pragma pack(push,1)
+
+__attribute__ ((aligned (1)))
+struct mhi_event_ctxt {
+	u32 reserved : 8;
+	u32 intmodc : 8;
+	u32 intmodt : 16;
+	u32 ertype;
+	u32 msivec;
+	u64 rbase; // __packed __aligned(4);
+	u64 rlen; //__packed __aligned(4);
+	u64 rp; // __packed __aligned(4);
+	u64 wp; // __packed __aligned(4);
+};
+
+__attribute__ ((aligned (1)))
+struct mhi_chan_ctxt {
+	u32 chstate : 8;
+	u32 brstmode : 2;
+	u32 pollcfg : 6;
+	u32 reserved : 16;
+	u32 chtype;
+	u32 erindex;
+	u64 rbase; // __packed __aligned(4);
+	u64 rlen; // __packed __aligned(4);
+	u64 rp; // __packed __aligned(4);
+	u64 wp; // __packed __aligned(4);
+};
+
+__attribute__ ((aligned (1)))
+struct mhi_cmd_ctxt {
+	u32 reserved0;
+	u32 reserved1;
+	u32 reserved2;
+	u64 rbase; // __packed __aligned(4);
+	u64 rlen; // __packed __aligned(4);
+	u64 rp; // __packed __aligned(4);
+	u64 wp; // __packed __aligned(4);
+};
+
+__attribute__ ((aligned (1)))
+struct mhi_tre {
+	u64 ptr;
+	u32 dword[2];
+};
+
+__attribute__ ((aligned (1)))
+struct bhi_vec_entry {
+	u64 dma_addr;
+	u64 size;
+};
+
+#pragma pack(pop)
+
+enum mhi_cmd_type {
+	MHI_CMD_TYPE_NOP = 1,
+	MHI_CMD_TYPE_RESET = 16,
+	MHI_CMD_TYPE_STOP = 17,
+	MHI_CMD_TYPE_START = 18,
+	MHI_CMD_TYPE_TSYNC = 24,
+};
+
+/* no operation command */
+#define MHI_TRE_CMD_NOOP_PTR (0)
+#define MHI_TRE_CMD_NOOP_DWORD0 (0)
+#define MHI_TRE_CMD_NOOP_DWORD1 (MHI_CMD_TYPE_NOP << 16)
+
+/* channel reset command */
+#define MHI_TRE_CMD_RESET_PTR (0)
+#define MHI_TRE_CMD_RESET_DWORD0 (0)
+#define MHI_TRE_CMD_RESET_DWORD1(chid) ((chid << 24) | \
+					(MHI_CMD_TYPE_RESET << 16))
+
+/* channel stop command */
+#define MHI_TRE_CMD_STOP_PTR (0)
+#define MHI_TRE_CMD_STOP_DWORD0 (0)
+#define MHI_TRE_CMD_STOP_DWORD1(chid) ((chid << 24) | (MHI_CMD_TYPE_STOP << 16))
+
+/* channel start command */
+#define MHI_TRE_CMD_START_PTR (0)
+#define MHI_TRE_CMD_START_DWORD0 (0)
+#define MHI_TRE_CMD_START_DWORD1(chid) ((chid << 24) | \
+					(MHI_CMD_TYPE_START << 16))
+
+/* time sync cfg command */
+#define MHI_TRE_CMD_TSYNC_CFG_PTR (0)
+#define MHI_TRE_CMD_TSYNC_CFG_DWORD0 (0)
+#define MHI_TRE_CMD_TSYNC_CFG_DWORD1(er) ((MHI_CMD_TYPE_TSYNC << 16) | \
+					  (er << 24))
+
+#define MHI_TRE_GET_CMD_CHID(tre) (((tre)->dword[1] >> 24) & 0xFF)
+#define MHI_TRE_GET_CMD_TYPE(tre) (((tre)->dword[1] >> 16) & 0xFF)
+
+/* event descriptor macros */
+#define MHI_TRE_EV_PTR(ptr) (ptr)
+#define MHI_TRE_EV_DWORD0(code, len) ((code << 24) | len)
+#define MHI_TRE_EV_DWORD1(chid, type) ((chid << 24) | (type << 16))
+#define MHI_TRE_GET_EV_PTR(tre) ((tre)->ptr)
+#define MHI_TRE_GET_EV_CODE(tre) (((tre)->dword[0] >> 24) & 0xFF)
+#define MHI_TRE_GET_EV_LEN(tre) ((tre)->dword[0] & 0xFFFF)
+#define MHI_TRE_GET_EV_CHID(tre) (((tre)->dword[1] >> 24) & 0xFF)
+#define MHI_TRE_GET_EV_TYPE(tre) (((tre)->dword[1] >> 16) & 0xFF)
+#define MHI_TRE_GET_EV_STATE(tre) (((tre)->dword[0] >> 24) & 0xFF)
+#define MHI_TRE_GET_EV_EXECENV(tre) (((tre)->dword[0] >> 24) & 0xFF)
+#define MHI_TRE_GET_EV_SEQ(tre) ((tre)->dword[0])
+#define MHI_TRE_GET_EV_TIME(tre) ((tre)->ptr)
+#define MHI_TRE_GET_EV_COOKIE(tre) lower_32_bits((tre)->ptr)
+#define MHI_TRE_GET_EV_VEID(tre) (((tre)->dword[0] >> 16) & 0xFF)
+#define MHI_TRE_GET_EV_LINKSPEED(tre) (((tre)->dword[1] >> 24) & 0xFF)
+#define MHI_TRE_GET_EV_LINKWIDTH(tre) ((tre)->dword[0] & 0xFF)
+
+/* transfer descriptor macros */
+#define MHI_TRE_DATA_PTR(ptr) (ptr)
+#define MHI_TRE_DATA_DWORD0(len) (len & MHI_MAX_MTU)
+#define MHI_TRE_DATA_DWORD1(bei, ieot, ieob, chain) ((2 << 16) | (bei << 10) \
+	| (ieot << 9) | (ieob << 8) | chain)
+
+/* rsc transfer descriptor macros */
+#define MHI_RSCTRE_DATA_PTR(ptr, len) (((u64)len << 48) | ptr)
+#define MHI_RSCTRE_DATA_DWORD0(cookie) (cookie)
+#define MHI_RSCTRE_DATA_DWORD1 (MHI_PKT_TYPE_COALESCING << 16)
+
+enum MHI_CMD {
+	MHI_CMD_RESET_CHAN,
+	MHI_CMD_START_CHAN,
+	MHI_CMD_TIMSYNC_CFG,
+};
+
+enum MHI_PKT_TYPE {
+	MHI_PKT_TYPE_INVALID = 0x0,
+	MHI_PKT_TYPE_NOOP_CMD = 0x1,
+	MHI_PKT_TYPE_TRANSFER = 0x2,
+	MHI_PKT_TYPE_COALESCING = 0x8,
+	MHI_PKT_TYPE_RESET_CHAN_CMD = 0x10,
+	MHI_PKT_TYPE_STOP_CHAN_CMD = 0x11,
+	MHI_PKT_TYPE_START_CHAN_CMD = 0x12,
+	MHI_PKT_TYPE_STATE_CHANGE_EVENT = 0x20,
+	MHI_PKT_TYPE_CMD_COMPLETION_EVENT = 0x21,
+	MHI_PKT_TYPE_TX_EVENT = 0x22,
+	MHI_PKT_TYPE_RSC_TX_EVENT = 0x28,
+	MHI_PKT_TYPE_EE_EVENT = 0x40,
+	MHI_PKT_TYPE_TSYNC_EVENT = 0x48,
+	MHI_PKT_TYPE_BW_REQ_EVENT = 0x50,
+	MHI_PKT_TYPE_STALE_EVENT, /* internal event */
+};
+
+/* MHI transfer completion events */
+enum MHI_EV_CCS {
+	MHI_EV_CC_INVALID = 0x0,
+	MHI_EV_CC_SUCCESS = 0x1,
+	MHI_EV_CC_EOT = 0x2,
+	MHI_EV_CC_OVERFLOW = 0x3,
+	MHI_EV_CC_EOB = 0x4,
+	MHI_EV_CC_OOB = 0x5,
+	MHI_EV_CC_DB_MODE = 0x6,
+	MHI_EV_CC_UNDEFINED_ERR = 0x10,
+	MHI_EV_CC_BAD_TRE = 0x11,
+};
+
+enum MHI_CH_STATE {
+	MHI_CH_STATE_DISABLED = 0x0,
+	MHI_CH_STATE_ENABLED = 0x1,
+	MHI_CH_STATE_RUNNING = 0x2,
+	MHI_CH_STATE_SUSPENDED = 0x3,
+	MHI_CH_STATE_STOP = 0x4,
+	MHI_CH_STATE_ERROR = 0x5,
+};
+
+enum MHI_BRSTMODE {
+	MHI_BRSTMODE_DISABLE = 0x2,
+	MHI_BRSTMODE_ENABLE = 0x3,
+};
+
+#define MHI_INVALID_BRSTMODE(mode) (mode != MHI_BRSTMODE_DISABLE && \
+				    mode != MHI_BRSTMODE_ENABLE)
+
+extern const char * const mhi_ee_str[MHI_EE_MAX];
+#define TO_MHI_EXEC_STR(ee) (((ee) >= MHI_EE_MAX) ? \
+			     "INVALID_EE" : mhi_ee_str[ee])
+
+#define MHI_IN_PBL(ee) (ee == MHI_EE_PBL || ee == MHI_EE_PTHRU || \
+			ee == MHI_EE_EDL)
+
+#define MHI_IN_MISSION_MODE(ee) (ee == MHI_EE_AMSS || ee == MHI_EE_WFW)
+
+enum MHI_ST_TRANSITION {
+	MHI_ST_TRANSITION_PBL,
+	MHI_ST_TRANSITION_READY,
+	MHI_ST_TRANSITION_SBL,
+	MHI_ST_TRANSITION_MISSION_MODE,
+	MHI_ST_TRANSITION_MAX,
+};
+
+extern const char * const mhi_state_tran_str[MHI_ST_TRANSITION_MAX];
+#define TO_MHI_STATE_TRANS_STR(state) (((state) >= MHI_ST_TRANSITION_MAX) ? \
+				"INVALID_STATE" : mhi_state_tran_str[state])
+
+extern const char * const mhi_state_str[MHI_STATE_MAX];
+#define TO_MHI_STATE_STR(state) ((state >= MHI_STATE_MAX || \
+				  !mhi_state_str[state]) ? \
+				"INVALID_STATE" : mhi_state_str[state])
+
+enum {
+	MHI_PM_BIT_DISABLE,
+	MHI_PM_BIT_POR,
+	MHI_PM_BIT_M0,
+	MHI_PM_BIT_M2,
+	MHI_PM_BIT_M3_ENTER,
+	MHI_PM_BIT_M3,
+	MHI_PM_BIT_M3_EXIT,
+	MHI_PM_BIT_FW_DL_ERR,
+	MHI_PM_BIT_SYS_ERR_DETECT,
+	MHI_PM_BIT_SYS_ERR_PROCESS,
+	MHI_PM_BIT_SHUTDOWN_PROCESS,
+	MHI_PM_BIT_LD_ERR_FATAL_DETECT,
+	MHI_PM_BIT_MAX
+};
+
+/* internal power states */
+enum MHI_PM_STATE {
+	MHI_PM_DISABLE = BIT(MHI_PM_BIT_DISABLE), /* MHI is not enabled */
+	MHI_PM_POR = BIT(MHI_PM_BIT_POR), /* reset state */
+	MHI_PM_M0 = BIT(MHI_PM_BIT_M0),
+	MHI_PM_M2 = BIT(MHI_PM_BIT_M2),
+	MHI_PM_M3_ENTER = BIT(MHI_PM_BIT_M3_ENTER),
+	MHI_PM_M3 = BIT(MHI_PM_BIT_M3),
+	MHI_PM_M3_EXIT = BIT(MHI_PM_BIT_M3_EXIT),
+	/* firmware download failure state */
+	MHI_PM_FW_DL_ERR = BIT(MHI_PM_BIT_FW_DL_ERR),
+	MHI_PM_SYS_ERR_DETECT = BIT(MHI_PM_BIT_SYS_ERR_DETECT),
+	MHI_PM_SYS_ERR_PROCESS = BIT(MHI_PM_BIT_SYS_ERR_PROCESS),
+	MHI_PM_SHUTDOWN_PROCESS = BIT(MHI_PM_BIT_SHUTDOWN_PROCESS),
+	/* link not accessible */
+	MHI_PM_LD_ERR_FATAL_DETECT = BIT(MHI_PM_BIT_LD_ERR_FATAL_DETECT),
+};
+
+#define MHI_REG_ACCESS_VALID(pm_state) ((pm_state & (MHI_PM_POR | MHI_PM_M0 | \
+		MHI_PM_M2 | MHI_PM_M3_ENTER | MHI_PM_M3_EXIT | \
+		MHI_PM_SYS_ERR_DETECT | MHI_PM_SYS_ERR_PROCESS | \
+		MHI_PM_SHUTDOWN_PROCESS | MHI_PM_FW_DL_ERR)))
+#define MHI_PM_IN_ERROR_STATE(pm_state) (pm_state >= MHI_PM_FW_DL_ERR)
+#define MHI_PM_IN_FATAL_STATE(pm_state) (pm_state == MHI_PM_LD_ERR_FATAL_DETECT)
+#define MHI_DB_ACCESS_VALID(mhi_cntrl) (mhi_cntrl->pm_state & \
+					mhi_cntrl->db_access)
+#define MHI_WAKE_DB_CLEAR_VALID(pm_state) (pm_state & (MHI_PM_M0 | \
+						MHI_PM_M2 | MHI_PM_M3_EXIT))
+#define MHI_WAKE_DB_SET_VALID(pm_state) (pm_state & MHI_PM_M2)
+#define MHI_WAKE_DB_FORCE_SET_VALID(pm_state) MHI_WAKE_DB_CLEAR_VALID(pm_state)
+#define MHI_EVENT_ACCESS_INVALID(pm_state) (pm_state == MHI_PM_DISABLE || \
+					    MHI_PM_IN_ERROR_STATE(pm_state))
+#define MHI_PM_IN_SUSPEND_STATE(pm_state) (pm_state & \
+					   (MHI_PM_M3_ENTER | MHI_PM_M3))
+
+/* accepted buffer type for the channel */
+enum MHI_XFER_TYPE {
+	MHI_XFER_BUFFER,
+	MHI_XFER_SKB,
+	MHI_XFER_SCLIST,
+	MHI_XFER_NOP, /* CPU offload channel, host does not accept transfer */
+	MHI_XFER_DMA, /* receive dma address, already mapped by client */
+	MHI_XFER_RSC_DMA, /* RSC type, accept premapped buffer */
+	MHI_XFER_QMAP, /* for sending data packets in QMAP */
+};
+
+#define NR_OF_CMD_RINGS (1)
+#define CMD_EL_PER_RING (128)
+#define PRIMARY_CMD_RING (0)
+#define MHI_DEV_WAKE_DB (127)
+#define MHI_MAX_MTU (0xffff)
+
+enum MHI_ER_TYPE {
+	MHI_ER_TYPE_INVALID = 0x0,
+	MHI_ER_TYPE_VALID = 0x1,
+};
+
+enum mhi_er_data_type {
+	MHI_ER_DATA_ELEMENT_TYPE,
+	MHI_ER_CTRL_ELEMENT_TYPE,
+	MHI_ER_TSYNC_ELEMENT_TYPE,
+	MHI_ER_DATA_TYPE_MAX = MHI_ER_TSYNC_ELEMENT_TYPE,
+};
+
+enum mhi_ch_ee_mask {
+	MHI_CH_EE_PBL = BIT(MHI_EE_PBL),
+	MHI_CH_EE_SBL = BIT(MHI_EE_SBL),
+	MHI_CH_EE_AMSS = BIT(MHI_EE_AMSS),
+	MHI_CH_EE_RDDM = BIT(MHI_EE_RDDM),
+	MHI_CH_EE_PTHRU = BIT(MHI_EE_PTHRU),
+	MHI_CH_EE_WFW = BIT(MHI_EE_WFW),
+	MHI_CH_EE_EDL = BIT(MHI_EE_EDL),
+};
+
+enum mhi_ch_type {
+	MHI_CH_TYPE_INVALID = 0,
+	MHI_CH_TYPE_OUTBOUND = DMA_TO_DEVICE,
+	MHI_CH_TYPE_INBOUND = DMA_FROM_DEVICE,
+	MHI_CH_TYPE_INBOUND_COALESCED = 3,
+};
+
+struct db_cfg {
+	bool reset_req;
+	bool db_mode;
+	u32 pollcfg;
+	enum MHI_BRSTMODE brstmode;
+	dma_addr_t db_val;
+	void (*process_db)(struct mhi_controller *mhi_cntrl,
+			   struct db_cfg *db_cfg, void __iomem *io_addr,
+			   dma_addr_t db_val);
+};
+
+struct mhi_pm_transitions {
+	enum MHI_PM_STATE from_state;
+	u32 to_states;
+};
+
+struct state_transition {
+	struct list_head node;
+	enum MHI_ST_TRANSITION state;
+};
+
+struct mhi_ctxt {
+	struct mhi_event_ctxt *er_ctxt;
+	struct mhi_chan_ctxt *chan_ctxt;
+	struct mhi_cmd_ctxt *cmd_ctxt;
+	dma_addr_t er_ctxt_addr;
+	dma_addr_t chan_ctxt_addr;
+	dma_addr_t cmd_ctxt_addr;
+};
+
+struct mhi_ring {
+	dma_addr_t dma_handle;
+	dma_addr_t iommu_base;
+	u64 *ctxt_wp; /* point to ctxt wp */
+	void *pre_aligned;
+	void *base;
+	void *rp;
+	void *wp;
+	size_t el_size;
+	size_t len;
+	size_t elements;
+	size_t alloc_size;
+	void __iomem *db_addr;
+};
+
+struct mhi_cmd {
+	struct mhi_ring ring;
+	spinlock_t lock;
+};
+
+struct mhi_buf_info {
+	dma_addr_t p_addr;
+	void *v_addr;
+	void *bb_addr;
+	void *wp;
+	size_t len;
+	void *cb_buf;
+	bool used; /* indicate element is free to use */
+	bool pre_mapped; /* already pre-mapped by client */
+	enum dma_data_direction dir;
+	struct page *page;
+};
+
+struct mhi_event {
+	u32 er_index;
+	u32 intmod;
+	u32 msi;
+	int chan; /* this event ring is dedicated to a channel */
+	u32 priority;
+	enum mhi_er_data_type data_type;
+	struct mhi_ring ring;
+	struct db_cfg db_cfg;
+	bool hw_ring;
+	bool cl_manage;
+	bool offload_ev; /* managed by a device driver */
+	spinlock_t lock;
+	struct mhi_chan *mhi_chan; /* dedicated to channel */
+	struct tasklet_struct task;
+	int (*process_event)(struct mhi_controller *mhi_cntrl,
+			     struct mhi_event *mhi_event,
+			     u32 event_quota);
+	struct mhi_controller *mhi_cntrl;
+};
+
+struct mhi_chan {
+	u32 chan;
+	const char *name;
+	/*
+	 * important, when consuming increment tre_ring first, when releasing
+	 * decrement buf_ring first. If tre_ring has space, buf_ring
+	 * guranteed to have space so we do not need to check both rings.
+	 */
+	struct mhi_ring buf_ring;
+	struct mhi_ring tre_ring;
+	u32 er_index;
+	u32 tre_index;
+	u32 intmod;
+	enum mhi_ch_type type;
+	enum dma_data_direction dir;
+	struct db_cfg db_cfg;
+	u32 ee_mask;
+	enum MHI_XFER_TYPE xfer_type;
+	enum MHI_CH_STATE ch_state;
+	enum MHI_EV_CCS ccs;
+	bool lpm_notify;
+	bool configured;
+	bool offload_ch;
+	bool pre_alloc;
+	bool auto_start;
+	bool wake_capable; /* channel should wake up system */
+	/* functions that generate the transfer ring elements */
+	int (*gen_tre)(struct mhi_controller *, struct mhi_chan *, void *,
+		       void *, size_t, enum MHI_FLAGS);
+	int (*queue_xfer)(struct mhi_device *, struct mhi_chan *, void *,
+			  size_t, enum MHI_FLAGS);
+	bool (*queue_full)(struct mhi_device *, struct mhi_chan *);
+	/* xfer call back */
+	struct mhi_device *mhi_dev;
+	void (*xfer_cb)(struct mhi_device *, struct mhi_result *);
+	struct mutex mutex;
+	struct completion completion;
+	rwlock_t lock;
+	struct list_head node;
+    spinlock_t skb_write_lock;
+    spinlock_t dma_write_lock;
+	spinlock_t qmap_write_lock;	
+	spinlock_t tre_lock;
+};
+
+struct tsync_node {
+	struct list_head node;
+	u32 sequence;
+	u64 local_time;
+	u64 remote_time;
+	struct mhi_device *mhi_dev;
+	void (*cb_func)(struct mhi_device *mhi_dev, u32 sequence,
+			u64 local_time, u64 remote_time);
+};
+
+struct mhi_timesync {
+	u32 er_index;
+	void __iomem *db;
+	void __iomem *time_reg;
+	enum MHI_EV_CCS ccs;
+	struct completion completion;
+	spinlock_t lock; /* list protection */
+	struct mutex lpm_mutex; /* lpm protection */
+	struct list_head head;
+};
+
+struct mhi_bus {
+	struct list_head controller_list;
+	struct mutex lock;
+};
+
+struct mhi_chan_properties {
+    u32 reg;            /* physical channel number */
+    char *label;        /* given name for the channel */
+    u32 num_elements;   /* number of elements transfer ring support */
+    u32 event_ring;     /* event ring index associated with this channel */
+    u32 chan_dir;       /* channel direction as defined by enum dma_data_direction */
+    u32 data_type;      /* data transfer type accepted */
+    u32 doorbell_mode;  /* channel doorbell mode configuration */ 
+    u32 ee;             /* channel execution enviornment (EE) mask */
+    bool auto_queue;    /* MHI bus driver will pre-allocate buffers for this channel and queue to hardware */
+    bool auto_start;    /* MHI host driver to automatically start channels once mhi device driver probe is complete */
+};
+
+#define NUM_MHI_XFER_RINGS           16
+#define NUM_MHI_XFER_RING_ELEMENTS   512
+
+#define NUM_MHI_EVT_RINGS            4
+#define NUM_MHI_EVT_RING_ELEMENTS    2048
+
+#define NUM_MHI_HW_EVT_RINGS         2
+
+/* Control Segment */
+struct mhi_ctrl_seg {
+	struct mhi_tre 
+	    xfer_ring_element[NUM_MHI_XFER_RINGS][NUM_MHI_XFER_RING_ELEMENTS];
+	struct mhi_tre
+	    evt_ring_element[NUM_MHI_EVT_RINGS][NUM_MHI_EVT_RING_ELEMENTS];
+	struct mhi_tre cmd_ring_element[NUM_MHI_XFER_RING_ELEMENTS];
+	struct mhi_chan_ctxt xfer_ring[NUM_MHI_XFER_RINGS];
+	struct mhi_event_ctxt evt_ring[NUM_MHI_EVT_RINGS];
+	struct mhi_cmd_ctxt cmd_ring[NR_OF_CMD_RINGS];
+};
+
+/* default MHI timeout */
+#define MHI_TIMEOUT_MS (2000)
+extern struct mhi_bus mhi_bus;
+
+/* debug fs related functions */
+int mhi_debugfs_mhi_chan_show(struct seq_file *m, void *d);
+int mhi_debugfs_mhi_event_show(struct seq_file *m, void *d);
+int mhi_debugfs_mhi_states_show(struct seq_file *m, void *d);
+int mhi_debugfs_trigger_reset(void *data, u64 val);
+
+void mhi_deinit_debugfs(struct mhi_controller *mhi_cntrl);
+void mhi_init_debugfs(struct mhi_controller *mhi_cntrl);
+
+/* power management apis */
+enum MHI_PM_STATE __must_check mhi_tryset_pm_state(
+					struct mhi_controller *mhi_cntrl,
+					enum MHI_PM_STATE state);
+const char *to_mhi_pm_state_str(enum MHI_PM_STATE state);
+void mhi_reset_chan(struct mhi_controller *mhi_cntrl,
+		    struct mhi_chan *mhi_chan);
+enum mhi_ee mhi_get_exec_env(struct mhi_controller *mhi_cntrl);
+int mhi_queue_state_transition(struct mhi_controller *mhi_cntrl,
+			       enum MHI_ST_TRANSITION state);
+void mhi_pm_st_worker(struct work_struct *work);
+void mhi_fw_load_worker(struct work_struct *work);
+void mhi_pm_sys_err_worker(struct work_struct *work);
+int mhi_ready_state_transition(struct mhi_controller *mhi_cntrl);
+void mhi_ctrl_ev_task(unsigned long data);
+int mhi_pm_m0_transition(struct mhi_controller *mhi_cntrl);
+void mhi_pm_m1_transition(struct mhi_controller *mhi_cntrl);
+int mhi_pm_m3_transition(struct mhi_controller *mhi_cntrl);
+void mhi_notify(struct mhi_device *mhi_dev, enum MHI_CB cb_reason);
+int mhi_process_data_event_ring(struct mhi_controller *mhi_cntrl,
+				struct mhi_event *mhi_event, u32 event_quota);
+int mhi_process_ctrl_ev_ring(struct mhi_controller *mhi_cntrl,
+			     struct mhi_event *mhi_event, u32 event_quota);
+int mhi_process_tsync_event_ring(struct mhi_controller *mhi_cntrl,
+				 struct mhi_event *mhi_event, u32 event_quota);
+int mhi_send_cmd(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan,
+		 enum MHI_CMD cmd);
+int __mhi_device_get_sync(struct mhi_controller *mhi_cntrl);
+
+/* queue transfer buffer */
+int mhi_gen_tre(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan,
+		void *buf, void *cb, size_t buf_len, enum MHI_FLAGS flags);
+int mhi_queue_buf(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+int mhi_queue_skb(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+int mhi_queue_sclist(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+int mhi_queue_nop(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+int mhi_queue_dma(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+int mhi_queue_qmap(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan,
+		  void *buf, size_t len, enum MHI_FLAGS mflags);
+
+bool mhi_qmap_full(struct mhi_device *mhi_dev, struct mhi_chan *mhi_chan);
+
+/* register access methods */
+void mhi_db_brstmode(struct mhi_controller *mhi_cntrl, struct db_cfg *db_cfg,
+		     void __iomem *db_addr, dma_addr_t wp);
+void mhi_db_brstmode_disable(struct mhi_controller *mhi_cntrl,
+			     struct db_cfg *db_mode, void __iomem *db_addr,
+			     dma_addr_t wp);
+int __must_check mhi_read_reg(struct mhi_controller *mhi_cntrl,
+			      void __iomem *base, u32 offset, u32 *out);
+int __must_check mhi_read_reg_field(struct mhi_controller *mhi_cntrl,
+				    void __iomem *base, u32 offset, u32 mask,
+				    u32 shift, u32 *out);
+void mhi_write_reg(struct mhi_controller *mhi_cntrl, void __iomem *base,
+		   u32 offset, u32 val);
+void mhi_write_reg_field(struct mhi_controller *mhi_cntrl, void __iomem *base,
+			 u32 offset, u32 mask, u32 shift, u32 val);
+void mhi_ring_er_db(struct mhi_event *mhi_event);
+void mhi_write_db(struct mhi_controller *mhi_cntrl, void __iomem *db_addr,
+		  dma_addr_t wp);
+void mhi_ring_cmd_db(struct mhi_controller *mhi_cntrl, struct mhi_cmd *mhi_cmd);
+void mhi_ring_chan_db(struct mhi_controller *mhi_cntrl,
+		      struct mhi_chan *mhi_chan);
+int mhi_get_capability_offset(struct mhi_controller *mhi_cntrl, u32 capability,
+			      u32 *offset);
+int mhi_init_timesync(struct mhi_controller *mhi_cntrl);
+int mhi_create_timesync_sysfs(struct mhi_controller *mhi_cntrl);
+void mhi_destroy_timesync(struct mhi_controller *mhi_cntrl);
+int mhi_create_vote_sysfs(struct mhi_controller *mhi_cntrl);
+void mhi_destroy_vote_sysfs(struct mhi_controller *mhi_cntrl);
+int mhi_early_notify_device(struct device *dev, void *data);
+
+#if 0
+static inline u64 __raw_readq_no_log(const volatile void __iomem *addr)
+{
+	u64 val;
+	asm volatile(ALTERNATIVE("ldr %0, [%1]",
+				 "ldar %0, [%1]",
+				 ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE)
+		     : "=r" (val) : "r" (addr));
+	return val;
+}
+
+#define readq_relaxed_no_log(c) \
+	({ u64 __v = le64_to_cpu((__force __le64)__raw_readq_no_log(c)); __v; })
+
+#define readq_no_log(c) \
+		({ u64 __v = readq_relaxed_no_log(c); __iormb(__v); __v; })
+#endif
+
+#ifndef readq
+static inline __u64 readq(const volatile void __iomem *addr)
+{
+	const volatile u32 __iomem *p = addr;
+	u32 low, high;
+
+	high = readl(p + 1);
+	low = readl(p);
+
+	return low + ((u64)high << 32);
+}
+#endif
+
+#ifndef writeq
+static inline void writeq(__u64 val, volatile void __iomem *addr)
+{
+	writel(val >> 32, addr+4);
+	writel(val, addr);
+}
+#endif
+
+/* timesync log support */
+static inline void mhi_timesync_log(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_timesync *mhi_tsync = mhi_cntrl->mhi_tsync;
+
+	if (mhi_tsync && mhi_cntrl->tsync_log)
+		mhi_cntrl->tsync_log(mhi_cntrl,
+				     readq(mhi_tsync->time_reg)); //readq_no_log(mhi_tsync->time_reg));
+}
+
+/* memory allocation methods */
+static inline void *mhi_alloc_coherent(struct mhi_controller *mhi_cntrl,
+				       size_t size,
+				       dma_addr_t *dma_handle,
+				       gfp_t gfp)
+{
+	void *buf = dma_alloc_coherent(mhi_cntrl->dev, size, dma_handle, gfp | __GFP_ZERO);
+
+	if (buf)
+		atomic_add(size, &mhi_cntrl->alloc_size);
+
+	return buf;
+}
+static inline void mhi_free_coherent(struct mhi_controller *mhi_cntrl,
+				     size_t size,
+				     void *vaddr,
+				     dma_addr_t dma_handle)
+{
+	atomic_sub(size, &mhi_cntrl->alloc_size);
+	dma_free_coherent(mhi_cntrl->dev, size, vaddr, dma_handle);
+}
+struct mhi_device *mhi_alloc_device(struct mhi_controller *mhi_cntrl);
+static inline void mhi_dealloc_device(struct mhi_controller *mhi_cntrl,
+				      struct mhi_device *mhi_dev)
+{
+	kfree(mhi_dev);
+}
+int mhi_destroy_device(struct device *dev, void *data);
+void mhi_create_devices(struct mhi_controller *mhi_cntrl);
+int mhi_alloc_bhie_table(struct mhi_controller *mhi_cntrl,
+			 struct image_info **image_info, size_t alloc_size);
+void mhi_free_bhie_table(struct mhi_controller *mhi_cntrl,
+			 struct image_info *image_info);
+
+int mhi_map_single_no_bb(struct mhi_controller *mhi_cntrl,
+			 struct mhi_buf_info *buf_info);
+int mhi_map_single_use_bb(struct mhi_controller *mhi_cntrl,
+			  struct mhi_buf_info *buf_info);
+void mhi_unmap_single_no_bb(struct mhi_controller *mhi_cntrl,
+			    struct mhi_buf_info *buf_info);
+void mhi_unmap_single_use_bb(struct mhi_controller *mhi_cntrl,
+			     struct mhi_buf_info *buf_info);
+
+/* initialization methods */
+int mhi_init_chan_ctxt(struct mhi_controller *mhi_cntrl,
+		       struct mhi_chan *mhi_chan);
+void mhi_deinit_chan_ctxt(struct mhi_controller *mhi_cntrl,
+			  struct mhi_chan *mhi_chan);
+int mhi_init_mmio(struct mhi_controller *mhi_cntrl);
+int mhi_init_dev_ctxt(struct mhi_controller *mhi_cntrl);
+void mhi_deinit_dev_ctxt(struct mhi_controller *mhi_cntrl);
+int mhi_init_irq_setup(struct mhi_controller *mhi_cntrl);
+void mhi_deinit_free_irq(struct mhi_controller *mhi_cntrl);
+int mhi_dtr_init(void);
+int mhi_dtr_deinit(void);
+void mhi_rddm_prepare(struct mhi_controller *mhi_cntrl,
+		      struct image_info *img_info);
+
+/* isr handlers */
+irqreturn_t mhi_msi_handlr(int irq_number, void *dev);
+irqreturn_t mhi_intvec_threaded_handlr(int irq_number, void *dev);
+irqreturn_t mhi_intvec_handlr(int irq_number, void *dev);
+irqreturn_t mhi_msi_handlr_all(int irq_number, void *dev);
+
+void mhi_ev_task(unsigned long data);
+
+void mhi_intvec_task(unsigned long data);
+void mhi_intvec_worker(struct work_struct *work);
+
+#ifdef CONFIG_MHI_DEBUG
+
+#define MHI_ASSERT(cond, msg) do { \
+	if (cond) \
+		panic(msg); \
+} while (0)
+
+#else
+
+#define MHI_ASSERT(cond, msg) do { \
+	if (cond) { \
+		MHI_ERR(msg); \
+		WARN_ON(cond); \
+	} \
+} while (0)
+
+#endif
+
+#endif /* _MHI_INT_H */
diff --git a/drivers/staging/em9190/core/mhi_main.c b/drivers/staging/em9190/core/mhi_main.c
new file mode 100644
index 000000000000..ae468f914b56
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_main.c
@@ -0,0 +1,3178 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include "../inc/mhi.h"
+#include "mhi_internal.h"
+
+static void __mhi_unprepare_channel(struct mhi_controller *mhi_cntrl,
+				    struct mhi_chan *mhi_chan);
+
+/**
+ * @brief read mhi register
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            base					base address
+ * @param[in ]            offset				register offset
+ * @param[out]            out					read result
+ */
+int __must_check mhi_read_reg(struct mhi_controller *mhi_cntrl,
+			      void __iomem *base,
+			      u32 offset,
+			      u32 *out)
+{
+	u32 tmp = readl_relaxed(base + offset);
+
+	/* unexpected value, query the link status */
+	if (PCI_INVALID_READ(tmp) &&
+	    mhi_cntrl->link_status(mhi_cntrl, mhi_cntrl->priv_data))
+		return -EIO;
+
+	*out = tmp;
+
+	return 0;
+}
+
+/**
+ * @brief read mhi register field
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            base					base address
+ * @param[in ]            offset				register offset
+ * @param[in ]            mask					mask
+ * @param[in ]            shift				    shift
+ * @param[out]            out					read result
+ */
+int __must_check mhi_read_reg_field(struct mhi_controller *mhi_cntrl,
+				    void __iomem *base,
+				    u32 offset,
+				    u32 mask,
+				    u32 shift,
+				    u32 *out)
+{
+	u32 tmp;
+	int ret;
+
+	ret = mhi_read_reg(mhi_cntrl, base, offset, &tmp);
+	if (ret)
+		return ret;
+
+	*out = (tmp & mask) >> shift;
+
+	return 0;
+}
+
+/**
+ * @brief get capability offset 
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            capability			capability
+ * @param[out]            offset				register offset
+ */
+int mhi_get_capability_offset(struct mhi_controller *mhi_cntrl,
+			      u32 capability,
+			      u32 *offset)
+{
+	u32 cur_cap, next_offset;
+	int ret;
+
+	/* get the 1st supported capability offset */
+	ret = mhi_read_reg_field(mhi_cntrl, mhi_cntrl->regs, MISC_OFFSET,
+				 MISC_CAP_MASK, MISC_CAP_SHIFT, offset);
+	if (ret)
+		return ret;
+	do {
+		ret = mhi_read_reg_field(mhi_cntrl, mhi_cntrl->regs, *offset,
+					 CAP_CAPID_MASK, CAP_CAPID_SHIFT,
+					 &cur_cap);
+		if (ret)
+			return ret;
+
+		if (cur_cap == capability)
+			return 0;
+
+		ret = mhi_read_reg_field(mhi_cntrl, mhi_cntrl->regs, *offset,
+					 CAP_NEXT_CAP_MASK, CAP_NEXT_CAP_SHIFT,
+					 &next_offset);
+		if (ret)
+			return ret;
+
+		*offset += next_offset;
+	} while (next_offset);
+
+	return -ENXIO;
+}
+
+/**
+ * @brief write mhi register
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            base 					base address
+ * @param[in ]            offset				register offset
+ * @param[in ]            val					value
+ */
+void mhi_write_reg(struct mhi_controller *mhi_cntrl,
+		   void __iomem *base,
+		   u32 offset,
+		   u32 val)
+{
+	writel_relaxed(val, base + offset);
+}
+
+/**
+ * @brief write mhi register field
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            base					base address
+ * @param[in ]            offset				register offset
+ * @param[in ]            mask					mask
+ * @param[in ]            shift				    shift
+ * @param[in ]            val					value
+ */
+void mhi_write_reg_field(struct mhi_controller *mhi_cntrl,
+			 void __iomem *base,
+			 u32 offset,
+			 u32 mask,
+			 u32 shift,
+			 u32 val)
+{
+	int ret;
+	u32 tmp;
+
+	ret = mhi_read_reg(mhi_cntrl, base, offset, &tmp);
+	if (ret)
+		return;
+
+	tmp &= ~mask;
+	tmp |= (val << shift);
+	mhi_write_reg(mhi_cntrl, base, offset, tmp);
+}
+
+/**
+ * @brief write mhi doorbell
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            db_addr				doorbell address
+ * @param[in ]            wp					write pointer
+ */
+void mhi_write_db(struct mhi_controller *mhi_cntrl,
+		  void __iomem *db_addr,
+		  dma_addr_t wp)
+{
+	mhi_write_reg(mhi_cntrl, db_addr, 4, upper_32_bits(wp));
+	mhi_write_reg(mhi_cntrl, db_addr, 0, lower_32_bits(wp));
+}
+
+/**
+ * @brief doorbell burst mode
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            db_cfg				doorbell configuration
+ * @param[in ]            db_addr				doorbell address
+ * @param[in ]            wp					write pointer
+ */
+void mhi_db_brstmode(struct mhi_controller *mhi_cntrl,
+		     struct db_cfg *db_cfg,
+		     void __iomem *db_addr,
+		     dma_addr_t wp)
+{
+    MHI_LOG("db_mode %d\n", db_cfg->db_mode);
+
+	if (db_cfg->db_mode) {
+		db_cfg->db_val = wp;
+        MHI_LOG("mhi_write_db wp 0x%llx\n", (unsigned long long)wp);
+		mhi_write_db(mhi_cntrl, db_addr, wp);
+		db_cfg->db_mode = 0;
+	}
+}
+
+/**
+ * @brief doorbell burst mode disabled
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            db_cfg				doorbell configuration
+ * @param[in ]            db_addr				doorbell address
+ * @param[in ]            wp					write pointer
+ */
+void mhi_db_brstmode_disable(struct mhi_controller *mhi_cntrl,
+			     struct db_cfg *db_cfg,
+			     void __iomem *db_addr,
+			     dma_addr_t wp)
+{
+    MHI_LOG("mhi_write_db wp 0x%llx\n", (unsigned long long)wp);
+	db_cfg->db_val = wp;
+	mhi_write_db(mhi_cntrl, db_addr, wp);
+}
+
+/**
+ * @brief ring event doorbell
+ *
+ * @param[in ]            mhi_event 			mhi event
+ */
+void mhi_ring_er_db(struct mhi_event *mhi_event)
+{
+	struct mhi_ring *ring = &mhi_event->ring;
+
+	mhi_event->db_cfg.process_db(mhi_event->mhi_cntrl, &mhi_event->db_cfg,
+				     ring->db_addr, *ring->ctxt_wp);
+}
+
+/**
+ * @brief ring command doorbell
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            mhi_cmd 				command
+ */
+void mhi_ring_cmd_db(struct mhi_controller *mhi_cntrl, struct mhi_cmd *mhi_cmd)
+{
+	dma_addr_t db;
+	struct mhi_ring *ring = &mhi_cmd->ring;
+
+	db = ring->iommu_base + (ring->wp - ring->base);
+	*ring->ctxt_wp = db;
+	mhi_write_db(mhi_cntrl, ring->db_addr, db);
+}
+
+/**
+ * @brief ring channel doorbell
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            mhi_chan 				channel
+ */
+void mhi_ring_chan_db(struct mhi_controller *mhi_cntrl,
+		      struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *ring = &mhi_chan->tre_ring;
+	dma_addr_t db;
+
+    MHI_LOG("chan %d\n", mhi_chan->chan);
+
+	db = ring->iommu_base + (ring->wp - ring->base);
+	*ring->ctxt_wp = db;
+	mhi_chan->db_cfg.process_db(mhi_cntrl, &mhi_chan->db_cfg, ring->db_addr,
+				    db);
+}
+
+/**
+ * @brief get exec environment index 
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            dev_ee 				device EE
+ */
+static enum mhi_ee mhi_translate_dev_ee(struct mhi_controller *mhi_cntrl,
+					u32 dev_ee)
+{
+	enum mhi_ee i;
+
+	for (i = MHI_EE_PBL; i < MHI_EE_MAX; i++)
+		if (mhi_cntrl->ee_table[i] == dev_ee)
+			return i;
+
+	return MHI_EE_NOT_SUPPORTED;
+}
+
+/**
+ * @brief get exec environment from device
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+enum mhi_ee mhi_get_exec_env(struct mhi_controller *mhi_cntrl)
+{
+	u32 exec;
+	int ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->bhi, BHI_EXECENV, &exec);
+
+	return (ret) ? MHI_EE_MAX : mhi_translate_dev_ee(mhi_cntrl, exec);
+}
+
+/**
+ * @brief get mhi state
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+enum mhi_dev_state mhi_get_mhi_state(struct mhi_controller *mhi_cntrl)
+{
+	u32 state;
+	int ret = mhi_read_reg_field(mhi_cntrl, mhi_cntrl->regs, MHISTATUS,
+				     MHISTATUS_MHISTATE_MASK,
+				     MHISTATUS_MHISTATE_SHIFT, &state);
+	return ret ? MHI_STATE_MAX : state;
+}
+EXPORT_SYMBOL(mhi_get_mhi_state);
+
+/**
+ * @brief queue sclist
+ *
+ * @param[in ]            mhi_dev 				mhi device
+ * @param[in ]            mhi_chan 				channel
+ * @param[in ]            buf 					buffer
+ * @param[in ]            len 					buffer size
+ * @param[in ]            mhi_cntrl 			flags
+ */
+int mhi_queue_sclist(struct mhi_device *mhi_dev,
+		     struct mhi_chan *mhi_chan,
+		     void *buf,
+		     size_t len,
+		     enum MHI_FLAGS mflags)
+{
+	return -EINVAL;
+}
+
+/**
+ * @brief queue sclist
+ *
+ * @param[in ]            mhi_dev 				mhi device
+ * @param[in ]            mhi_chan 				channel
+ * @param[in ]            buf 					buffer
+ * @param[in ]            len 					buffer size
+ * @param[in ]            mhi_cntrl 			flags
+ */
+int mhi_queue_nop(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan,
+		  void *buf,
+		  size_t len,
+		  enum MHI_FLAGS mflags)
+{
+	return -EINVAL;
+}
+
+/**
+ * @brief add an element to ring
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            ring 					ring
+ */
+static void mhi_add_ring_element(struct mhi_controller *mhi_cntrl,
+				 struct mhi_ring *ring)
+{
+	ring->wp += ring->el_size;
+	if (ring->wp >= (ring->base + ring->len))
+		ring->wp = ring->base;
+	/* smp update */
+	smp_wmb();
+}
+
+/**
+ * @brief remove an element from ring
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            ring 					ring
+ */
+static void mhi_del_ring_element(struct mhi_controller *mhi_cntrl,
+				 struct mhi_ring *ring)
+{
+	ring->rp += ring->el_size;
+	if (ring->rp >= (ring->base + ring->len))
+		ring->rp = ring->base;
+	/* smp update */
+	smp_wmb();
+}
+
+/**
+ * @brief get number of available ring elements
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ * @param[in ]            ring 					ring
+ */
+static int get_nr_avail_ring_elements(struct mhi_controller *mhi_cntrl,
+				      struct mhi_ring *ring)
+{
+	int nr_el;
+
+	if (ring->wp < ring->rp)
+		nr_el = ((ring->rp - ring->wp) / ring->el_size) - 1;
+	else {
+		nr_el = (ring->rp - ring->base) / ring->el_size;
+		nr_el += ((ring->base + ring->len - ring->wp) /
+			  ring->el_size) - 1;
+	}
+	return nr_el;
+}
+
+/**
+ * @brief convert physical address to virtual address
+ *
+ * @param[in ]            ring 					ring
+ * @param[in ]            addr 					physical address
+ */
+static void *mhi_to_virtual(struct mhi_ring *ring, dma_addr_t addr)
+{
+	return (addr - ring->iommu_base) + ring->base;
+}
+
+/**
+ * @brief convert virtual address to physical address
+ *
+ * @param[in ]            ring 					ring
+ * @param[in ]            addr 					physical address
+ */
+dma_addr_t mhi_to_physical(struct mhi_ring *ring, void *addr)
+{
+	return (addr - ring->base) + ring->iommu_base;
+}
+
+/**
+ * @brief advance the ring
+ *
+ * @param[in ]            mhi_cntrl					nhi controller
+ * @param[in ]            ring 						ring
+ */
+static void mhi_recycle_ev_ring_element(struct mhi_controller *mhi_cntrl,
+					struct mhi_ring *ring)
+{
+	dma_addr_t ctxt_wp;
+
+	/* update the WP */
+	ring->wp += ring->el_size;
+	ctxt_wp = *ring->ctxt_wp + ring->el_size;
+
+	if (ring->wp >= (ring->base + ring->len)) {
+		ring->wp = ring->base;
+		ctxt_wp = ring->iommu_base;
+	}
+
+	*ring->ctxt_wp = ctxt_wp;
+
+	/* update the RP */
+	ring->rp += ring->el_size;
+	if (ring->rp >= (ring->base + ring->len))
+		ring->rp = ring->base;
+
+	/* visible to other cores */
+	smp_wmb();
+}
+
+/**
+ * @brief check if the ring is full
+ *
+ * @param[in ]            mhi_cntrl					nhi controller
+ * @param[in ]            ring 						ring
+ */
+static bool mhi_is_ring_full(struct mhi_controller *mhi_cntrl,
+			     struct mhi_ring *ring)
+{
+	void *tmp = ring->wp + ring->el_size;
+
+	if (tmp >= (ring->base + ring->len))
+		tmp = ring->base;
+
+	return (tmp == ring->rp);
+}
+
+/**
+ * @brief map DMA buffer
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            buf_info					buffer
+ */
+int mhi_map_single_no_bb(struct mhi_controller *mhi_cntrl,
+			 struct mhi_buf_info *buf_info)
+{
+	if (buf_info->page) { 
+		buf_info->p_addr = dma_map_page(mhi_cntrl->dev, buf_info->page, 0, buf_info->len,
+								buf_info->dir);
+	}							
+	else {
+		buf_info->p_addr = dma_map_single(mhi_cntrl->dev, buf_info->v_addr,
+					  			buf_info->len, buf_info->dir);
+	}
+
+	if (dma_mapping_error(mhi_cntrl->dev, buf_info->p_addr)) {
+		MHI_ERR("map failed!\n");
+		return -ENOMEM;
+	}
+
+	MHI_LOG("map success buf_info 0x%p p_addr 0x%llx len %ld!\n", buf_info, 
+		(unsigned long long)buf_info->p_addr, (long)buf_info->len);
+	
+	return 0;
+}
+
+/**
+ * @brief map DMA buffer
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            buf_info					buffer
+ */
+int mhi_map_single_use_bb(struct mhi_controller *mhi_cntrl,
+			  struct mhi_buf_info *buf_info)
+{
+	void *buf = mhi_alloc_coherent(mhi_cntrl, buf_info->len,
+				       &buf_info->p_addr, GFP_ATOMIC);
+
+	if (!buf)
+		return -ENOMEM;
+
+	if (buf_info->dir == DMA_TO_DEVICE)
+		memcpy(buf, buf_info->v_addr, buf_info->len);
+
+	buf_info->bb_addr = buf;
+
+    MHI_LOG("map success buf_info 0x%p p_addr 0x%llx len %ld!\n", buf_info, 
+		(unsigned long long)buf_info->p_addr, (long)buf_info->len);
+
+	return 0;
+}
+
+/**
+ * @brief unmap DMA buffer
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            buf_info					buffer
+ */
+void mhi_unmap_single_no_bb(struct mhi_controller *mhi_cntrl,
+			    struct mhi_buf_info *buf_info)
+{
+	if (buf_info->page) {
+		dma_unmap_page(mhi_cntrl->dev, buf_info->p_addr, buf_info->len,
+			 buf_info->dir);
+	}		 
+	else {	
+		dma_unmap_single(mhi_cntrl->dev, buf_info->p_addr, buf_info->len,
+			 buf_info->dir);
+	}
+	
+    MHI_LOG("unmap buf_info 0x%p p_addr 0x%llx len %ld!\n", buf_info, 
+		(unsigned long long)buf_info->p_addr, (long)buf_info->len);
+}
+
+/**
+ * @brief unmap DMA buffer
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            buf_info					buffer 
+ */
+void mhi_unmap_single_use_bb(struct mhi_controller *mhi_cntrl,
+			    struct mhi_buf_info *buf_info)
+{
+	if (buf_info->dir == DMA_FROM_DEVICE)
+		memcpy(buf_info->v_addr, buf_info->bb_addr, buf_info->len);
+
+	mhi_free_coherent(mhi_cntrl, buf_info->len, buf_info->bb_addr,
+			  buf_info->p_addr);
+
+    MHI_LOG("unmap buf_info 0x%p p_addr 0x%llx len %ld!\n", buf_info, (unsigned long long)buf_info->p_addr, (long)buf_info->len);
+}
+
+/**
+ * @brief queue data packet for sending
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            mhi_chan					mhi channel
+ * @param[in ]            buf						buffer
+ * @param[in ]            len						buffer size
+ * @param[in ]            mflags					flags
+ */
+int mhi_queue_skb(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan,
+		  void *buf,
+		  size_t len,
+		  enum MHI_FLAGS mflags)
+{
+	struct sk_buff *skb = buf;
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+	struct mhi_ring *buf_ring = &mhi_chan->buf_ring;
+	struct mhi_buf_info *buf_info;
+	struct mhi_tre *mhi_tre;
+	int ret;
+
+    spin_lock(&mhi_chan->skb_write_lock);
+
+    MHI_LOG("skb %p len %ld", skb, (long)len);
+
+	if (mhi_is_ring_full(mhi_cntrl, tre_ring)) {
+        spin_unlock(&mhi_chan->skb_write_lock);
+		return -ENOMEM;
+    }
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_VERB("MHI is not in activate state, pm_state:%s\n",
+			 to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+
+        spin_unlock(&mhi_chan->skb_write_lock);
+		return -EIO;
+	}
+
+	/* we're in M3 or transitioning to M3 */
+	if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	}
+
+	/* toggle wake to exit out of M2 */
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+
+	/* generate the tre */
+	buf_info = buf_ring->wp;
+	buf_info->v_addr = skb->data;
+	buf_info->cb_buf = skb;
+	buf_info->wp = tre_ring->wp;
+	buf_info->dir = mhi_chan->dir;
+	buf_info->len = len;
+    buf_info->pre_mapped = false;
+	buf_info->page = NULL;
+	ret = mhi_cntrl->map_single(mhi_cntrl, buf_info);
+	if (ret)
+		goto map_error;
+
+	mhi_tre = tre_ring->wp;
+
+	mhi_tre->ptr = MHI_TRE_DATA_PTR(buf_info->p_addr);
+	mhi_tre->dword[0] = MHI_TRE_DATA_DWORD0(buf_info->len);
+	mhi_tre->dword[1] = MHI_TRE_DATA_DWORD1(1, 1, 0, 0);	/* BEI = 1 IEOT = 1 */
+
+	MHI_LOG("pre_mapped %d\n", buf_info->pre_mapped);
+
+	MHI_LOG("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	MHI_VERB("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	/* increment WP */
+	mhi_add_ring_element(mhi_cntrl, tre_ring);
+	mhi_add_ring_element(mhi_cntrl, buf_ring);
+
+	if (mhi_chan->dir == DMA_TO_DEVICE)
+		atomic_inc(&mhi_cntrl->pending_pkts);
+
+	MHI_LOG("pm_state:%d db_access:%d\n", mhi_cntrl->pm_state, 
+        mhi_cntrl->db_access);
+
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl))) {
+		read_lock_bh(&mhi_chan->lock);
+	    
+        MHI_LOG("chan:%d ring chan db\n", mhi_chan->chan);
+
+        mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		read_unlock_bh(&mhi_chan->lock);
+	}
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+    MHI_LOG("return 0\n");
+    spin_unlock(&mhi_chan->skb_write_lock);
+	return 0;
+
+map_error:
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+    spin_unlock(&mhi_chan->skb_write_lock);
+	return ret;
+}
+
+/**
+ * @brief queue DMA request
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            mhi_chan					mhi channel
+ * @param[in ]            buf						buffer
+ * @param[in ]            len						buffer size
+ * @param[in ]            mflags					flags
+ */
+int mhi_queue_dma(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan,
+		  void *buf,
+		  size_t len,
+		  enum MHI_FLAGS mflags)
+{
+	struct mhi_buf *mhi_buf = buf;
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+	struct mhi_ring *buf_ring = &mhi_chan->buf_ring;
+	struct mhi_buf_info *buf_info;
+	struct mhi_tre *mhi_tre;
+
+    spin_lock(&mhi_chan->dma_write_lock);
+
+	if (mhi_is_ring_full(mhi_cntrl, tre_ring)) {
+
+        spin_unlock(&mhi_chan->dma_write_lock);
+
+		return -ENOMEM;
+    }
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_VERB("MHI is not in activate state, pm_state:%s\n",
+			 to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+        spin_unlock(&mhi_chan->dma_write_lock);
+
+		return -EIO;
+	}
+
+	/* we're in M3 or transitioning to M3 */
+	if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	}
+
+	/* toggle wake to exit out of M2 */
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+
+	/* generate the tre */
+	buf_info = buf_ring->wp;
+	MHI_ASSERT(buf_info->used, "TRE Not Freed\n");
+	buf_info->p_addr = mhi_buf->dma_addr;
+	buf_info->pre_mapped = true;
+	buf_info->cb_buf = mhi_buf;
+	buf_info->wp = tre_ring->wp;
+	buf_info->dir = mhi_chan->dir;
+	buf_info->len = len;
+	buf_info->page = NULL;
+
+	mhi_tre = tre_ring->wp;
+
+	if (mhi_chan->xfer_type == MHI_XFER_RSC_DMA) {
+		buf_info->used = true;
+		mhi_tre->ptr =
+			MHI_RSCTRE_DATA_PTR(buf_info->p_addr, buf_info->len);
+		mhi_tre->dword[0] =
+			MHI_RSCTRE_DATA_DWORD0(buf_ring->wp - buf_ring->base);
+		mhi_tre->dword[1] = MHI_RSCTRE_DATA_DWORD1;
+	} else {
+		mhi_tre->ptr = MHI_TRE_DATA_PTR(buf_info->p_addr);
+		mhi_tre->dword[0] = MHI_TRE_DATA_DWORD0(buf_info->len);
+		mhi_tre->dword[1] = MHI_TRE_DATA_DWORD1(1, 1, 0, 0);    /* BEI = 1 IEOT = 1 */
+	}
+
+	MHI_LOG("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	MHI_VERB("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	/* increment WP */
+	mhi_add_ring_element(mhi_cntrl, tre_ring);
+	mhi_add_ring_element(mhi_cntrl, buf_ring);
+
+	if (mhi_chan->dir == DMA_TO_DEVICE)
+		atomic_inc(&mhi_cntrl->pending_pkts);
+
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl))) {
+		read_lock_bh(&mhi_chan->lock);
+		mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		read_unlock_bh(&mhi_chan->lock);
+	}
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+    spin_unlock(&mhi_chan->dma_write_lock);
+
+	return 0;
+}
+
+/**
+ * @brief check if QMAP channel is full
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            mhi_chan					mhi channel
+ */
+bool mhi_qmap_full(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+	int ret = false;
+
+    spin_lock(&mhi_chan->qmap_write_lock);
+
+	ret = mhi_is_ring_full(mhi_cntrl, tre_ring);
+
+    spin_unlock(&mhi_chan->qmap_write_lock);
+
+	return ret;
+}
+
+/**
+ * @brief queue QMAP request
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            mhi_chan					mhi channel
+ * @param[in ]            buf						buffer
+ * @param[in ]            len						buffer size
+ * @param[in ]            mflags					flags
+ */
+int mhi_queue_qmap(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan,
+		  void *buf,
+		  size_t len,
+		  enum MHI_FLAGS mflags)
+{
+	struct mhi_buf *mhi_buf = buf;
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+	struct mhi_ring *buf_ring = &mhi_chan->buf_ring;
+	struct mhi_buf_info *buf_info;
+	struct mhi_tre *mhi_tre;
+	int ret = 0;
+
+    spin_lock(&mhi_chan->qmap_write_lock);
+
+	if (mhi_is_ring_full(mhi_cntrl, tre_ring)) {
+
+        spin_unlock(&mhi_chan->qmap_write_lock);
+
+		return -ENOMEM;
+    }
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_VERB("MHI is not in activate state, pm_state:%s\n",
+			 to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+        spin_unlock(&mhi_chan->qmap_write_lock);
+
+		return -EIO;
+	}
+
+	/* we're in M3 or transitioning to M3 */
+	if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	}
+
+	/* toggle wake to exit out of M2 */
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+
+	/* generate the tre */
+	buf_info = buf_ring->wp;
+	MHI_ASSERT(buf_info->used, "TRE Not Freed\n");
+	buf_info->v_addr = mhi_buf->buf;
+	buf_info->pre_mapped = false;
+	buf_info->cb_buf = mhi_buf;
+	buf_info->wp = tre_ring->wp;
+	buf_info->dir = mhi_chan->dir;
+	buf_info->len = len;
+	buf_info->page = mhi_buf->page;
+	ret = mhi_cntrl->map_single(mhi_cntrl, buf_info);
+	if (ret)
+		goto map_error;
+
+    MHI_LOG("map success buf_info 0x%p p_addr 0x%llx len %ld!\n", buf_info, 
+		(unsigned long long)buf_info->p_addr, (long)buf_info->len);
+
+	mhi_tre = tre_ring->wp;
+
+	mhi_tre->ptr = MHI_TRE_DATA_PTR(buf_info->p_addr);
+	mhi_tre->dword[0] = MHI_TRE_DATA_DWORD0(buf_info->len);
+	mhi_tre->dword[1] = MHI_TRE_DATA_DWORD1(1, 1, 0, 0);	/* BEI = 1 IEOT = 1 */
+
+	MHI_LOG("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	MHI_VERB("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	/* increment WP */
+	mhi_add_ring_element(mhi_cntrl, tre_ring);
+	mhi_add_ring_element(mhi_cntrl, buf_ring);
+
+	if (mhi_chan->dir == DMA_TO_DEVICE)
+		atomic_inc(&mhi_cntrl->pending_pkts);
+
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl))) {
+		read_lock_bh(&mhi_chan->lock);
+		mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		read_unlock_bh(&mhi_chan->lock);
+	}
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+    spin_unlock(&mhi_chan->qmap_write_lock);
+
+	return 0;
+
+map_error:
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+    spin_unlock(&mhi_chan->qmap_write_lock);
+	return ret;
+}
+
+/**
+ * @brief generate block for sending
+ *
+ * @param[in ]            mhi_cntrl					nhi controller
+ * @param[in ]            mhi_chan					mhi channel
+ * @param[in ]            buf						buffer to send
+ * @param[in ]            cb						call back fn
+ * @param[in ]            buf_len					buffer size
+ * @param[in ]            mflags					flags
+ */
+int mhi_gen_tre(struct mhi_controller *mhi_cntrl,
+		struct mhi_chan *mhi_chan,
+		void *buf,
+		void *cb,
+		size_t buf_len,
+		enum MHI_FLAGS flags)
+{
+	struct mhi_ring *buf_ring, *tre_ring;
+	struct mhi_tre *mhi_tre;
+	struct mhi_buf_info *buf_info;
+	int eot, eob, chain, bei;
+	int ret;
+	bool lock = false;
+
+	if ((mhi_chan->xfer_type == MHI_XFER_BUFFER) && (mhi_chan->dir == DMA_FROM_DEVICE))
+		lock = true;
+
+	if (lock)
+		spin_lock_bh(&mhi_chan->tre_lock);
+
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+
+	buf_info = buf_ring->wp;
+	buf_info->v_addr = buf;
+	buf_info->cb_buf = cb;
+	buf_info->wp = tre_ring->wp;
+	buf_info->dir = mhi_chan->dir;
+	buf_info->len = buf_len;
+	buf_info->page = NULL;
+	ret = mhi_cntrl->map_single(mhi_cntrl, buf_info);
+	if (ret) {
+		if (lock)
+			spin_unlock_bh(&mhi_chan->tre_lock);
+
+		return ret;
+	}
+
+	eob = !!(flags & MHI_EOB);
+	eot = !!(flags & MHI_EOT);
+	chain = !!(flags & MHI_CHAIN);
+	bei = !!(mhi_chan->intmod);
+
+	mhi_tre = tre_ring->wp;
+	mhi_tre->ptr = MHI_TRE_DATA_PTR(buf_info->p_addr);
+	mhi_tre->dword[0] = MHI_TRE_DATA_DWORD0(buf_len);
+	mhi_tre->dword[1] = MHI_TRE_DATA_DWORD1(bei, eot, eob, chain);
+
+	MHI_VERB("chan:%d WP:0x%llx TRE:0x%llx 0x%08x 0x%08x\n", mhi_chan->chan,
+		 (u64)mhi_to_physical(tre_ring, mhi_tre), mhi_tre->ptr,
+		 mhi_tre->dword[0], mhi_tre->dword[1]);
+
+	/* increment WP */
+	mhi_add_ring_element(mhi_cntrl, tre_ring);
+	mhi_add_ring_element(mhi_cntrl, buf_ring);
+
+	if (lock)
+		spin_unlock_bh(&mhi_chan->tre_lock);
+
+	return 0;
+}
+
+/**
+ * @brief queue the buffer for sending
+ *
+ * @param[in ]            mhi_dev					nhi device
+ * @param[in ]            mhi_chan					mhi channel
+ * @param[in ]            buf						buffer to send
+ * @param[in ]            len						buffer size
+ * @param[in ]            mflags					flags
+ */
+int mhi_queue_buf(struct mhi_device *mhi_dev,
+		  struct mhi_chan *mhi_chan,
+		  void *buf,
+		  size_t len,
+		  enum MHI_FLAGS mflags)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_ring *tre_ring;
+	unsigned long flags;
+	int ret;
+
+	/*
+	 * this check here only as a guard, it's always
+	 * possible mhi can enter error while executing rest of function,
+	 * which is not fatal so we do not need to hold pm_lock
+	 */
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_VERB("MHI is not in active state, pm_state:%s\n",
+			 to_mhi_pm_state_str(mhi_cntrl->pm_state));
+
+		return -EIO;
+	}
+
+	tre_ring = &mhi_chan->tre_ring;
+	if (mhi_is_ring_full(mhi_cntrl, tre_ring))
+		return -ENOMEM;
+
+	ret = mhi_chan->gen_tre(mhi_cntrl, mhi_chan, buf, buf, len, mflags);
+	if (unlikely(ret))
+		return ret;
+
+	read_lock_irqsave(&mhi_cntrl->pm_lock, flags);
+
+	/* we're in M3 or transitioning to M3 */
+	if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	}
+
+	/* toggle wake to exit out of M2 */
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+
+	if (mhi_chan->dir == DMA_TO_DEVICE)
+		atomic_inc(&mhi_cntrl->pending_pkts);
+
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl))) {
+		unsigned long flags;
+
+		read_lock_irqsave(&mhi_chan->lock, flags);
+		mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		read_unlock_irqrestore(&mhi_chan->lock, flags);
+	}
+
+	read_unlock_irqrestore(&mhi_cntrl->pm_lock, flags);
+
+	return 0;
+}
+
+/**
+ * @brief destroy specific device
+ *
+ * @param[in ]            dev						device
+ * @param[in ]            data						context
+ */
+int mhi_destroy_device(struct device *dev, void *data)
+{
+	struct mhi_device *mhi_dev;
+	struct mhi_controller *mhi_cntrl;
+
+	if (dev->bus != &mhi_bus_type)
+		return 0;
+
+	mhi_dev = to_mhi_device(dev);
+	mhi_cntrl = mhi_dev->mhi_cntrl;
+
+	/* only destroying virtual devices thats attached to bus */
+	if (mhi_dev->dev_type ==  MHI_CONTROLLER_TYPE)
+		return 0;
+
+	MHI_LOG("destroy device for chan:%s\n", mhi_dev->chan_name);
+
+    MHI_DEFAULT("device del %04x_%02u.%02u.%02u_%s",
+			     mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus,
+			     mhi_dev->slot, mhi_dev->chan_name);
+
+	/* notify the client and remove the device from mhi bus */
+	device_del(dev);
+	put_device(dev);
+
+	return 0;
+}
+
+/**
+ * @brief early notify the device
+ *
+ * @param[in ]            dev						device
+ * @param[in ]            data						context
+ */
+int mhi_early_notify_device(struct device *dev, void *data)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+
+	/* skip early notification */
+	if (!mhi_dev->early_notif)
+		return 0;
+
+	MHI_LOG("Early notification for dev:%s\n", mhi_dev->chan_name);
+
+	mhi_notify(mhi_dev, MHI_CB_FATAL_ERROR);
+
+	return 0;
+}
+
+/**
+ * @brief notify the device for status change
+ *
+ * @param[in ]            mhi_dev					mhi	device
+ * @param[in ]            cb_reason					callback reason
+ */
+void mhi_notify(struct mhi_device *mhi_dev, enum MHI_CB cb_reason)
+{
+	struct mhi_driver *mhi_drv;
+
+	if (!mhi_dev->dev.driver)
+		return;
+
+	mhi_drv = to_mhi_driver(mhi_dev->dev.driver);
+
+	if (mhi_drv->status_cb)
+		mhi_drv->status_cb(mhi_dev, cb_reason);
+}
+
+/**
+ * @brief assign of node
+ *
+ * @param[in ]            mhi_cntrl					mhi	controller
+ * @param[in ]            mhi_dev					mhi	device
+ */
+static void mhi_assign_of_node(struct mhi_controller *mhi_cntrl,
+			       struct mhi_device *mhi_dev)
+{
+	struct device_node *controller, *node;
+	const char *dt_name;
+	int ret;
+
+	controller = of_find_node_by_name(mhi_cntrl->of_node, "mhi_devices");
+	if (!controller)
+		return;
+
+	for_each_available_child_of_node(controller, node) {
+		ret = of_property_read_string(node, "mhi,chan", &dt_name);
+		if (ret)
+			continue;
+		if (!strcmp(mhi_dev->chan_name, dt_name)) {
+			mhi_dev->dev.of_node = node;
+			break;
+		}
+	}
+}
+
+/**
+ * @brief print local and remote time to a buffer
+ *
+ * @param[in ]            dev						device
+ * @param[in ]            attr						device attribute
+ * @param[out]            buf						buffer
+ */
+static ssize_t time_show(struct device *dev,
+			 struct device_attribute *attr,
+			 char *buf)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	u64 t_host, t_device;
+	int ret;
+
+	ret = mhi_get_remote_time_sync(mhi_dev, &t_host, &t_device);
+	if (ret) {
+		MHI_ERR("Failed to obtain time, ret:%d\n", ret);
+		return ret;
+	}
+
+	return scnprintf(buf, PAGE_SIZE, "local: %llu remote: %llu (ticks)\n",
+			 t_host, t_device);
+}
+static DEVICE_ATTR_RO(time);
+
+/**
+ * @brief print local and remote time to a buffer
+ *
+ * @param[in ]            dev						device
+ * @param[in ]            attr						device attribute
+ * @param[out]            buf						buffer
+ */
+static ssize_t time_us_show(struct device *dev,
+			    struct device_attribute *attr,
+			    char *buf)
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	u64 t_host, t_device;
+	int ret;
+
+	ret = mhi_get_remote_time_sync(mhi_dev, &t_host, &t_device);
+	if (ret) {
+		MHI_ERR("Failed to obtain time, ret:%d\n", ret);
+		return ret;
+	}
+
+	return scnprintf(buf, PAGE_SIZE, "local: %llu remote: %llu (us)\n",
+			 LOCAL_TICKS_TO_US(t_host),
+			 REMOTE_TICKS_TO_US(t_device));
+}
+static DEVICE_ATTR_RO(time_us);
+
+static struct attribute *mhi_tsync_attrs[] = {
+	&dev_attr_time.attr,
+	&dev_attr_time_us.attr,
+	NULL,
+};
+
+static const struct attribute_group mhi_tsync_group = {
+	.attrs = mhi_tsync_attrs,
+};
+
+/**
+ * @brief destroy time sync sysfs
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+void mhi_destroy_timesync(struct mhi_controller *mhi_cntrl)
+{
+	if (mhi_cntrl->mhi_tsync) {
+		sysfs_remove_group(&mhi_cntrl->mhi_dev->dev.kobj,
+				   &mhi_tsync_group);
+		kfree(mhi_cntrl->mhi_tsync);
+		mhi_cntrl->mhi_tsync = NULL;
+	}
+}
+
+/**
+ * @brief create time sync sysfs
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+int mhi_create_timesync_sysfs(struct mhi_controller *mhi_cntrl)
+{
+	return sysfs_create_group(&mhi_cntrl->mhi_dev->dev.kobj,
+				  &mhi_tsync_group);
+}
+
+/**
+ * @brief create time sync device
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+static void mhi_create_time_sync_dev(struct mhi_controller *mhi_cntrl)
+{
+	struct mhi_device *mhi_dev;
+	int ret;
+
+	if (!MHI_IN_MISSION_MODE(mhi_cntrl->ee))
+		return;
+
+	mhi_dev = mhi_alloc_device(mhi_cntrl);
+	if (!mhi_dev)
+		return;
+
+	mhi_dev->dev_type = MHI_TIMESYNC_TYPE;
+	mhi_dev->chan_name = "TIME_SYNC";
+	dev_set_name(&mhi_dev->dev, "%04x_%02u.%02u.%02u_%s", mhi_dev->dev_id,
+		     mhi_dev->domain, mhi_dev->bus, mhi_dev->slot,
+		     mhi_dev->chan_name);
+
+	/* add if there is a matching DT node */
+	mhi_assign_of_node(mhi_cntrl, mhi_dev);
+
+    MHI_DEFAULT("device add %04x_%02u.%02u.%02u_%s",
+                mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus,
+                mhi_dev->slot, mhi_dev->chan_name);
+
+    ret = device_add(&mhi_dev->dev);
+	if (ret) {
+		MHI_ERR("Failed to register dev for  chan:%s\n",
+			mhi_dev->chan_name);
+		mhi_dealloc_device(mhi_cntrl, mhi_dev);
+		return;
+	}
+
+	mhi_cntrl->tsync_dev = mhi_dev;
+}
+
+/**
+ * @brief bind mhi channels into mhi devices
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+void mhi_create_devices(struct mhi_controller *mhi_cntrl)
+{
+	int i;
+	struct mhi_chan *mhi_chan;
+	struct mhi_device *mhi_dev;
+	int ret;
+
+	/*
+	 * we need to create time sync device before creating other
+	 * devices, because client may try to capture time during
+	 * clint probe.
+	 */
+	mhi_create_time_sync_dev(mhi_cntrl);
+
+	MHI_LOG("max_chan %d", mhi_cntrl->max_chan);
+
+	mhi_chan = mhi_cntrl->mhi_chan;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {
+		if (!mhi_chan->configured || mhi_chan->mhi_dev ||
+		    !(mhi_chan->ee_mask & BIT(mhi_cntrl->ee)))
+			continue;
+		mhi_dev = mhi_alloc_device(mhi_cntrl);
+		if (!mhi_dev) {
+			MHI_ERR("mhi_alloc_device failed");
+        	return;
+        }
+	    
+        MHI_LOG("chan %d mhi_alloc_device is OK!", i);
+
+		mhi_dev->dev_type = MHI_XFER_TYPE;
+		switch (mhi_chan->dir) {
+		case DMA_TO_DEVICE:
+			mhi_dev->ul_chan = mhi_chan;
+			mhi_dev->ul_chan_id = mhi_chan->chan;
+			mhi_dev->ul_xfer = mhi_chan->queue_xfer;
+			mhi_dev->ul_event_id = mhi_chan->er_index;
+			mhi_dev->ul_full = mhi_chan->queue_full;
+			break;
+		case DMA_NONE:
+		case DMA_BIDIRECTIONAL:
+			mhi_dev->ul_chan_id = mhi_chan->chan;
+			mhi_dev->ul_event_id = mhi_chan->er_index;
+			goto from;
+		case DMA_FROM_DEVICE:
+from:	
+			/* we use dl_chan for offload channels */
+			mhi_dev->dl_chan = mhi_chan;
+			mhi_dev->dl_chan_id = mhi_chan->chan;
+			mhi_dev->dl_xfer = mhi_chan->queue_xfer;
+			mhi_dev->dl_event_id = mhi_chan->er_index;
+		}
+
+		mhi_chan->mhi_dev = mhi_dev;
+
+		/* check next channel if it matches */
+		if ((i + 1) < mhi_cntrl->max_chan && mhi_chan[1].configured) {
+			if (!strcmp(mhi_chan[1].name, mhi_chan->name)) {
+				i++;
+				mhi_chan++;
+				if (mhi_chan->dir == DMA_TO_DEVICE) {
+					mhi_dev->ul_chan = mhi_chan;
+					mhi_dev->ul_chan_id = mhi_chan->chan;
+					mhi_dev->ul_xfer = mhi_chan->queue_xfer;
+					mhi_dev->ul_event_id =
+						mhi_chan->er_index;
+				} else {
+					mhi_dev->dl_chan = mhi_chan;
+					mhi_dev->dl_chan_id = mhi_chan->chan;
+					mhi_dev->dl_xfer = mhi_chan->queue_xfer;
+					mhi_dev->dl_event_id =
+						mhi_chan->er_index;
+				}
+				mhi_chan->mhi_dev = mhi_dev;
+			}
+		}
+
+		mhi_dev->chan_name = mhi_chan->name;
+		dev_set_name(&mhi_dev->dev, "%04x_%02u.%02u.%02u_%s",
+			     mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus,
+			     mhi_dev->slot, mhi_dev->chan_name);
+
+		/* add if there is a matching DT node */
+		mhi_assign_of_node(mhi_cntrl, mhi_dev);
+
+		/*
+		 * if set, these device should get a early notification during
+		 * early notification state
+		 */
+		mhi_dev->early_notif =
+			of_property_read_bool(mhi_dev->dev.of_node,
+					      "mhi,early-notify");
+		/* init wake source */
+		if (mhi_dev->dl_chan && mhi_dev->dl_chan->wake_capable)
+			device_init_wakeup(&mhi_dev->dev, true);
+
+        MHI_LOG("chan %d device_add", i);
+
+        MHI_DEFAULT("chan %d device add %04x_%02u.%02u.%02u_%s", i,
+			     mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus,
+			     mhi_dev->slot, mhi_dev->chan_name);
+
+		ret = device_add(&mhi_dev->dev);
+		if (ret) {
+			MHI_ERR("Failed to register dev for  chan:%s\n",
+				mhi_dev->chan_name);
+			mhi_dealloc_device(mhi_cntrl, mhi_dev);
+		}
+	}
+}
+
+/**
+ * @brief transfer event handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            event						event
+ * @param[in ]            mhi_chan					mhi channel
+ */
+static int parse_xfer_event(struct mhi_controller *mhi_cntrl,
+			    struct mhi_tre *event,
+			    struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring, *tre_ring;
+	u32 ev_code;
+	struct mhi_result result;
+	unsigned long flags = 0;
+
+	ev_code = MHI_TRE_GET_EV_CODE(event);
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+
+	result.transaction_status = (ev_code == MHI_EV_CC_OVERFLOW) ?
+		-EOVERFLOW : 0;
+
+	/*
+	 * if it's a DB Event then we need to grab the lock
+	 * with preemption disable and as a write because we
+	 * have to update db register and another thread could
+	 * be doing same.
+	 */
+	if (ev_code >= MHI_EV_CC_OOB)
+		write_lock_irqsave(&mhi_chan->lock, flags);
+	else
+		read_lock_bh(&mhi_chan->lock);
+
+	if (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)
+		goto end_process_tx_event;
+
+    MHI_LOG("ev_code %d\n", ev_code);
+
+	switch (ev_code) {
+	case MHI_EV_CC_OVERFLOW:
+	case MHI_EV_CC_EOB:
+	case MHI_EV_CC_EOT:
+	{
+		dma_addr_t ptr = MHI_TRE_GET_EV_PTR(event);
+		struct mhi_tre *local_rp, *ev_tre;
+		void *dev_rp;
+		struct mhi_buf_info *buf_info;
+		u16 xfer_len;
+
+		/* Get the TRB this event points to */
+		ev_tre = mhi_to_virtual(tre_ring, ptr);
+
+		/* device rp after servicing the TREs */
+		dev_rp = ev_tre + 1;
+		if (dev_rp >= (tre_ring->base + tre_ring->len))
+			dev_rp = tre_ring->base;
+
+		result.dir = mhi_chan->dir;
+
+		/* local rp */
+		local_rp = tre_ring->rp;
+		while (local_rp != dev_rp) {
+			buf_info = buf_ring->rp;
+			/* if it's last tre get len from the event */
+			if (local_rp == ev_tre)
+				xfer_len = MHI_TRE_GET_EV_LEN(event);
+			else
+				xfer_len = buf_info->len;
+
+			/* unmap if it's not premapped by client */
+			if (likely(!buf_info->pre_mapped))
+				mhi_cntrl->unmap_single(mhi_cntrl, buf_info);
+
+			result.buf_addr = buf_info->cb_buf;
+			result.bytes_xferd = xfer_len;
+			mhi_del_ring_element(mhi_cntrl, buf_ring);
+			mhi_del_ring_element(mhi_cntrl, tre_ring);
+			local_rp = tre_ring->rp;
+
+			/* notify client */
+            MHI_LOG("xfer_cb transferred %ld\n", (long)result.bytes_xferd);
+
+			mhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);
+
+			if (mhi_chan->dir == DMA_TO_DEVICE)
+				atomic_dec(&mhi_cntrl->pending_pkts);
+
+			/*
+			 * recycle the buffer if buffer is pre-allocated,
+			 * if there is error, not much we can do apart from
+			 * dropping the packet
+			 */
+			if (mhi_chan->pre_alloc) {
+				if (mhi_queue_buf(mhi_chan->mhi_dev, mhi_chan,
+						  buf_info->cb_buf,
+						  buf_info->len, MHI_EOT)) {
+					MHI_ERR(
+						"Error recycling buffer for chan:%d\n",
+						mhi_chan->chan);
+					kfree(buf_info->cb_buf);
+				}
+			}
+		};
+		break;
+	} /* CC_EOT */
+	case MHI_EV_CC_OOB:
+	case MHI_EV_CC_DB_MODE:
+	{
+		unsigned long flags;
+
+		MHI_VERB("DB_MODE/OOB Detected chan %d.\n", mhi_chan->chan);
+		MHI_LOG("DB_MODE/OOB Detected chan %d.\n", mhi_chan->chan);
+		mhi_chan->db_cfg.db_mode = 1;
+		read_lock_irqsave(&mhi_cntrl->pm_lock, flags);
+		MHI_LOG("wp %p rp %p\n", tre_ring->wp, tre_ring->rp);
+		if (tre_ring->wp != tre_ring->rp &&
+		    MHI_DB_ACCESS_VALID(mhi_cntrl)) {
+		    MHI_LOG("mhi_ring_chan_db\n");
+			mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		}
+		read_unlock_irqrestore(&mhi_cntrl->pm_lock, flags);
+		break;
+	}
+	case MHI_EV_CC_BAD_TRE:
+		MHI_ASSERT(1, "Received BAD TRE event for ring");
+		break;
+	default:
+		MHI_CRITICAL("Unknown TX completion.\n");
+
+		break;
+	} /* switch(MHI_EV_READ_CODE(EV_TRB_CODE,event)) */
+
+end_process_tx_event:
+	if (ev_code >= MHI_EV_CC_OOB)
+		write_unlock_irqrestore(&mhi_chan->lock, flags);
+	else
+		read_unlock_bh(&mhi_chan->lock);
+
+	return 0;
+}
+
+/**
+ * @brief rsc event handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            tre						event
+ * @param[in ]            mhi_chan					mhi channel
+ */
+static int parse_rsc_event(struct mhi_controller *mhi_cntrl,
+			   struct mhi_tre *event,
+			   struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring, *tre_ring;
+	struct mhi_buf_info *buf_info;
+	struct mhi_result result;
+	int ev_code;
+	u32 cookie; /* offset to local descriptor */
+	u16 xfer_len;
+
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+
+	ev_code = MHI_TRE_GET_EV_CODE(event);
+	cookie = MHI_TRE_GET_EV_COOKIE(event);
+	xfer_len = MHI_TRE_GET_EV_LEN(event);
+
+	/* received out of bound cookie */
+	MHI_ASSERT(cookie >= buf_ring->len, "Invalid Cookie\n");
+
+	buf_info = buf_ring->base + cookie;
+
+	result.transaction_status = (ev_code == MHI_EV_CC_OVERFLOW) ?
+		-EOVERFLOW : 0;
+	result.bytes_xferd = xfer_len;
+	result.buf_addr = buf_info->cb_buf;
+	result.dir = mhi_chan->dir;
+
+	read_lock_bh(&mhi_chan->lock);
+
+	if (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)
+		goto end_process_rsc_event;
+
+	MHI_ASSERT(!buf_info->used, "TRE already Freed\n");
+
+	/* notify the client */
+	mhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);
+
+	/*
+	 * Note: We're arbitrarily incrementing RP even though, completion
+	 * packet we processed might not be the same one, reason we can do this
+	 * is because device guaranteed to cache descriptors in order it
+	 * receive, so even though completion event is different we can re-use
+	 * all descriptors in between.
+	 * Example:
+	 * Transfer Ring has descriptors: A, B, C, D
+	 * Last descriptor host queue is D (WP) and first descriptor
+	 * host queue is A (RP).
+	 * The completion event we just serviced is descriptor C.
+	 * Then we can safely queue descriptors to replace A, B, and C
+	 * even though host did not receive any completions.
+	 */
+	mhi_del_ring_element(mhi_cntrl, tre_ring);
+	buf_info->used = false;
+
+end_process_rsc_event:
+	read_unlock_bh(&mhi_chan->lock);
+
+	return 0;
+}
+
+/**
+ * @brief command completion handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            tre						request
+ */
+static void mhi_process_cmd_completion(struct mhi_controller *mhi_cntrl,
+				       struct mhi_tre *tre)
+{
+	dma_addr_t ptr = MHI_TRE_GET_EV_PTR(tre);
+	struct mhi_cmd *cmd_ring = &mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING];
+	struct mhi_ring *mhi_ring = &cmd_ring->ring;
+	struct mhi_tre *cmd_pkt;
+	struct mhi_chan *mhi_chan;
+	struct mhi_timesync *mhi_tsync;
+	enum mhi_cmd_type type;
+	u32 chan;
+
+	cmd_pkt = mhi_to_virtual(mhi_ring, ptr);
+
+	/* out of order completion received */
+	// MHI_ASSERT(cmd_pkt != mhi_ring->rp, "Out of order cmd completion");
+
+	type = MHI_TRE_GET_CMD_TYPE(cmd_pkt);
+
+	if (type == MHI_CMD_TYPE_TSYNC) {
+		mhi_tsync = mhi_cntrl->mhi_tsync;
+		mhi_tsync->ccs = MHI_TRE_GET_EV_CODE(tre);
+		complete(&mhi_tsync->completion);
+	} else {
+		chan = MHI_TRE_GET_CMD_CHID(cmd_pkt);
+		mhi_chan = &mhi_cntrl->mhi_chan[chan];
+		write_lock_bh(&mhi_chan->lock);
+		mhi_chan->ccs = MHI_TRE_GET_EV_CODE(tre);
+		complete(&mhi_chan->completion);
+		write_unlock_bh(&mhi_chan->lock);
+	}
+
+	mhi_del_ring_element(mhi_cntrl, mhi_ring);
+}
+
+/**
+ * @brief ctrl event ring handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_event					mhi evnet
+ * @param[in ]            event_quota				event quota
+ */
+int mhi_process_ctrl_ev_ring(struct mhi_controller *mhi_cntrl,
+			     struct mhi_event *mhi_event,
+			     u32 event_quota)
+{
+	struct mhi_tre *dev_rp, *local_rp;
+	struct mhi_ring *ev_ring = &mhi_event->ring;
+	struct mhi_event_ctxt *er_ctxt =
+		&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];
+	int count = 0;
+	u32 chan;
+	struct mhi_chan *mhi_chan;
+
+	MHI_LOG("enter er_index %d", mhi_event->er_index);
+
+	/*
+	 * this is a quick check to avoid unnecessary event processing
+	 * in case we already in error state, but it's still possible
+	 * to transition to error state while processing events
+	 */
+	if (unlikely(MHI_EVENT_ACCESS_INVALID(mhi_cntrl->pm_state))) {
+		MHI_ERR("No EV access, PM_STATE:%s\n",
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+
+	dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+	local_rp = ev_ring->rp;
+
+	MHI_LOG("dev_rp 0x%p local_rp 0x%p", dev_rp, local_rp);
+
+	while (dev_rp != local_rp) {
+        enum MHI_PKT_TYPE type = MHI_TRE_GET_EV_TYPE(local_rp);
+
+		MHI_LOG("type %d", type);
+
+		MHI_VERB("Processing Event:0x%llx 0x%08x 0x%08x\n",
+			local_rp->ptr, local_rp->dword[0], local_rp->dword[1]);
+
+		switch (type) {
+		case MHI_PKT_TYPE_BW_REQ_EVENT:
+		{
+			struct mhi_link_info *link_info;
+
+			link_info = &mhi_cntrl->mhi_link_info;
+			write_lock_irq(&mhi_cntrl->pm_lock);
+			link_info->target_link_speed =
+				MHI_TRE_GET_EV_LINKSPEED(local_rp);
+			link_info->target_link_width =
+				MHI_TRE_GET_EV_LINKWIDTH(local_rp);
+			write_unlock_irq(&mhi_cntrl->pm_lock);
+			MHI_VERB(
+				 "Received BW_REQ with link speed:0x%x width:0x%x\n",
+				 link_info->target_link_speed,
+				 link_info->target_link_width);
+			mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+					     MHI_CB_BW_REQ);
+			break;
+		}
+		case MHI_PKT_TYPE_STATE_CHANGE_EVENT:
+		{
+			enum mhi_dev_state new_state;
+
+			new_state = MHI_TRE_GET_EV_STATE(local_rp);
+
+			MHI_DEFAULT("MHI state change event to state:%s\n",
+				TO_MHI_STATE_STR(new_state));
+
+			switch (new_state) {
+			case MHI_STATE_M0:
+				mhi_pm_m0_transition(mhi_cntrl);
+				break;
+			case MHI_STATE_M1:
+				mhi_pm_m1_transition(mhi_cntrl);
+				break;
+			case MHI_STATE_M3:
+				mhi_pm_m3_transition(mhi_cntrl);
+				break;
+			case MHI_STATE_SYS_ERR:
+			{
+				enum MHI_PM_STATE new_state;
+
+				MHI_ERR("MHI system error detected\n");
+				write_lock_irq(&mhi_cntrl->pm_lock);
+				new_state = mhi_tryset_pm_state(mhi_cntrl,
+							MHI_PM_SYS_ERR_DETECT);
+				write_unlock_irq(&mhi_cntrl->pm_lock);
+				if (new_state == MHI_PM_SYS_ERR_DETECT)
+					schedule_work(
+						&mhi_cntrl->syserr_worker);
+				break;
+			}
+			default:
+				MHI_ERR("Unsupported STE:%s\n",
+					TO_MHI_STATE_STR(new_state));
+			}
+
+			break;
+		}
+		case MHI_PKT_TYPE_CMD_COMPLETION_EVENT:
+			mhi_process_cmd_completion(mhi_cntrl, local_rp);
+			break;
+		case MHI_PKT_TYPE_EE_EVENT:
+		{
+			enum MHI_ST_TRANSITION st = MHI_ST_TRANSITION_MAX;
+			enum mhi_ee event = MHI_TRE_GET_EV_EXECENV(local_rp);
+
+			/* convert device ee to host ee */
+			event = mhi_translate_dev_ee(mhi_cntrl, event);
+
+			MHI_DEFAULT("MHI EE received event:%s\n",
+				TO_MHI_EXEC_STR(event));
+			switch (event) {
+			case MHI_EE_SBL:
+				st = MHI_ST_TRANSITION_SBL;
+				break;
+			case MHI_EE_WFW:
+			case MHI_EE_AMSS:
+				st = MHI_ST_TRANSITION_MISSION_MODE;
+				break;
+			case MHI_EE_RDDM:
+				mhi_cntrl->status_cb(mhi_cntrl,
+						     mhi_cntrl->priv_data,
+						     MHI_CB_EE_RDDM);
+				write_lock_irq(&mhi_cntrl->pm_lock);
+				mhi_cntrl->ee = event;
+				write_unlock_irq(&mhi_cntrl->pm_lock);
+				wake_up_all(&mhi_cntrl->state_event);
+				break;
+			default:
+				MHI_ERR("Unhandled EE event:%s\n",
+					TO_MHI_EXEC_STR(event));
+			}
+			if (st != MHI_ST_TRANSITION_MAX)
+				mhi_queue_state_transition(mhi_cntrl, st);
+
+			break;
+		}
+		case MHI_PKT_TYPE_TX_EVENT:
+        {
+		    chan = MHI_TRE_GET_EV_CHID(local_rp);
+		    mhi_chan = &mhi_cntrl->mhi_chan[chan];
+
+			MHI_LOG("TX event:chan %d\n", chan);
+			
+            parse_xfer_event(mhi_cntrl, local_rp, mhi_chan);
+
+            break;
+        }
+
+		default:
+			MHI_ERR("Unhandled Event: 0x%x\n", type);
+			break;
+		}
+
+		mhi_recycle_ev_ring_element(mhi_cntrl, ev_ring);
+		local_rp = ev_ring->rp;
+		dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+	    MHI_LOG("in while dev_rp 0x%p local_rp 0x%p", dev_rp, local_rp);
+		count++;
+	}
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))
+		mhi_ring_er_db(mhi_event);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	MHI_VERB("exit er_index:%u\n", mhi_event->er_index);
+	MHI_LOG("exit er_index:%u\n", mhi_event->er_index);
+
+	return count;
+}
+
+/**
+ * @brief data event ring handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_event					mhi evnet
+ * @param[in ]            event_quota				event quota
+ */
+int mhi_process_data_event_ring(struct mhi_controller *mhi_cntrl,
+				struct mhi_event *mhi_event,
+				u32 event_quota)
+{
+	struct mhi_tre *dev_rp, *local_rp;
+	struct mhi_ring *ev_ring = &mhi_event->ring;
+	struct mhi_event_ctxt *er_ctxt =
+		&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];
+	int count = 0;
+	u32 chan;
+	struct mhi_chan *mhi_chan;
+
+	if (unlikely(MHI_EVENT_ACCESS_INVALID(mhi_cntrl->pm_state))) {
+		MHI_ERR("No EV access, PM_STATE:%s\n",
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+
+	dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+	local_rp = ev_ring->rp;
+
+	while (dev_rp != local_rp && event_quota > 0) {
+		enum MHI_PKT_TYPE type = MHI_TRE_GET_EV_TYPE(local_rp);
+
+		MHI_LOG("Processing Event:0x%llx 0x%08x 0x%08x\n",
+			local_rp->ptr, local_rp->dword[0], local_rp->dword[1]);
+
+		MHI_VERB("Processing Event:0x%llx 0x%08x 0x%08x\n",
+			local_rp->ptr, local_rp->dword[0], local_rp->dword[1]);
+
+		chan = MHI_TRE_GET_EV_CHID(local_rp);
+		mhi_chan = &mhi_cntrl->mhi_chan[chan];
+
+	    MHI_LOG("chan:%d type:%d\n", chan, type);
+
+		if (likely(type == MHI_PKT_TYPE_TX_EVENT)) {
+			parse_xfer_event(mhi_cntrl, local_rp, mhi_chan);
+			event_quota--;
+		} else if (type == MHI_PKT_TYPE_RSC_TX_EVENT) {
+			parse_rsc_event(mhi_cntrl, local_rp, mhi_chan);
+			event_quota--;
+		}
+
+		mhi_recycle_ev_ring_element(mhi_cntrl, ev_ring);
+		local_rp = ev_ring->rp;
+		dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+		count++;
+	}
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))
+		mhi_ring_er_db(mhi_event);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	MHI_VERB("exit er_index:%u\n", mhi_event->er_index);
+
+	return count;
+}
+
+/**
+ * @brief tsync event ring handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_event					mhi evnet
+ * @param[in ]            event_quota				evnet quota
+ */
+int mhi_process_tsync_event_ring(struct mhi_controller *mhi_cntrl,
+				 struct mhi_event *mhi_event,
+				 u32 event_quota)
+{
+	struct mhi_tre *dev_rp, *local_rp;
+	struct mhi_ring *ev_ring = &mhi_event->ring;
+	struct mhi_event_ctxt *er_ctxt =
+		&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];
+	struct mhi_timesync *mhi_tsync = mhi_cntrl->mhi_tsync;
+	int count = 0;
+	u32 sequence;
+	u64 remote_time;
+
+	if (unlikely(MHI_EVENT_ACCESS_INVALID(mhi_cntrl->pm_state))) {
+		MHI_ERR("No EV access, PM_STATE:%s\n",
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+		return -EIO;
+	}
+
+	dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+	local_rp = ev_ring->rp;
+
+	while (dev_rp != local_rp) {
+		enum MHI_PKT_TYPE type = MHI_TRE_GET_EV_TYPE(local_rp);
+		struct tsync_node *tsync_node;
+
+		MHI_VERB("Processing Event:0x%llx 0x%08x 0x%08x\n",
+			local_rp->ptr, local_rp->dword[0], local_rp->dword[1]);
+
+		MHI_ASSERT(type != MHI_PKT_TYPE_TSYNC_EVENT, "!TSYNC event");
+
+		sequence = MHI_TRE_GET_EV_SEQ(local_rp);
+		remote_time = MHI_TRE_GET_EV_TIME(local_rp);
+
+		do {
+			spin_lock_irq(&mhi_tsync->lock);
+			tsync_node = list_first_entry_or_null(&mhi_tsync->head,
+						      struct tsync_node, node);
+			MHI_ASSERT(!tsync_node, "Unexpected Event");
+
+			if (unlikely(!tsync_node))
+				break;
+
+			list_del(&tsync_node->node);
+			spin_unlock_irq(&mhi_tsync->lock);
+
+			/*
+			 * device may not able to process all time sync commands
+			 * host issue and only process last command it receive
+			 */
+			if (tsync_node->sequence == sequence) {
+				tsync_node->cb_func(tsync_node->mhi_dev,
+						    sequence,
+						    tsync_node->local_time,
+						    remote_time);
+				kfree(tsync_node);
+			} else {
+				kfree(tsync_node);
+			}
+		} while (true);
+
+		mhi_recycle_ev_ring_element(mhi_cntrl, ev_ring);
+		local_rp = ev_ring->rp;
+		dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+		count++;
+	}
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))
+		mhi_ring_er_db(mhi_event);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	MHI_VERB("exit er_index:%u\n", mhi_event->er_index);
+
+	return count;
+}
+
+/**
+ * @brief event handler
+ *
+ * @param[in ]            data					mhi evnet
+ */
+void mhi_ev_task(unsigned long data)
+{
+	struct mhi_event *mhi_event = (struct mhi_event *)data;
+	struct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;
+
+	MHI_VERB("Enter for ev_index:%d\n", mhi_event->er_index);
+	MHI_LOG("Enter for ev_index:%d\n", mhi_event->er_index);
+
+	/* process all pending events */
+	spin_lock_bh(&mhi_event->lock);
+	mhi_event->process_event(mhi_cntrl, mhi_event, U32_MAX);
+	spin_unlock_bh(&mhi_event->lock);
+}
+
+/**
+ * @brief control event handler
+ *
+ * @param[in ]            data					mhi evnet
+ */
+void mhi_ctrl_ev_task(unsigned long data)
+{
+	struct mhi_event *mhi_event = (struct mhi_event *)data;
+	struct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;
+	enum mhi_dev_state state;
+	enum MHI_PM_STATE pm_state = 0;
+	int ret;
+
+	MHI_VERB("Enter for ev_index:%d\n", mhi_event->er_index);
+
+	/*
+	 * we can check pm_state w/o a lock here because there is no way
+	 * pm_state can change from reg access valid to no access while this
+	 * therad being executed.
+	 */
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		/*
+		 * we may have a pending event but not allowed to
+		 * process it since we probably in a suspended state,
+		 * trigger a resume.
+		 */
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+		return;
+	}
+
+	/* process ctrl events events */
+	ret = mhi_event->process_event(mhi_cntrl, mhi_event, U32_MAX);
+
+	/*
+	 * we received a MSI but no events to process maybe device went to
+	 * SYS_ERR state, check the state
+	 */
+	if (!ret) {
+		write_lock_irq(&mhi_cntrl->pm_lock);
+		state = mhi_get_mhi_state(mhi_cntrl);
+		if (state == MHI_STATE_SYS_ERR) {
+			MHI_ERR("MHI system error detected\n");
+			pm_state = mhi_tryset_pm_state(mhi_cntrl,
+						       MHI_PM_SYS_ERR_DETECT);
+		}
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		if (pm_state == MHI_PM_SYS_ERR_DETECT)
+			schedule_work(&mhi_cntrl->syserr_worker);
+	}
+}
+
+/**
+ * @brief irq handler for one msi
+ *
+ * @param[in ]            irq_number				irq triggered
+ * @param[in ]            dev						mhi	controller
+ */
+irqreturn_t mhi_msi_handlr(int irq_number, void *dev)
+{
+	struct mhi_event *mhi_event = dev;
+	struct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;
+	struct mhi_event_ctxt *er_ctxt =
+		&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];
+	struct mhi_ring *ev_ring = &mhi_event->ring;
+	void *dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+
+	MHI_LOG("mhi_msi_handlr: irq %d er_index %d\n", irq_number, mhi_event->er_index);
+
+	/* confirm ER has pending events to process before scheduling work */
+	if (ev_ring->rp == dev_rp) {
+	    MHI_LOG("rp 0x%p dev_rp 0x%p\n", ev_ring->rp, dev_rp);
+        if (mhi_cntrl->msi_allocated > 1)
+            return IRQ_HANDLED;
+        else                 
+		    return IRQ_NONE; 
+    }
+
+	/* client managed event ring, notify pending data */
+	if (mhi_event->cl_manage) {
+		struct mhi_chan *mhi_chan = mhi_event->mhi_chan;
+		struct mhi_device *mhi_dev = mhi_chan->mhi_dev;
+
+		if (mhi_dev)
+			mhi_dev->status_cb(mhi_dev, MHI_CB_PENDING_DATA);
+	} else {
+	    MHI_LOG("schedule event task!\n");
+    
+    	tasklet_schedule(&mhi_event->task);
+    }
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * @brief irq handler for all 
+ *
+ * @param[in ]            irq_number				irq triggered
+ * @param[in ]            dev						mhi	controller
+ */
+irqreturn_t mhi_msi_handlr_all(int irq_number, void *dev)
+{
+	struct mhi_controller *mhi_cntrl = dev;
+	struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+    int i, j = 0;
+    irqreturn_t ret; 
+
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+	    
+        MHI_LOG("irq %d er_index %d\n", irq_number, mhi_event->er_index);
+
+		ret = mhi_msi_handlr(mhi_cntrl->irq[0], mhi_event);
+
+        if (ret == IRQ_NONE)
+            j++;
+	}
+
+    /* assume it is BHI interrupt if now events */
+    if (j == mhi_cntrl->total_ev_rings) {
+        mhi_intvec_handlr(irq_number, dev);
+		tasklet_schedule(&mhi_cntrl->intvec_task);
+    }
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * @brief intvec task handler
+ *
+ * @param[in ]            data					mhi evnet
+ */
+void mhi_intvec_task(unsigned long data)
+{
+	struct mhi_controller *mhi_cntrl = (struct mhi_controller *)data;
+
+	MHI_LOG("controller 0x%px\n", mhi_cntrl);
+
+	schedule_work(&mhi_cntrl->intvec_worker);
+}
+
+/**
+ * @brief intvec worker handler
+ *
+ * @param[in ]            work 					work item
+ */
+void mhi_intvec_worker(struct work_struct *work)
+{
+	struct mhi_controller *mhi_cntrl = container_of(work,
+		struct mhi_controller,
+		intvec_worker);
+
+	MHI_LOG("controller 0x%px\n", mhi_cntrl);
+
+	mhi_intvec_threaded_handlr(mhi_cntrl->irq[0], mhi_cntrl);
+}
+
+/**
+ * @brief irq threaded fn
+ *
+ * @param[in ]            irq_number				irq triggered
+ * @param[in ]            dev						mhi	controller
+ */
+irqreturn_t mhi_intvec_threaded_handlr(int irq_number, void *dev)
+{
+	struct mhi_controller *mhi_cntrl = dev;
+	enum mhi_dev_state state = MHI_STATE_MAX;
+	enum MHI_PM_STATE pm_state = 0;
+	enum mhi_ee ee = 0;
+
+	MHI_VERB("Enter\n");
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	if (MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		state = mhi_get_mhi_state(mhi_cntrl);
+		ee = mhi_cntrl->ee;
+		mhi_cntrl->ee = mhi_get_exec_env(mhi_cntrl);
+
+		/* to avoid accessive logging on some systems */
+		if ((mhi_cntrl->previous_ee != mhi_cntrl->ee) || 
+			(mhi_cntrl->previous_dev_state != state)) {
+
+			MHI_DEFAULT("device ee:%s dev_state:%s\n",
+				TO_MHI_EXEC_STR(mhi_cntrl->ee),
+				TO_MHI_STATE_STR(state));
+		}
+
+		if ((mhi_cntrl->previous_ee == MHI_EE_MAX) && 
+			(mhi_cntrl->previous_dev_state == MHI_STATE_MAX)) {
+
+			if (state == MHI_STATE_READY)
+				mhi_queue_state_transition(mhi_cntrl, state);
+		}
+
+		mhi_cntrl->previous_dev_state = state;
+		mhi_cntrl->previous_ee = mhi_cntrl->ee;
+
+		/*
+				if (state == MHI_STATE_READY)
+					mhi_queue_state_transition(mhi_cntrl, state);
+		*/
+	}
+
+	if (state == MHI_STATE_SYS_ERR) {
+		MHI_ERR("MHI system error detected\n");
+		pm_state = mhi_tryset_pm_state(mhi_cntrl,
+			MHI_PM_SYS_ERR_DETECT);
+	}
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/* if device in rddm don't bother processing sys error */
+	if (mhi_cntrl->ee == MHI_EE_RDDM) {
+		if (mhi_cntrl->ee != ee) {
+			mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+				MHI_CB_EE_RDDM);
+			wake_up_all(&mhi_cntrl->state_event);
+		}
+		goto exit_intvec;
+	}
+
+	if (pm_state == MHI_PM_SYS_ERR_DETECT) {
+		wake_up_all(&mhi_cntrl->state_event);
+
+		/* for fatal errors, we let controller decide next step */
+		if (MHI_IN_PBL(ee))
+			mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+				MHI_CB_FATAL_ERROR);
+		else
+			schedule_work(&mhi_cntrl->syserr_worker);
+	}
+
+exit_intvec:
+	MHI_VERB("Exit\n");
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * @brief irq handler
+ *
+ * @param[in ]            irq_number				irq triggered
+ * @param[in ]            dev						mhi	controller
+ */
+irqreturn_t mhi_intvec_handlr(int irq_number, void *dev)
+{
+
+	struct mhi_controller *mhi_cntrl = dev;
+
+	/* wake up any events waiting for state change */
+	MHI_VERB("Enter\n");
+	wake_up_all(&mhi_cntrl->state_event);
+	MHI_VERB("Exit\n");
+
+	return IRQ_WAKE_THREAD;
+}
+
+/**
+ * @brief send command
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					mhi	channel
+ * @param[in ]            cmd						command
+ */
+int mhi_send_cmd(struct mhi_controller *mhi_cntrl,
+		 struct mhi_chan *mhi_chan,
+		 enum MHI_CMD cmd)
+{
+	struct mhi_tre *cmd_tre = NULL;
+	struct mhi_cmd *mhi_cmd = &mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING];
+	struct mhi_ring *ring = &mhi_cmd->ring;
+	int chan = 0;
+
+	MHI_VERB("Entered, MHI pm_state:%s dev_state:%s ee:%s\n",
+		 to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		 TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		 TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	if (mhi_chan)
+		chan = mhi_chan->chan;
+
+	spin_lock_bh(&mhi_cmd->lock);
+	if (!get_nr_avail_ring_elements(mhi_cntrl, ring)) {
+		spin_unlock_bh(&mhi_cmd->lock);
+		return -ENOMEM;
+	}
+
+	/* prepare the cmd tre */
+	cmd_tre = ring->wp;
+	switch (cmd) {
+	case MHI_CMD_RESET_CHAN:
+		cmd_tre->ptr = MHI_TRE_CMD_RESET_PTR;
+		cmd_tre->dword[0] = MHI_TRE_CMD_RESET_DWORD0;
+		cmd_tre->dword[1] = MHI_TRE_CMD_RESET_DWORD1(chan);
+		break;
+	case MHI_CMD_START_CHAN:
+		cmd_tre->ptr = MHI_TRE_CMD_START_PTR;
+		cmd_tre->dword[0] = MHI_TRE_CMD_START_DWORD0;
+		cmd_tre->dword[1] = MHI_TRE_CMD_START_DWORD1(chan);
+		break;
+	case MHI_CMD_TIMSYNC_CFG:
+		cmd_tre->ptr = MHI_TRE_CMD_TSYNC_CFG_PTR;
+		cmd_tre->dword[0] = MHI_TRE_CMD_TSYNC_CFG_DWORD0;
+		cmd_tre->dword[1] = MHI_TRE_CMD_TSYNC_CFG_DWORD1
+			(mhi_cntrl->mhi_tsync->er_index);
+		break;
+	}
+
+
+	MHI_VERB("WP:0x%llx TRE: 0x%llx 0x%08x 0x%08x\n",
+		 (u64)mhi_to_physical(ring, cmd_tre), cmd_tre->ptr,
+		 cmd_tre->dword[0], cmd_tre->dword[1]);
+
+	/* queue to hardware */
+	mhi_add_ring_element(mhi_cntrl, ring);
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))
+		mhi_ring_cmd_db(mhi_cntrl, mhi_cmd);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	spin_unlock_bh(&mhi_cmd->lock);
+
+	return 0;
+}
+
+/**
+ * @brief start the mhi channel
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					mhi	channel
+ */
+static int __mhi_prepare_channel(struct mhi_controller *mhi_cntrl,
+				 struct mhi_chan *mhi_chan)
+{
+	int ret = 0;
+
+	MHI_LOG("Entered: preparing channel:%d\n", mhi_chan->chan);
+
+	if (!(BIT(mhi_cntrl->ee) & mhi_chan->ee_mask)) {
+		MHI_ERR("Current EE:%s Required EE Mask:0x%x for chan:%s\n",
+			TO_MHI_EXEC_STR(mhi_cntrl->ee), mhi_chan->ee_mask,
+			mhi_chan->name);
+		return -ENOTCONN;
+	}
+
+	mutex_lock(&mhi_chan->mutex);
+
+	/* if channel is not disable state do not allow to start */
+	if (mhi_chan->ch_state != MHI_CH_STATE_DISABLED) {
+		ret = -EIO;
+		MHI_LOG("channel:%d is not in disabled state, ch_state%d\n",
+			mhi_chan->chan, mhi_chan->ch_state);
+		goto error_init_chan;
+	}
+
+	/* client manages channel context for offload channels */
+	if (!mhi_chan->offload_ch) {
+		ret = mhi_init_chan_ctxt(mhi_cntrl, mhi_chan);
+		if (ret) {
+			MHI_ERR("Error with init chan\n");
+			goto error_init_chan;
+		}
+	}
+
+	reinit_completion(&mhi_chan->completion);
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("MHI host is not in active state\n");
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+		ret = -EIO;
+		goto error_pm_state;
+	}
+
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+	mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+
+	ret = mhi_send_cmd(mhi_cntrl, mhi_chan, MHI_CMD_START_CHAN);
+	if (ret) {
+		MHI_ERR("Failed to send start chan cmd\n");
+		goto error_pm_state;
+	}
+
+	ret = wait_for_completion_timeout(&mhi_chan->completion,
+				msecs_to_jiffies(mhi_cntrl->timeout_ms));
+	if (!ret || mhi_chan->ccs != MHI_EV_CC_SUCCESS) {
+		MHI_ERR("Failed to receive cmd completion for chan:%d\n",
+			mhi_chan->chan);
+		ret = -EIO;
+		goto error_pm_state;
+	}
+
+	write_lock_irq(&mhi_chan->lock);
+	mhi_chan->ch_state = MHI_CH_STATE_ENABLED;
+	write_unlock_irq(&mhi_chan->lock);
+
+	/* pre allocate buffer for xfer ring */
+	if (mhi_chan->pre_alloc) {
+		int nr_el = get_nr_avail_ring_elements(mhi_cntrl,
+						       &mhi_chan->tre_ring);
+		size_t len = mhi_cntrl->buffer_len;
+
+		while (nr_el--) {
+			void *buf;
+
+			buf = kmalloc(len, GFP_KERNEL);
+			if (!buf) {
+				ret = -ENOMEM;
+				goto error_pre_alloc;
+			}
+
+			/* prepare transfer descriptors */
+			ret = mhi_chan->gen_tre(mhi_cntrl, mhi_chan, buf, buf,
+						len, MHI_EOT);
+			if (ret) {
+				MHI_ERR("Chan:%d error prepare buffer\n",
+					mhi_chan->chan);
+				kfree(buf);
+				goto error_pre_alloc;
+			}
+		}
+
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		if (MHI_DB_ACCESS_VALID(mhi_cntrl)) {
+			read_lock_irq(&mhi_chan->lock);
+			mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+			read_unlock_irq(&mhi_chan->lock);
+		}
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+	}
+
+	mutex_unlock(&mhi_chan->mutex);
+
+	MHI_LOG("Chan:%d successfully moved to start state\n", mhi_chan->chan);
+
+	return 0;
+
+error_pm_state:
+	if (!mhi_chan->offload_ch)
+		mhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);
+
+error_init_chan:
+	mutex_unlock(&mhi_chan->mutex);
+
+	return ret;
+
+error_pre_alloc:
+	mutex_unlock(&mhi_chan->mutex);
+	__mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+
+	return ret;
+}
+
+/**
+ * @brief mark stale event so they will be ignored
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_event					event
+ * @param[in ]            er_ctxt					event context
+ * @param[in ]            chan						channel number
+ */
+static void mhi_mark_stale_events(struct mhi_controller *mhi_cntrl,
+				  struct mhi_event *mhi_event,
+				  struct mhi_event_ctxt *er_ctxt,
+				  int chan)
+{
+	struct mhi_tre *dev_rp, *local_rp;
+	struct mhi_ring *ev_ring;
+	unsigned long flags;
+
+	MHI_LOG("Marking all events for chan:%d as stale\n", chan);
+
+	ev_ring = &mhi_event->ring;
+
+	/* mark all stale events related to channel as STALE event */
+	spin_lock_irqsave(&mhi_event->lock, flags);
+	dev_rp = mhi_to_virtual(ev_ring, er_ctxt->rp);
+
+	local_rp = ev_ring->rp;
+	while (dev_rp != local_rp) {
+		if (MHI_TRE_GET_EV_TYPE(local_rp) ==
+		    MHI_PKT_TYPE_TX_EVENT &&
+		    chan == MHI_TRE_GET_EV_CHID(local_rp))
+			local_rp->dword[1] = MHI_TRE_EV_DWORD1(chan,
+					MHI_PKT_TYPE_STALE_EVENT);
+		local_rp++;
+		if (local_rp == (ev_ring->base + ev_ring->len))
+			local_rp = ev_ring->base;
+	}
+
+
+	MHI_LOG("Finished marking events as stale events\n");
+	spin_unlock_irqrestore(&mhi_event->lock, flags);
+}
+
+/**
+ * @brief reset mhi data channel
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					channel
+ */
+static void mhi_reset_data_chan(struct mhi_controller *mhi_cntrl,
+				struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring, *tre_ring;
+	struct mhi_result result;
+
+	/* reset any pending buffers */
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+	result.transaction_status = -ENOTCONN;
+	result.bytes_xferd = 0;
+	while (tre_ring->rp != tre_ring->wp) {
+		struct mhi_buf_info *buf_info = buf_ring->rp;
+
+		if (mhi_chan->dir == DMA_TO_DEVICE)
+			atomic_dec(&mhi_cntrl->pending_pkts);
+
+		if (!buf_info->pre_mapped)
+			mhi_cntrl->unmap_single(mhi_cntrl, buf_info);
+		mhi_del_ring_element(mhi_cntrl, buf_ring);
+		mhi_del_ring_element(mhi_cntrl, tre_ring);
+
+		if (mhi_chan->pre_alloc) {
+			kfree(buf_info->cb_buf);
+		} else {
+			result.buf_addr = buf_info->cb_buf;
+			mhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);
+		}
+	}
+}
+
+/**
+ * @brief reset mhi rsc channel
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					channel
+ */
+static void mhi_reset_rsc_chan(struct mhi_controller *mhi_cntrl,
+			       struct mhi_chan *mhi_chan)
+{
+	struct mhi_ring *buf_ring, *tre_ring;
+	struct mhi_result result;
+	struct mhi_buf_info *buf_info;
+
+	/* reset any pending buffers */
+	buf_ring = &mhi_chan->buf_ring;
+	tre_ring = &mhi_chan->tre_ring;
+	result.transaction_status = -ENOTCONN;
+	result.bytes_xferd = 0;
+
+	buf_info = buf_ring->base;
+	for (; (void *)buf_info < buf_ring->base + buf_ring->len; buf_info++) {
+		if (!buf_info->used)
+			continue;
+
+		result.buf_addr = buf_info->cb_buf;
+		mhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);
+		buf_info->used = false;
+	}
+}
+
+/**
+ * @brief reset mhi channel
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					channel
+ */
+void mhi_reset_chan(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan)
+{
+
+	struct mhi_event *mhi_event;
+	struct mhi_event_ctxt *er_ctxt;
+	int chan = mhi_chan->chan;
+
+	/* nothing to reset, client don't queue buffers */
+	if (mhi_chan->offload_ch)
+		return;
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_event = &mhi_cntrl->mhi_event[mhi_chan->er_index];
+	er_ctxt = &mhi_cntrl->mhi_ctxt->er_ctxt[mhi_chan->er_index];
+
+	mhi_mark_stale_events(mhi_cntrl, mhi_event, er_ctxt, chan);
+
+	if (mhi_chan->xfer_type == MHI_XFER_RSC_DMA)
+		mhi_reset_rsc_chan(mhi_cntrl, mhi_chan);
+	else
+		mhi_reset_data_chan(mhi_cntrl, mhi_chan);
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	MHI_LOG("Reset complete.\n");
+}
+
+/**
+ * @brief close/remove mhi channel
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            mhi_chan					channel 
+ */
+static void __mhi_unprepare_channel(struct mhi_controller *mhi_cntrl,
+				    struct mhi_chan *mhi_chan)
+{
+	int ret;
+
+	MHI_LOG("Entered: unprepare channel:%d\n", mhi_chan->chan);
+
+	/* no more processing events for this channel */
+	mutex_lock(&mhi_chan->mutex);
+	write_lock_irq(&mhi_chan->lock);
+	if (mhi_chan->ch_state == MHI_CH_STATE_DISABLED) {
+		MHI_LOG("chan:%d is already disabled\n", mhi_chan->chan);
+		write_unlock_irq(&mhi_chan->lock);
+		mutex_unlock(&mhi_chan->mutex);
+		return;
+	}
+
+	mhi_chan->ch_state = MHI_CH_STATE_DISABLED;
+	write_unlock_irq(&mhi_chan->lock);
+
+	reinit_completion(&mhi_chan->completion);
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+		goto error_invalid_state;
+	}
+
+	mhi_cntrl->wake_toggle(mhi_cntrl);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+	mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	ret = mhi_send_cmd(mhi_cntrl, mhi_chan, MHI_CMD_RESET_CHAN);
+	if (ret) {
+		MHI_ERR("Failed to send reset chan cmd\n");
+		goto error_invalid_state;
+	}
+
+	/* even if it fails we will still reset */
+	ret = wait_for_completion_timeout(&mhi_chan->completion,
+				msecs_to_jiffies(mhi_cntrl->timeout_ms));
+	if (!ret || mhi_chan->ccs != MHI_EV_CC_SUCCESS)
+		MHI_ERR("Failed to receive cmd completion, still resetting\n");
+
+error_invalid_state:
+	if (!mhi_chan->offload_ch) {
+		mhi_reset_chan(mhi_cntrl, mhi_chan);
+		mhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);
+	}
+	MHI_LOG("chan:%d successfully resetted\n", mhi_chan->chan);
+	mutex_unlock(&mhi_chan->mutex);
+}
+
+/**
+ * @brief close all mhi channel
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_close_all_channel(struct mhi_controller *mhi_cntrl)
+{
+	u32 i;
+    struct mhi_chan *mhi_chan;
+
+	mhi_chan = mhi_cntrl->mhi_chan;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {
+		if (!mhi_chan->configured)
+			continue;
+
+	    MHI_LOG("close %s channel %d\n", mhi_chan->name, mhi_chan->chan);
+
+		__mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+	}
+
+	return 0;
+}	
+
+#ifdef CONFIG_DEBUG_FS
+/**
+ * @brief write mhi states to debugfs
+ *
+ * @param[in ]            m						file
+ * @param[in ]            d
+ */
+int mhi_debugfs_mhi_states_show(struct seq_file *m, void *d)
+{
+	struct mhi_controller *mhi_cntrl = m->private;
+
+	seq_printf(m,
+		   "pm_state:%s dev_state:%s EE:%s M0:%u M2:%u M3:%u M3_Fast:%u wake:%d dev_wake:%u alloc_size:%u pending_pkts:%u\n",
+		   to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		   TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		   TO_MHI_EXEC_STR(mhi_cntrl->ee),
+		   mhi_cntrl->M0, mhi_cntrl->M2, mhi_cntrl->M3,
+		   mhi_cntrl->M3_FAST, mhi_cntrl->wake_set,
+		   atomic_read(&mhi_cntrl->dev_wake),
+		   atomic_read(&mhi_cntrl->alloc_size),
+		   atomic_read(&mhi_cntrl->pending_pkts));
+	return 0;
+}
+
+/**
+ * @brief write mhi events to debugfs
+ *
+ * @param[in ]            m						file
+ * @param[in ]            d
+ */
+int mhi_debugfs_mhi_event_show(struct seq_file *m, void *d)
+{
+	struct mhi_controller *mhi_cntrl = m->private;
+	struct mhi_event *mhi_event;
+	struct mhi_event_ctxt *er_ctxt;
+
+	int i;
+
+	er_ctxt = mhi_cntrl->mhi_ctxt->er_ctxt;
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, er_ctxt++,
+		     mhi_event++) {
+		struct mhi_ring *ring = &mhi_event->ring;
+
+		if (mhi_event->offload_ev) {
+			seq_printf(m, "Index:%d offload event ring\n", i);
+		} else {
+			seq_printf(m,
+				   "Index:%d modc:%d modt:%d base:0x%0llx len:0x%llx",
+				   i, er_ctxt->intmodc, er_ctxt->intmodt,
+				   er_ctxt->rbase, er_ctxt->rlen);
+			seq_printf(m,
+				   " rp:0x%llx wp:0x%llx local_rp:0x%llx db:0x%llx\n",
+				   er_ctxt->rp, er_ctxt->wp,
+				   (unsigned long long)mhi_to_physical(ring, ring->rp),
+				   (unsigned long long)mhi_event->db_cfg.db_val);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * @brief write mhi channel states to debugfs 
+ *
+ * @param[in ]            m						file
+ * @param[in ]            d						
+ */
+int mhi_debugfs_mhi_chan_show(struct seq_file *m, void *d)
+{
+	struct mhi_controller *mhi_cntrl = m->private;
+	struct mhi_chan *mhi_chan;
+	struct mhi_chan_ctxt *chan_ctxt;
+	int i;
+
+	mhi_chan = mhi_cntrl->mhi_chan;
+	chan_ctxt = mhi_cntrl->mhi_ctxt->chan_ctxt;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, chan_ctxt++, mhi_chan++) {
+		struct mhi_ring *ring = &mhi_chan->tre_ring;
+
+		if (mhi_chan->offload_ch) {
+			seq_printf(m, "%s(%u) offload channel\n",
+				   mhi_chan->name, mhi_chan->chan);
+		} else if (mhi_chan->mhi_dev) {
+			seq_printf(m,
+				   "%s(%u) state:0x%x brstmode:0x%x pllcfg:0x%x type:0x%x erindex:%u",
+				   mhi_chan->name, mhi_chan->chan,
+				   chan_ctxt->chstate, chan_ctxt->brstmode,
+				   chan_ctxt->pollcfg, chan_ctxt->chtype,
+				   chan_ctxt->erindex);
+			seq_printf(m,
+				   " base:0x%llx len:0x%llx wp:0x%llx local_rp:0x%llu local_wp:0x%llu db:0x%llx\n",
+				   chan_ctxt->rbase, chan_ctxt->rlen,
+				   chan_ctxt->wp,
+				   (unsigned long long)mhi_to_physical(ring, ring->rp),
+				   (unsigned long long)mhi_to_physical(ring, ring->wp),
+				   (unsigned long long)mhi_chan->db_cfg.db_val);
+		}
+	}
+
+	return 0;
+}
+#endif /* CONFIG_DEBUG_FS */
+
+/**
+ * @brief  move channel to start state
+ *
+ * @param[in ]            mhi_dev				mhi device
+ */
+int mhi_prepare_for_transfer(struct mhi_device *mhi_dev)
+{
+	int ret, dir;
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan;
+
+	for (dir = 0; dir < 2; dir++) {
+		mhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;
+
+		if (!mhi_chan)
+			continue;
+
+        MHI_LOG("__mhi_prepare_channel: chan 0x%p\n", mhi_chan);
+
+        ret = __mhi_prepare_channel(mhi_cntrl, mhi_chan);
+        if (ret) {
+			MHI_ERR("Error moving chan %s,%d to START state\n",
+				mhi_chan->name, mhi_chan->chan);
+			goto error_open_chan;
+		}
+	}
+
+    MHI_LOG("mhi_prepare_channel: return 0\n");
+	return 0;
+
+error_open_chan:
+	for (--dir; dir >= 0; dir--) {
+		mhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;
+
+		if (!mhi_chan)
+			continue;
+
+		__mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+	}
+
+    MHI_LOG("mhi_prepare_channel: return %d\n", ret);
+	return ret;
+}
+EXPORT_SYMBOL(mhi_prepare_for_transfer);
+
+/**
+ * @brief close/remove mhi channel
+ *
+ * @param[in ]            mhi_dev				mhi device
+ */
+void mhi_unprepare_from_transfer(struct mhi_device *mhi_dev)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan;
+	int dir;
+
+	for (dir = 0; dir < 2; dir++) {
+		mhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;
+
+		if (!mhi_chan)
+			continue;
+
+		__mhi_unprepare_channel(mhi_cntrl, mhi_chan);
+	}
+}
+EXPORT_SYMBOL(mhi_unprepare_from_transfer);
+
+/**
+ * @brief get number of free descriptors
+ *
+ * @param[in ]            mhi_dev				mhi device
+ * @param[out]            dir 					DMA direction
+ */
+int mhi_get_no_free_descriptors(struct mhi_device *mhi_dev,
+				enum dma_data_direction dir)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ?
+		mhi_dev->ul_chan : mhi_dev->dl_chan;
+	struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+
+	return get_nr_avail_ring_elements(mhi_cntrl, tre_ring);
+}
+EXPORT_SYMBOL(mhi_get_no_free_descriptors);
+
+/**
+ * @brief find matched mhi device
+ *
+ * @param[in ]            dev					device
+ * @param[out]            tmp 					matched mhi device
+ */
+#ifdef CENTOS
+static int __mhi_bdf_to_controller(struct device *dev, const void *tmp)
+#else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,3,0)
+static int __mhi_bdf_to_controller(struct device *dev, void *tmp)
+#else 
+static int __mhi_bdf_to_controller(struct device *dev, const void *tmp)
+#endif
+#endif
+{
+	struct mhi_device *mhi_dev = to_mhi_device(dev);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,7,0)
+	struct mhi_device *match = tmp;
+#else 
+	const struct mhi_device *match = tmp;
+#endif
+
+	/* return any none-zero value if match */
+	if (mhi_dev->dev_type == MHI_CONTROLLER_TYPE &&
+	    mhi_dev->domain == match->domain && mhi_dev->bus == match->bus &&
+	    mhi_dev->slot == match->slot && mhi_dev->dev_id == match->dev_id)
+		return 1;
+
+	return 0;
+}
+
+/**
+ * @brief map bus, device and function to controller
+ *
+ * @param[in ]            domain				PCIe domain    
+ * @param[in ]            bus 					PCIe bus number
+ * @param[in ]            slot 					PCIe slot
+ * @param[in ]            dev_id 				device id
+ */
+struct mhi_controller *mhi_bdf_to_controller(u32 domain,
+					     u32 bus,
+					     u32 slot,
+					     u32 dev_id)
+{
+	struct mhi_device tmp, *mhi_dev;
+	struct device *dev;
+
+	tmp.domain = domain;
+	tmp.bus = bus;
+	tmp.slot = slot;
+	tmp.dev_id = dev_id;
+
+	dev = bus_find_device(&mhi_bus_type, NULL, &tmp,
+			      __mhi_bdf_to_controller);
+
+	if (!dev)
+		return NULL;
+
+	mhi_dev = to_mhi_device(dev);
+
+	return mhi_dev->mhi_cntrl;
+}
+EXPORT_SYMBOL(mhi_bdf_to_controller);
+
+/**
+ * @brief poll to process event
+ *
+ * @param[in ]            mhi_dev			    mhi device
+ * @param[in ]            budget 				budget
+ */
+int mhi_poll(struct mhi_device *mhi_dev,
+	     u32 budget)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_chan *mhi_chan = mhi_dev->dl_chan;
+	struct mhi_event *mhi_event = &mhi_cntrl->mhi_event[mhi_chan->er_index];
+	int ret;
+
+	spin_lock_bh(&mhi_event->lock);
+	ret = mhi_event->process_event(mhi_cntrl, mhi_event, budget);
+	spin_unlock_bh(&mhi_event->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_poll);
+
+/**
+ * @brief get remote time sync
+ *
+ * @param[in ]            mhi_device 			Device associated with the channels
+ * @param[out]            t_host 				host time
+ * @param[out]            t_dev 				device time
+ */
+int mhi_get_remote_time_sync(struct mhi_device *mhi_dev,
+			     u64 *t_host,
+			     u64 *t_dev)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_timesync *mhi_tsync = mhi_cntrl->mhi_tsync;
+	int ret;
+
+	/* not all devices support time feature */
+	if (!mhi_tsync)
+		return -EIO;
+
+	/* bring to M0 state */
+	ret = __mhi_device_get_sync(mhi_cntrl);
+	if (ret)
+		return ret;
+
+	mutex_lock(&mhi_tsync->lpm_mutex);
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_ERR("MHI is not in active state, pm_state:%s\n",
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		ret = -EIO;
+		goto error_invalid_state;
+	}
+
+	/* disable link level low power modes */
+	ret = mhi_cntrl->lpm_disable(mhi_cntrl, mhi_cntrl->priv_data);
+	if (ret)
+		goto error_invalid_state;
+
+	/*
+	 * time critical code to fetch device times,
+	 * delay between these two steps should be
+	 * deterministic as possible.
+	 */
+	preempt_disable();
+	local_irq_disable();
+
+	*t_host = mhi_cntrl->time_get(mhi_cntrl, mhi_cntrl->priv_data);
+	*t_dev = readq(mhi_tsync->time_reg); //readq_relaxed_no_log(mhi_tsync->time_reg);
+
+	local_irq_enable();
+	preempt_enable();
+
+	mhi_cntrl->lpm_enable(mhi_cntrl, mhi_cntrl->priv_data);
+
+error_invalid_state:
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	mutex_unlock(&mhi_tsync->lpm_mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_get_remote_time_sync);
+
+#if 0
+static inline void __raw_writel_no_log(u32 val, volatile void __iomem *addr)
+{
+	asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
+}
+
+#define writel_relaxed_no_log(v, c) \
+	((void)__raw_writel_no_log((__force u32)cpu_to_le32(v), (c)))
+#endif
+
+/**
+ * @brief Get external modem time relative to host time
+ * @detail Trigger event to capture modem time, also capture host time so client
+ *		   can do a relative drift comparision. Recommended only tsync device calls 
+           this method and do not call this from atomic context
+ * @param[in ]            mhi_cntrl 			Device associated with the channels
+ * @param[in ]            sequence 				unique sequence id track event
+ * @param[in ]            cb_func 				callback function to call back
+ */
+int mhi_get_remote_time(struct mhi_device *mhi_dev,
+			u32 sequence,
+			void (*cb_func)(struct mhi_device *mhi_dev,
+					u32 sequence,
+					u64 local_time,
+					u64 remote_time))
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	struct mhi_timesync *mhi_tsync = mhi_cntrl->mhi_tsync;
+	struct tsync_node *tsync_node;
+	int ret;
+
+	/* not all devices support time feature */
+	if (!mhi_tsync)
+		return -EIO;
+
+	/* tsync db can only be rung in M0 state */
+	ret = __mhi_device_get_sync(mhi_cntrl);
+	if (ret)
+		return ret;
+
+	/*
+	 * technically we can use GFP_KERNEL, but wants to avoid
+	 * # of times scheduling out
+	 */
+	tsync_node = kzalloc(sizeof(*tsync_node), GFP_ATOMIC);
+	if (!tsync_node) {
+		ret = -ENOMEM;
+		goto error_no_mem;
+	}
+
+	tsync_node->sequence = sequence;
+	tsync_node->cb_func = cb_func;
+	tsync_node->mhi_dev = mhi_dev;
+
+	/* disable link level low power modes */
+	mhi_cntrl->lpm_disable(mhi_cntrl, mhi_cntrl->priv_data);
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))) {
+		MHI_ERR("MHI is not in active state, pm_state:%s\n",
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		ret = -EIO;
+		goto error_invalid_state;
+	}
+
+	spin_lock_irq(&mhi_tsync->lock);
+	list_add_tail(&tsync_node->node, &mhi_tsync->head);
+	spin_unlock_irq(&mhi_tsync->lock);
+
+	/*
+	 * time critical code, delay between these two steps should be
+	 * deterministic as possible.
+	 */
+	preempt_disable();
+	local_irq_disable();
+
+	tsync_node->local_time =
+		mhi_cntrl->time_get(mhi_cntrl, mhi_cntrl->priv_data);
+	writel(tsync_node->sequence, mhi_tsync->db);
+	//writel_relaxed_no_log(tsync_node->sequence, mhi_tsync->db);
+	/* write must go thru immediately */
+	wmb();
+
+	local_irq_enable();
+	preempt_enable();
+
+	ret = 0;
+
+error_invalid_state:
+	if (ret)
+		kfree(tsync_node);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->lpm_enable(mhi_cntrl, mhi_cntrl->priv_data);
+
+error_no_mem:
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_get_remote_time);
+
+/**
+ * @brief dump mhi registers
+ *
+ * @param[in ]            mhi_cntrl 			mhi controller
+ */
+void mhi_debug_reg_dump(struct mhi_controller *mhi_cntrl)
+{
+	enum mhi_dev_state state;
+	enum mhi_ee ee;
+	int i, ret;
+	u32 val = 0;
+	void __iomem *mhi_base = mhi_cntrl->regs;
+	void __iomem *bhi_base = mhi_cntrl->bhi;
+	void __iomem *bhie_base = mhi_cntrl->bhie;
+	void __iomem *wake_db = mhi_cntrl->wake_db;
+	struct {
+		const char *name;
+		int offset;
+		void *base;
+	} debug_reg[] = {
+		{ "MHI_CNTRL", MHICTRL, mhi_base},
+		{ "MHI_STATUS", MHISTATUS, mhi_base},
+		{ "MHI_WAKE_DB", 0, wake_db},
+		{ "BHI_EXECENV", BHI_EXECENV, bhi_base},
+		{ "BHI_STATUS", BHI_STATUS, bhi_base},
+		{ "BHI_ERRCODE", BHI_ERRCODE, bhi_base},
+		{ "BHI_ERRDBG1", BHI_ERRDBG1, bhi_base},
+		{ "BHI_ERRDBG2", BHI_ERRDBG2, bhi_base},
+		{ "BHI_ERRDBG3", BHI_ERRDBG3, bhi_base},
+		{ "BHIE_TXVEC_DB", BHIE_TXVECDB_OFFS, bhie_base},
+		{ "BHIE_TXVEC_STATUS", BHIE_TXVECSTATUS_OFFS, bhie_base},
+		{ "BHIE_RXVEC_DB", BHIE_RXVECDB_OFFS, bhie_base},
+		{ "BHIE_RXVEC_STATUS", BHIE_RXVECSTATUS_OFFS, bhie_base},
+		{ NULL },
+	};
+
+	MHI_LOG("host pm_state:%s dev_state:%s ee:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	state = mhi_get_mhi_state(mhi_cntrl);
+	ee = mhi_get_exec_env(mhi_cntrl);
+
+	MHI_LOG("device ee:%s dev_state:%s\n", TO_MHI_EXEC_STR(ee),
+		TO_MHI_STATE_STR(state));
+
+	for (i = 0; debug_reg[i].name; i++) {
+		ret = mhi_read_reg(mhi_cntrl, debug_reg[i].base,
+				   debug_reg[i].offset, &val);
+		MHI_LOG("reg:%s val:0x%x, ret:%d\n", debug_reg[i].name, val,
+			ret);
+	}
+}
+EXPORT_SYMBOL(mhi_debug_reg_dump);
diff --git a/drivers/staging/em9190/core/mhi_pm.c b/drivers/staging/em9190/core/mhi_pm.c
new file mode 100644
index 000000000000..5e3653721713
--- /dev/null
+++ b/drivers/staging/em9190/core/mhi_pm.c
@@ -0,0 +1,1693 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include "../inc/mhi.h"
+#include "mhi_internal.h"
+
+/*
+ * Not all MHI states transitions are sync transitions. Linkdown, SSR, and
+ * shutdown can happen anytime asynchronously. This function will transition to
+ * new state only if we're allowed to transitions.
+ *
+ * Priority increase as we go down, example while in any states from L0, start
+ * state from L1, L2, or L3 can be set.  Notable exception to this rule is state
+ * DISABLE.  From DISABLE state we can transition to only POR or state.  Also
+ * for example while in L2 state, user cannot jump back to L1 or L0 states.
+ * Valid transitions:
+ * L0: DISABLE <--> POR
+ *     POR <--> POR
+ *     POR -> M0 -> M2 --> M0
+ *     POR -> FW_DL_ERR
+ *     FW_DL_ERR <--> FW_DL_ERR
+ *     M0 <--> M0
+ *     M0 -> FW_DL_ERR
+ *     M0 -> M3_ENTER -> M3 -> M3_EXIT --> M0
+ * L1: SYS_ERR_DETECT -> SYS_ERR_PROCESS --> POR
+ * L2: SHUTDOWN_PROCESS -> DISABLE
+ * L3: LD_ERR_FATAL_DETECT <--> LD_ERR_FATAL_DETECT
+ *     LD_ERR_FATAL_DETECT -> SHUTDOWN_PROCESS
+ */
+static struct mhi_pm_transitions const mhi_state_transitions[] = {
+	/* L0 States */
+	{
+		MHI_PM_DISABLE,
+		MHI_PM_POR
+	},
+	{
+		MHI_PM_POR,
+		MHI_PM_POR | MHI_PM_DISABLE | MHI_PM_M0 |
+		MHI_PM_SYS_ERR_DETECT | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT | MHI_PM_FW_DL_ERR
+	},
+	{
+		MHI_PM_M0,
+		MHI_PM_M0 | MHI_PM_M2 | MHI_PM_M3_ENTER |
+		MHI_PM_SYS_ERR_DETECT | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT | MHI_PM_FW_DL_ERR
+	},
+	{
+		MHI_PM_M2,
+		MHI_PM_M0 | MHI_PM_SYS_ERR_DETECT | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	{
+		MHI_PM_M3_ENTER,
+		MHI_PM_M3 | MHI_PM_SYS_ERR_DETECT | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	{
+		MHI_PM_M3,
+		MHI_PM_M3_EXIT | MHI_PM_SYS_ERR_DETECT |
+		MHI_PM_SHUTDOWN_PROCESS | MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	{
+		MHI_PM_M3_EXIT,
+		MHI_PM_M0 | MHI_PM_SYS_ERR_DETECT | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	{
+		MHI_PM_FW_DL_ERR,
+		MHI_PM_FW_DL_ERR | MHI_PM_SYS_ERR_DETECT |
+		MHI_PM_SHUTDOWN_PROCESS | MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	/* L1 States */
+	{
+		MHI_PM_SYS_ERR_DETECT,
+		MHI_PM_SYS_ERR_PROCESS | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	{
+		MHI_PM_SYS_ERR_PROCESS,
+		MHI_PM_POR | MHI_PM_SHUTDOWN_PROCESS |
+		MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	/* L2 States */
+	{
+		MHI_PM_SHUTDOWN_PROCESS,
+		MHI_PM_DISABLE | MHI_PM_LD_ERR_FATAL_DETECT
+	},
+	/* L3 States */
+	{
+		MHI_PM_LD_ERR_FATAL_DETECT,
+		MHI_PM_LD_ERR_FATAL_DETECT | MHI_PM_SHUTDOWN_PROCESS
+	},
+};
+
+/**
+ * @brief try to set the PM state
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            state						state
+ */
+enum MHI_PM_STATE __must_check mhi_tryset_pm_state(
+				struct mhi_controller *mhi_cntrl,
+				enum MHI_PM_STATE state)
+{
+	unsigned long cur_state = mhi_cntrl->pm_state;
+	int index = find_last_bit(&cur_state, 32);
+
+	if (unlikely(index >= ARRAY_SIZE(mhi_state_transitions))) {
+		MHI_CRITICAL("cur_state:%s is not a valid pm_state\n",
+			     to_mhi_pm_state_str(cur_state));
+		return cur_state;
+	}
+
+	if (unlikely(mhi_state_transitions[index].from_state != cur_state)) {
+		MHI_ERR("index:%u cur_state:%s != actual_state: %s\n",
+			index, to_mhi_pm_state_str(cur_state),
+			to_mhi_pm_state_str
+			(mhi_state_transitions[index].from_state));
+		return cur_state;
+	}
+
+	if (unlikely(!(mhi_state_transitions[index].to_states & state))) {
+		MHI_LOG(
+			"Not allowing pm state transition from:%s to:%s state\n",
+			to_mhi_pm_state_str(cur_state),
+			to_mhi_pm_state_str(state));
+		return cur_state;
+	}
+
+	MHI_VERB("Transition to pm state from:%s to:%s\n",
+		 to_mhi_pm_state_str(cur_state), to_mhi_pm_state_str(state));
+
+	if (MHI_REG_ACCESS_VALID(cur_state) && MHI_REG_ACCESS_VALID(state))
+		mhi_timesync_log(mhi_cntrl);
+
+	mhi_cntrl->pm_state = state;
+	return mhi_cntrl->pm_state;
+}
+
+/**
+ * @brief set the PM state
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            state						state
+ */
+void mhi_set_mhi_state(struct mhi_controller *mhi_cntrl,
+		       enum mhi_dev_state state)
+{
+	if (state == MHI_STATE_RESET) {
+		mhi_write_reg_field(mhi_cntrl, mhi_cntrl->regs, MHICTRL,
+				    MHICTRL_RESET_MASK, MHICTRL_RESET_SHIFT, 1);
+	} else {
+		mhi_write_reg_field(mhi_cntrl, mhi_cntrl->regs, MHICTRL,
+			MHICTRL_MHISTATE_MASK, MHICTRL_MHISTATE_SHIFT, state);
+	}
+}
+EXPORT_SYMBOL(mhi_set_mhi_state);
+
+/**
+ * @brief nop for backward compatibility, allowed to ring db registers in M2 state
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+static void mhi_toggle_dev_wake_nop(struct mhi_controller *mhi_cntrl)
+{
+}
+
+/**
+ * @brief toggle device wake
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+static void mhi_toggle_dev_wake(struct mhi_controller *mhi_cntrl)
+{
+	mhi_cntrl->wake_get(mhi_cntrl, false);
+	mhi_cntrl->wake_put(mhi_cntrl, true);
+}
+
+/**
+ * @brief set device wake
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            force						force to wake up device
+ */
+void mhi_assert_dev_wake(struct mhi_controller *mhi_cntrl, bool force)
+{
+	unsigned long flags;
+
+	/* if set, regardless of count set the bit if not set */
+	if (unlikely(force)) {
+		spin_lock_irqsave(&mhi_cntrl->wlock, flags);
+		atomic_inc(&mhi_cntrl->dev_wake);
+		if (MHI_WAKE_DB_FORCE_SET_VALID(mhi_cntrl->pm_state) &&
+		    !mhi_cntrl->wake_set) {
+			mhi_write_db(mhi_cntrl, mhi_cntrl->wake_db, 1);
+			mhi_cntrl->wake_set = true;
+		}
+		spin_unlock_irqrestore(&mhi_cntrl->wlock, flags);
+	} else {
+		/* if resources requested already, then increment and exit */
+		if (likely(atomic_add_unless(&mhi_cntrl->dev_wake, 1, 0)))
+			return;
+
+		spin_lock_irqsave(&mhi_cntrl->wlock, flags);
+		if ((atomic_inc_return(&mhi_cntrl->dev_wake) == 1) &&
+		    MHI_WAKE_DB_SET_VALID(mhi_cntrl->pm_state) &&
+		    !mhi_cntrl->wake_set) {
+			mhi_write_db(mhi_cntrl, mhi_cntrl->wake_db, 1);
+			mhi_cntrl->wake_set = true;
+		}
+		spin_unlock_irqrestore(&mhi_cntrl->wlock, flags);
+	}
+}
+
+/**
+ * @brief clear device wake
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            override					override
+ */
+void mhi_deassert_dev_wake(struct mhi_controller *mhi_cntrl, bool override)
+{
+	unsigned long flags;
+
+	MHI_ASSERT(atomic_read(&mhi_cntrl->dev_wake) == 0, "dev_wake == 0");
+
+	/* resources not dropping to 0, decrement and exit */
+	if (likely(atomic_add_unless(&mhi_cntrl->dev_wake, -1, 1)))
+		return;
+
+	spin_lock_irqsave(&mhi_cntrl->wlock, flags);
+	if ((atomic_dec_return(&mhi_cntrl->dev_wake) == 0) &&
+	    MHI_WAKE_DB_CLEAR_VALID(mhi_cntrl->pm_state) && !override &&
+	    mhi_cntrl->wake_set) {
+		mhi_write_db(mhi_cntrl, mhi_cntrl->wake_db, 0);
+		mhi_cntrl->wake_set = false;
+	}
+	spin_unlock_irqrestore(&mhi_cntrl->wlock, flags);
+}
+
+/**
+ * @brief ready state handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+int mhi_ready_state_transition(struct mhi_controller *mhi_cntrl)
+{
+	void __iomem *base = mhi_cntrl->regs;
+	u32 reset = 1, ready = 0;
+	struct mhi_event *mhi_event;
+	enum MHI_PM_STATE cur_state;
+	int ret, i;
+
+	MHI_LOG("Waiting to enter READY state\n");
+
+	/* wait for RESET to be cleared and READY bit to be set */
+	wait_event_timeout(mhi_cntrl->state_event,
+			   MHI_PM_IN_FATAL_STATE(mhi_cntrl->pm_state) ||
+			   mhi_read_reg_field(mhi_cntrl, base, MHICTRL,
+					      MHICTRL_RESET_MASK,
+					      MHICTRL_RESET_SHIFT, &reset) ||
+			   mhi_read_reg_field(mhi_cntrl, base, MHISTATUS,
+					      MHISTATUS_READY_MASK,
+					      MHISTATUS_READY_SHIFT, &ready) ||
+			   (!reset && ready),
+			   msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	/* device enter into error state */
+	if (MHI_PM_IN_FATAL_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	/* device did not transition to ready state */
+	if (reset || !ready)
+		return -ETIMEDOUT;
+
+	MHI_LOG("Device in READY State\n");
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_POR);
+	mhi_cntrl->dev_state = MHI_STATE_READY;
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	if (cur_state != MHI_PM_POR) {
+		MHI_ERR("Error moving to state %s from %s\n",
+			to_mhi_pm_state_str(MHI_PM_POR),
+			to_mhi_pm_state_str(cur_state));
+		return -EIO;
+	}
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	if (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state))
+		goto error_mmio;
+
+	ret = mhi_init_mmio(mhi_cntrl);
+	if (ret) {
+		MHI_ERR("Error programming mmio registers\n");
+		goto error_mmio;
+	}
+
+#if 1   // Adam
+	/* add elements to all sw event rings */
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		struct mhi_ring *ring = &mhi_event->ring;
+
+		if (mhi_event->offload_ev || mhi_event->hw_ring)
+			continue;
+
+		ring->wp = ring->base + ring->len - ring->el_size;
+		*ring->ctxt_wp = ring->iommu_base + ring->len - ring->el_size;
+
+    	MHI_LOG("add event ring %d wp 0x%p ctxt_wp 0x%llx", i, ring->wp, *ring->ctxt_wp);
+
+        // adam
+	    //mhi_write_db(mhi_cntrl, ring->db_addr, *ring->ctxt_wp);
+
+		/* needs to update to all cores */
+		smp_wmb();
+
+		/* ring the db for event rings */
+		spin_lock_irq(&mhi_event->lock);
+    	MHI_LOG("db_addr 0x%p ctxt_wp 0x%llx", ring->db_addr, *ring->ctxt_wp);
+		mhi_ring_er_db(mhi_event);
+		spin_unlock_irq(&mhi_event->lock);
+	}
+#endif
+
+	/* set device into M0 state */
+	MHI_LOG("set device into M0 state\n");
+	mhi_set_mhi_state(mhi_cntrl, MHI_STATE_M0);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	return 0;
+
+error_mmio:
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	return -EIO;
+}
+
+/**
+ * @brief M0 state handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+int mhi_pm_m0_transition(struct mhi_controller *mhi_cntrl)
+{
+	enum MHI_PM_STATE cur_state;
+	struct mhi_chan *mhi_chan;
+	int i;
+
+	MHI_DEFAULT("Entered With State:%s PM_STATE:%s\n",
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		to_mhi_pm_state_str(mhi_cntrl->pm_state));
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	mhi_cntrl->dev_state = MHI_STATE_M0;
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M0);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+	if (unlikely(cur_state != MHI_PM_M0)) {
+		MHI_ERR("Failed to transition to state %s from %s\n",
+			to_mhi_pm_state_str(MHI_PM_M0),
+			to_mhi_pm_state_str(cur_state));
+		return -EIO;
+	}
+	mhi_cntrl->M0++;
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_get(mhi_cntrl, true);
+
+	/* ring all event rings and CMD ring only if we're in mission mode */
+	if (MHI_IN_MISSION_MODE(mhi_cntrl->ee)) {
+		struct mhi_event *mhi_event = mhi_cntrl->mhi_event;
+		struct mhi_cmd *mhi_cmd =
+			&mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING];
+
+	    MHI_LOG("MHI_IN_MISSION_MODE: ee %d", mhi_cntrl->ee);
+
+		for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+			if (mhi_event->offload_ev)
+				continue;
+
+			spin_lock_irq(&mhi_event->lock);
+			mhi_ring_er_db(mhi_event);
+			spin_unlock_irq(&mhi_event->lock);
+		}
+
+		/* only ring primary cmd ring */
+		spin_lock_irq(&mhi_cmd->lock);
+		if (mhi_cmd->ring.rp != mhi_cmd->ring.wp)
+			mhi_ring_cmd_db(mhi_cntrl, mhi_cmd);
+		spin_unlock_irq(&mhi_cmd->lock);
+	}
+
+    MHI_LOG("ring channel db registers");
+
+	/* ring channel db registers */
+	mhi_chan = mhi_cntrl->mhi_chan;
+	for (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {
+		struct mhi_ring *tre_ring = &mhi_chan->tre_ring;
+
+		write_lock_irq(&mhi_chan->lock);
+		if (mhi_chan->db_cfg.reset_req)
+			mhi_chan->db_cfg.db_mode = true;
+
+		/* only ring DB if ring is not empty */
+		if (tre_ring->base && tre_ring->wp  != tre_ring->rp)
+			mhi_ring_chan_db(mhi_cntrl, mhi_chan);
+		write_unlock_irq(&mhi_chan->lock);
+	}
+
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+	wake_up_all(&mhi_cntrl->state_event);
+
+	MHI_LOG("mission_mode_done %d\n", mhi_cntrl->mission_mode_done);
+	if (mhi_cntrl->mission_mode_done == false) {
+		mhi_cntrl->mission_mode_done = true; 
+		mhi_queue_state_transition(mhi_cntrl, MHI_ST_TRANSITION_MISSION_MODE);
+	}
+
+	MHI_VERB("Exited\n");
+	MHI_LOG("Exited\n");
+
+	return 0;
+}
+
+/**
+ * @brief M1 state handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+void mhi_pm_m1_transition(struct mhi_controller *mhi_cntrl)
+{
+	enum MHI_PM_STATE state;
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	/* if it fails, means we transition to M3 */
+	state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M2);
+	if (state == MHI_PM_M2) {
+		MHI_VERB("Entered M2 State\n");
+		mhi_set_mhi_state(mhi_cntrl, MHI_STATE_M2);
+		mhi_cntrl->dev_state = MHI_STATE_M2;
+		mhi_cntrl->M2++;
+
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		wake_up_all(&mhi_cntrl->state_event);
+
+		/* transfer pending, exit M2 immediately */
+		if (unlikely(atomic_read(&mhi_cntrl->pending_pkts) ||
+			     atomic_read(&mhi_cntrl->dev_wake))) {
+			MHI_VERB(
+				 "Exiting M2 Immediately, pending_pkts:%d dev_wake:%d\n",
+				 atomic_read(&mhi_cntrl->pending_pkts),
+				 atomic_read(&mhi_cntrl->dev_wake));
+			read_lock_bh(&mhi_cntrl->pm_lock);
+			mhi_cntrl->wake_get(mhi_cntrl, true);
+			mhi_cntrl->wake_put(mhi_cntrl, true);
+			read_unlock_bh(&mhi_cntrl->pm_lock);
+		} else {
+			mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+					     MHI_CB_IDLE);
+		}
+	} else {
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+	}
+}
+
+/**
+ * @brief M3 state handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+int mhi_pm_m3_transition(struct mhi_controller *mhi_cntrl)
+{
+	enum MHI_PM_STATE state;
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	mhi_cntrl->dev_state = MHI_STATE_M3;
+	state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M3);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+	if (state != MHI_PM_M3) {
+		MHI_ERR("Failed to transition to state %s from %s\n",
+			to_mhi_pm_state_str(MHI_PM_M3),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+	wake_up_all(&mhi_cntrl->state_event);
+	mhi_cntrl->M3++;
+
+	MHI_DEFAULT("Entered mhi_state:%s pm_state:%s\n",
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		to_mhi_pm_state_str(mhi_cntrl->pm_state));
+	return 0;
+}
+
+/**
+ * @brief mode transition handler
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ */
+static int mhi_pm_mission_mode_transition(struct mhi_controller *mhi_cntrl)
+{
+	int i, ret;
+	struct mhi_event *mhi_event;
+
+	MHI_LOG("Processing Mission Mode Transition\n");
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	if (MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+		mhi_cntrl->ee = mhi_get_exec_env(mhi_cntrl);
+        MHI_DEFAULT("current EE %s", TO_MHI_EXEC_STR(mhi_cntrl->ee));
+    }    
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	if (!MHI_IN_MISSION_MODE(mhi_cntrl->ee))
+		return -EIO;
+
+	wake_up_all(&mhi_cntrl->state_event);
+
+	mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+			     MHI_CB_EE_MISSION_MODE);
+
+	/* force MHI to be in M0 state before continuing */
+	ret = __mhi_device_get_sync(mhi_cntrl);
+	if (ret)
+		return ret;
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+
+	/* add elements to all HW event rings */
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		ret = -EIO;
+		goto error_mission_mode;
+	}
+
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		struct mhi_ring *ring = &mhi_event->ring;
+
+		if (mhi_event->offload_ev || !mhi_event->hw_ring)
+			continue;
+
+		ring->wp = ring->base + ring->len - ring->el_size;
+		*ring->ctxt_wp = ring->iommu_base + ring->len - ring->el_size;
+	    MHI_LOG("ring %d ctxt_wp 0x%p", i, ring->ctxt_wp);
+		/* all ring updates must get updated immediately */
+		smp_wmb();
+
+		spin_lock_irq(&mhi_event->lock);
+		if (MHI_DB_ACCESS_VALID(mhi_cntrl))
+			mhi_ring_er_db(mhi_event);
+		spin_unlock_irq(&mhi_event->lock);
+
+	}
+
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	/* only init timesync once */
+	if (!mhi_cntrl->timesync_done) {
+
+		/* setup support for time sync */
+		mhi_init_timesync(mhi_cntrl);
+
+		mhi_cntrl->timesync_done = true;
+	}
+
+	MHI_LOG("Adding new devices\n");
+
+	/* add supported devices */
+	//MHI_LOG("device mutex 0x%px lock\n", &mhi_cntrl->dev_mutex);
+    mutex_lock(&mhi_cntrl->dev_mutex);
+	MHI_LOG("create_device_done %d\n", mhi_cntrl->create_device_done);
+	if (mhi_cntrl->create_device_done == false) {
+		mhi_create_devices(mhi_cntrl);
+		mhi_cntrl->create_device_done = true;
+		
+		/* setup sysfs nodes for userspace votes */
+		mhi_create_vote_sysfs(mhi_cntrl);
+	}
+	//MHI_LOG("device mutex 0x%px unlock\n", &mhi_cntrl->dev_mutex);
+    mutex_unlock(&mhi_cntrl->dev_mutex);
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+
+error_mission_mode:
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	MHI_LOG("Exit with ret:%d\n", ret);
+
+	return ret;
+}
+
+/**
+ * @brief handles both sys_err and shutdown transitions
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            transition_state			pm state
+ */
+static void mhi_pm_disable_transition(struct mhi_controller *mhi_cntrl,
+				      enum MHI_PM_STATE transition_state)
+{
+	enum MHI_PM_STATE cur_state, prev_state;
+	struct mhi_event *mhi_event;
+	struct mhi_cmd_ctxt *cmd_ctxt;
+	struct mhi_cmd *mhi_cmd;
+	struct mhi_event_ctxt *er_ctxt;
+	int ret, i;
+
+	MHI_DEFAULT("Enter with from pm_state:%s MHI_STATE:%s to pm_state:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+		to_mhi_pm_state_str(transition_state));
+
+	/* We must notify MHI control driver so it can clean up first */
+	if (transition_state == MHI_PM_SYS_ERR_PROCESS) {
+		/*
+		 * if controller support rddm, we do not process
+		 * sys error state, instead we will jump directly
+		 * to rddm state
+		 */
+		if (mhi_cntrl->rddm_image) {
+			MHI_LOG(
+				"Controller Support RDDM, skipping SYS_ERR_PROCESS\n");
+			return;
+		}
+		mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+				     MHI_CB_SYS_ERROR);
+	}
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	prev_state = mhi_cntrl->pm_state;
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, transition_state);
+	if (cur_state == transition_state) {
+		mhi_cntrl->ee = MHI_EE_DISABLE_TRANSITION;
+		mhi_cntrl->dev_state = MHI_STATE_RESET;
+	}
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/* wake up any threads waiting for state transitions */
+	wake_up_all(&mhi_cntrl->state_event);
+
+	/* not handling sys_err, could be middle of shut down */
+	if (cur_state != transition_state) {
+		MHI_LOG("Failed to transition to state:0x%x from:0x%x\n",
+			transition_state, cur_state);
+		mutex_unlock(&mhi_cntrl->pm_mutex);
+		return;
+	}
+
+	/* trigger MHI RESET so device will not access host ddr */
+	if (MHI_REG_ACCESS_VALID(prev_state)) {
+		u32 in_reset = -1;
+		unsigned long timeout = msecs_to_jiffies(mhi_cntrl->timeout_ms);
+
+		MHI_LOG("Trigger device into MHI_RESET\n");
+		mhi_set_mhi_state(mhi_cntrl, MHI_STATE_RESET);
+
+		/* wait for reset to be cleared */
+		ret = wait_event_timeout(mhi_cntrl->state_event,
+					 mhi_read_reg_field(mhi_cntrl,
+						mhi_cntrl->regs, MHICTRL,
+						MHICTRL_RESET_MASK,
+						MHICTRL_RESET_SHIFT, &in_reset)
+					 || !in_reset, timeout);
+		if ((!ret || in_reset) && cur_state == MHI_PM_SYS_ERR_PROCESS) {
+			MHI_CRITICAL("Device failed to exit RESET state\n");
+			mutex_unlock(&mhi_cntrl->pm_mutex);
+			return;
+		}
+
+		/*
+		 * device cleares INTVEC as part of RESET processing,
+		 * re-program it
+		 */
+		mhi_write_reg(mhi_cntrl, mhi_cntrl->bhi, BHI_INTVEC, 0);
+	}
+
+	MHI_LOG("Waiting for all pending event ring processing to complete\n");
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		if (mhi_event->offload_ev)
+			continue;
+		tasklet_kill(&mhi_event->task);
+	}
+
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	MHI_LOG("Reset all active channels and remove mhi devices\n");
+	//MHI_LOG("device mutex 0x%px lock\n", &mhi_cntrl->dev_mutex);
+    mutex_lock(&mhi_cntrl->dev_mutex);
+	device_for_each_child(mhi_cntrl->dev, NULL, mhi_destroy_device);
+	//MHI_LOG("device mutex 0x%px unlock\n", &mhi_cntrl->dev_mutex);
+	mhi_cntrl->create_device_done = false;
+
+	MHI_LOG("Finish resetting channels\n");
+
+	/* remove support for userspace votes */
+	mhi_destroy_vote_sysfs(mhi_cntrl);
+    mutex_unlock(&mhi_cntrl->dev_mutex);
+
+	MHI_LOG("Waiting for all pending threads to complete\n");
+	wake_up_all(&mhi_cntrl->state_event);
+	flush_work(&mhi_cntrl->st_worker);
+	flush_work(&mhi_cntrl->fw_worker);
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+
+	MHI_ASSERT(atomic_read(&mhi_cntrl->dev_wake), "dev_wake != 0");
+	MHI_ASSERT(atomic_read(&mhi_cntrl->pending_pkts), "pending_pkts != 0");
+
+	/* reset the ev rings and cmd rings */
+	MHI_LOG("Resetting EV CTXT and CMD CTXT\n");
+	mhi_cmd = mhi_cntrl->mhi_cmd;
+	cmd_ctxt = mhi_cntrl->mhi_ctxt->cmd_ctxt;
+	for (i = 0; i < NR_OF_CMD_RINGS; i++, mhi_cmd++, cmd_ctxt++) {
+		struct mhi_ring *ring = &mhi_cmd->ring;
+
+		ring->rp = ring->base;
+		ring->wp = ring->base;
+		cmd_ctxt->rp = cmd_ctxt->rbase;
+		cmd_ctxt->wp = cmd_ctxt->rbase;
+
+		/* make sure db_addr is valid before ring it */
+		if (ring->db_addr) 
+			mhi_ring_cmd_db(mhi_cntrl, mhi_cmd);
+	}
+
+	mhi_event = mhi_cntrl->mhi_event;
+	er_ctxt = mhi_cntrl->mhi_ctxt->er_ctxt;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, er_ctxt++,
+		     mhi_event++) {
+		struct mhi_ring *ring = &mhi_event->ring;
+
+		/* do not touch offload er */
+		if (mhi_event->offload_ev)
+			continue;
+
+		ring->rp = ring->base;
+		ring->wp = ring->base;
+		er_ctxt->rp = er_ctxt->rbase;
+		er_ctxt->wp = er_ctxt->rbase;
+	}
+
+	/* remove support for time sync */
+	mhi_destroy_timesync(mhi_cntrl);
+
+	if (cur_state == MHI_PM_SYS_ERR_PROCESS) {
+		mhi_ready_state_transition(mhi_cntrl);
+	} else {
+		/* move to disable state */
+		write_lock_irq(&mhi_cntrl->pm_lock);
+		cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_DISABLE);
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		if (unlikely(cur_state != MHI_PM_DISABLE))
+			MHI_ERR("Error moving from pm state:%s to state:%s\n",
+				to_mhi_pm_state_str(cur_state),
+				to_mhi_pm_state_str(MHI_PM_DISABLE));
+	}
+
+	MHI_DEFAULT("Exit with pm_state:%s mhi_state:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+}
+
+#ifdef CONFIG_DEBUG_FS
+
+/**
+ * @brief trigger mhi reset
+ *
+ * @param[in ]            data						mhi controller
+ * @param[in ]            val						value
+ */
+int mhi_debugfs_trigger_reset(void *data, u64 val)
+{
+	struct mhi_controller *mhi_cntrl = data;
+	enum MHI_PM_STATE cur_state;
+	int ret;
+
+	MHI_LOG("Trigger MHI Reset\n");
+
+	/* exit lpm first */
+	mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+	mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->dev_state == MHI_STATE_M0 ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("Did not enter M0 state, cur_state:%s pm_state:%s\n",
+			TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_SYS_ERR_DETECT);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	if (cur_state == MHI_PM_SYS_ERR_DETECT)
+		schedule_work(&mhi_cntrl->syserr_worker);
+
+	return 0;
+}
+#endif
+
+/**
+ * @brief queue a new work item and scheduler work
+ *
+ * @param[in ]            mhi_cntrl					mhi controller
+ * @param[in ]            state						state
+ */
+int mhi_queue_state_transition(struct mhi_controller *mhi_cntrl,
+			       enum MHI_ST_TRANSITION state)
+{
+	struct state_transition *item = kmalloc(sizeof(*item), GFP_ATOMIC);
+	unsigned long flags;
+
+	if (!item)
+		return -ENOMEM;
+
+	item->state = state;
+	spin_lock_irqsave(&mhi_cntrl->transition_lock, flags);
+	list_add_tail(&item->node, &mhi_cntrl->transition_list);
+	spin_unlock_irqrestore(&mhi_cntrl->transition_lock, flags);
+
+	schedule_work(&mhi_cntrl->st_worker);
+
+	return 0;
+}
+
+/**
+ * @brief sys error handler
+ *
+ * @param[in ]            work 					work item
+ */
+void mhi_pm_sys_err_worker(struct work_struct *work)
+{
+	struct mhi_controller *mhi_cntrl = container_of(work,
+							struct mhi_controller,
+							syserr_worker);
+
+	MHI_DEFAULT("Enter with pm_state:%s MHI_STATE:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	mhi_pm_disable_transition(mhi_cntrl, MHI_PM_SYS_ERR_PROCESS);
+}
+
+/**
+ * @brief power state handler
+ *
+ * @param[in ]            work 					work item
+ */
+void mhi_pm_st_worker(struct work_struct *work)
+{
+	struct state_transition *itr, *tmp;
+	LIST_HEAD(head);
+	struct mhi_controller *mhi_cntrl = container_of(work,
+							struct mhi_controller,
+							st_worker);
+	spin_lock_irq(&mhi_cntrl->transition_lock);
+	list_splice_tail_init(&mhi_cntrl->transition_list, &head);
+	spin_unlock_irq(&mhi_cntrl->transition_lock);
+
+	list_for_each_entry_safe(itr, tmp, &head, node) {
+		list_del(&itr->node);
+		MHI_LOG("Transition to state:%s\n",
+			TO_MHI_STATE_TRANS_STR(itr->state));
+
+		switch (itr->state) {
+		case MHI_ST_TRANSITION_PBL:
+			write_lock_irq(&mhi_cntrl->pm_lock);
+			if (MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {
+				mhi_cntrl->ee = mhi_get_exec_env(mhi_cntrl);
+                MHI_DEFAULT("current EE %s", TO_MHI_EXEC_STR(mhi_cntrl->ee));
+            }    
+			write_unlock_irq(&mhi_cntrl->pm_lock);
+			if (MHI_IN_PBL(mhi_cntrl->ee))
+				wake_up_all(&mhi_cntrl->state_event);
+			break;
+		case MHI_ST_TRANSITION_SBL:
+			write_lock_irq(&mhi_cntrl->pm_lock);
+			mhi_cntrl->ee = MHI_EE_SBL;
+			write_unlock_irq(&mhi_cntrl->pm_lock);
+			wake_up_all(&mhi_cntrl->state_event);
+           	//MHI_LOG("device mutex 0x%px lock\n", &mhi_cntrl->dev_mutex);
+            mutex_lock(&mhi_cntrl->dev_mutex);
+			if (mhi_cntrl->create_device_done == false) {
+				mhi_create_devices(mhi_cntrl);
+				mhi_cntrl->create_device_done = true;
+			}
+           	//MHI_LOG("device mutex 0x%px unlock\n", &mhi_cntrl->dev_mutex);
+            mutex_unlock(&mhi_cntrl->dev_mutex);
+            break;
+		case MHI_ST_TRANSITION_MISSION_MODE:
+			mhi_pm_mission_mode_transition(mhi_cntrl);
+			break;
+		case MHI_ST_TRANSITION_READY:
+			mhi_ready_state_transition(mhi_cntrl);
+			break;
+		default:
+			break;
+		}
+		kfree(itr);
+	}
+}
+
+/**
+ * @brief async power up device 
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_async_power_up(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+	u32 val;
+	enum mhi_ee current_ee;
+	enum MHI_ST_TRANSITION next_state;
+
+	MHI_LOG("Requested to power on\n");
+
+#if 0
+	if (mhi_cntrl->msi_allocated < mhi_cntrl->total_ev_rings)
+		return -EINVAL;
+#endif
+
+	/* set to default wake if any one is not set */
+	if (!mhi_cntrl->wake_get || !mhi_cntrl->wake_put ||
+	    !mhi_cntrl->wake_toggle) {
+		mhi_cntrl->wake_get = mhi_assert_dev_wake;
+		mhi_cntrl->wake_put = mhi_deassert_dev_wake;
+		mhi_cntrl->wake_toggle = (mhi_cntrl->db_access & MHI_PM_M2) ?
+			mhi_toggle_dev_wake_nop : mhi_toggle_dev_wake;
+	}
+
+	mutex_lock(&mhi_cntrl->pm_mutex);
+	mhi_cntrl->pm_state = MHI_PM_DISABLE;
+
+	if (!mhi_cntrl->pre_init) {
+		/* setup device context */
+	    MHI_LOG("mhi_init_ctrl_seq");
+        mhi_init_ctrl_seq(mhi_cntrl);
+
+	    MHI_LOG("mhi_init_dev_ctxt");
+		ret = mhi_init_dev_ctxt(mhi_cntrl);
+		if (ret) {
+			MHI_ERR("Error setting dev_context\n");
+			goto error_dev_ctxt;
+		}
+	}
+
+	MHI_LOG("mhi_init_irq_setup");
+	ret = mhi_init_irq_setup(mhi_cntrl);
+	if (ret) {
+		MHI_ERR("Error setting up irq\n");
+		goto error_setup_irq;
+	}
+
+	/* setup bhi offset & intvec */
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->regs, BHIOFF, &val);
+	if (ret) {
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		MHI_ERR("Error getting bhi offset\n");
+		goto error_bhi_offset;
+	}
+
+	mhi_cntrl->bhi = mhi_cntrl->regs + val;
+
+	/* setup bhie offset if not set */
+	if (mhi_cntrl->fbc_download && !mhi_cntrl->bhie) {
+		ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->regs, BHIEOFF, &val);
+		if (ret) {
+			write_unlock_irq(&mhi_cntrl->pm_lock);
+			MHI_ERR("Error getting bhie offset\n");
+			goto error_bhi_offset;
+		}
+
+		mhi_cntrl->bhie = mhi_cntrl->regs + val;
+	}
+
+	mhi_write_reg(mhi_cntrl, mhi_cntrl->bhi, BHI_INTVEC, 0);
+	mhi_cntrl->pm_state = MHI_PM_POR;
+	mhi_cntrl->ee = MHI_EE_MAX;
+	current_ee = mhi_get_exec_env(mhi_cntrl);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+    MHI_DEFAULT("current EE %s", TO_MHI_EXEC_STR(current_ee));
+
+	/* confirm device is in valid exec env */
+	if (!MHI_IN_PBL(current_ee) && current_ee != MHI_EE_AMSS && 
+		current_ee != MHI_EE_SBL) {
+		MHI_ERR("Not a valid ee for power on\n");
+		ret = -EIO;
+		goto error_bhi_offset;
+	}
+
+	/* transition to next state */
+	next_state = MHI_IN_PBL(current_ee) ?
+		MHI_ST_TRANSITION_PBL : MHI_ST_TRANSITION_READY;
+
+	if (next_state == MHI_ST_TRANSITION_PBL)
+		schedule_work(&mhi_cntrl->fw_worker);
+
+	mhi_queue_state_transition(mhi_cntrl, next_state);
+
+	mhi_init_debugfs(mhi_cntrl);
+
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	MHI_LOG("Power on setup success\n");
+
+	return 0;
+
+error_bhi_offset:
+	mhi_deinit_free_irq(mhi_cntrl);
+
+error_setup_irq:
+	if (!mhi_cntrl->pre_init)
+		mhi_deinit_dev_ctxt(mhi_cntrl);
+
+error_dev_ctxt:
+	mutex_unlock(&mhi_cntrl->pm_mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_async_power_up);
+
+/**
+ * @brief transition MHI into error state and notify critical clients
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+void mhi_control_error(struct mhi_controller *mhi_cntrl)
+{
+	enum MHI_PM_STATE cur_state;
+
+	MHI_LOG("Enter with pm_state:%s MHI_STATE:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_LD_ERR_FATAL_DETECT);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	if (cur_state != MHI_PM_LD_ERR_FATAL_DETECT) {
+		MHI_ERR("Failed to transition to state:%s from:%s\n",
+			to_mhi_pm_state_str(MHI_PM_LD_ERR_FATAL_DETECT),
+			to_mhi_pm_state_str(cur_state));
+		goto exit_control_error;
+	}
+
+	/* start notifying all clients who request early notification */
+	device_for_each_child(mhi_cntrl->dev, NULL, mhi_early_notify_device);
+
+exit_control_error:
+	MHI_LOG("Exit with pm_state:%s MHI_STATE:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+}
+EXPORT_SYMBOL(mhi_control_error);
+
+/**
+ * @brief power down device
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            graceful 					if need to force the linkdown
+ */
+void mhi_power_down(struct mhi_controller *mhi_cntrl, bool graceful)
+{
+	enum MHI_PM_STATE cur_state;
+
+	/* if it's not graceful shutdown, force MHI to a linkdown state */
+	if (!graceful) {
+		mutex_lock(&mhi_cntrl->pm_mutex);
+		write_lock_irq(&mhi_cntrl->pm_lock);
+		cur_state = mhi_tryset_pm_state(mhi_cntrl,
+						MHI_PM_LD_ERR_FATAL_DETECT);
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		mutex_unlock(&mhi_cntrl->pm_mutex);
+		if (cur_state != MHI_PM_LD_ERR_FATAL_DETECT)
+			MHI_ERR("Failed to move to state:%s from:%s\n",
+				to_mhi_pm_state_str(MHI_PM_LD_ERR_FATAL_DETECT),
+				to_mhi_pm_state_str(mhi_cntrl->pm_state));
+	}
+	mhi_pm_disable_transition(mhi_cntrl, MHI_PM_SHUTDOWN_PROCESS);
+
+	mhi_deinit_debugfs(mhi_cntrl);
+
+	mhi_deinit_free_irq(mhi_cntrl);
+
+	if (!mhi_cntrl->pre_init) {
+		/* free all allocated resources */
+		if (mhi_cntrl->fbc_image) {
+			mhi_free_bhie_table(mhi_cntrl, mhi_cntrl->fbc_image);
+			mhi_cntrl->fbc_image = NULL;
+		}
+		mhi_deinit_dev_ctxt(mhi_cntrl);
+	}
+}
+EXPORT_SYMBOL(mhi_power_down);
+
+/**
+ * @brief power up in sync
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_sync_power_up(struct mhi_controller *mhi_cntrl)
+{
+	int ret = mhi_async_power_up(mhi_cntrl);
+
+	if (ret)
+		return ret;
+
+	wait_event_timeout(mhi_cntrl->state_event,
+			   MHI_IN_MISSION_MODE(mhi_cntrl->ee) ||
+			   MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+			   msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	return (MHI_IN_MISSION_MODE(mhi_cntrl->ee)) ? 0 : -EIO;
+}
+EXPORT_SYMBOL(mhi_sync_power_up);
+
+/**
+ * @brief suspend device
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_pm_suspend(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+	enum MHI_PM_STATE new_state;
+	struct mhi_chan *itr, *tmp;
+	struct mhi_device *mhi_dev = mhi_cntrl->mhi_dev;
+
+	if (mhi_cntrl->pm_state == MHI_PM_DISABLE)
+		return -EINVAL;
+
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	/* do a quick check to see if any pending votes to keep us busy */
+	if (atomic_read(&mhi_cntrl->dev_wake) ||
+	    atomic_read(&mhi_cntrl->pending_pkts) ||
+	    atomic_read(&mhi_dev->bus_vote)) {
+		MHI_VERB("Busy, aborting M3\n");
+		return -EBUSY;
+	}
+
+	/* exit MHI out of M2 state */
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_get(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->dev_state == MHI_STATE_M0 ||
+				 mhi_cntrl->dev_state == MHI_STATE_M1 ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR(
+			"Did not enter M0||M1 state, cur_state:%s pm_state:%s\n",
+			TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		ret = -EIO;
+		goto error_m0_entry;
+	}
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+
+	/*
+	 * Check the votes once more to see if we should abort
+	 * suepend. We're asserting wake so count would be @ least 1
+	 */
+	if (atomic_read(&mhi_cntrl->dev_wake) > 1 ||
+	    atomic_read(&mhi_cntrl->pending_pkts) ||
+	    atomic_read(&mhi_dev->bus_vote)) {
+		MHI_VERB("Busy, aborting M3\n");
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		ret = -EBUSY;
+		goto error_m0_entry;
+	}
+
+	/* anytime after this, we will resume thru runtime pm framework */
+	MHI_LOG("Allowing M3 transition\n");
+	new_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M3_ENTER);
+	if (new_state != MHI_PM_M3_ENTER) {
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		MHI_ERR("Error setting to pm_state:%s from pm_state:%s\n",
+			to_mhi_pm_state_str(MHI_PM_M3_ENTER),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+
+		ret = -EIO;
+		goto error_m0_entry;
+	}
+
+	/* set dev to M3 and wait for completion */
+	mhi_set_mhi_state(mhi_cntrl, MHI_STATE_M3);
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+	MHI_LOG("Wait for M3 completion\n");
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->dev_state == MHI_STATE_M3 ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("Did not enter M3 state, cur_state:%s pm_state:%s\n",
+			TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+
+	/* notify any clients we enter lpm */
+	list_for_each_entry_safe(itr, tmp, &mhi_cntrl->lpm_chans, node) {
+		mutex_lock(&itr->mutex);
+		if (itr->mhi_dev)
+			mhi_notify(itr->mhi_dev, MHI_CB_LPM_ENTER);
+		mutex_unlock(&itr->mutex);
+	}
+
+	return 0;
+
+error_m0_entry:
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_pm_suspend);
+
+/**
+ * @brief Faster suspend path where we transition host to inactive state 
+ * w/o suspending device. Useful for cases where we want apps to
+ * go into power collapse but keep the physical link in active state.
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            notify_client 			if need to notify client
+ */
+int mhi_pm_fast_suspend(struct mhi_controller *mhi_cntrl, bool notify_client)
+{
+	int ret;
+	enum MHI_PM_STATE new_state;
+	struct mhi_chan *itr, *tmp;
+
+	if (mhi_cntrl->pm_state == MHI_PM_DISABLE)
+		return -EINVAL;
+
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	/* do a quick check to see if any pending votes to keep us busy */
+	if (atomic_read(&mhi_cntrl->pending_pkts)) {
+		MHI_VERB("Busy, aborting M3\n");
+		return -EBUSY;
+	}
+
+	/* disable ctrl event processing */
+	tasklet_disable(&mhi_cntrl->mhi_event->task);
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+
+	/*
+	 * Check the votes once more to see if we should abort
+	 * suspend.
+	 */
+	if (atomic_read(&mhi_cntrl->pending_pkts)) {
+		MHI_VERB("Busy, aborting M3\n");
+		ret = -EBUSY;
+		goto error_suspend;
+	}
+
+	/* anytime after this, we will resume thru runtime pm framework */
+	MHI_LOG("Allowing Fast M3 transition\n");
+
+	/* save the current states */
+	mhi_cntrl->saved_pm_state = mhi_cntrl->pm_state;
+	mhi_cntrl->saved_dev_state = mhi_cntrl->dev_state;
+
+	/* If we're in M2, we need to switch back to M0 first */
+	if (mhi_cntrl->pm_state == MHI_PM_M2) {
+		new_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M0);
+		if (new_state != MHI_PM_M0) {
+			MHI_ERR("Error set pm_state to:%s from pm_state:%s\n",
+				to_mhi_pm_state_str(MHI_PM_M0),
+				to_mhi_pm_state_str(mhi_cntrl->pm_state));
+			ret = -EIO;
+			goto error_suspend;
+		}
+	}
+
+	new_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M3_ENTER);
+	if (new_state != MHI_PM_M3_ENTER) {
+		MHI_ERR("Error setting to pm_state:%s from pm_state:%s\n",
+			to_mhi_pm_state_str(MHI_PM_M3_ENTER),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		ret = -EIO;
+		goto error_suspend;
+	}
+
+	/* set dev to M3_FAST and host to M3 */
+	new_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M3);
+	if (new_state != MHI_PM_M3) {
+		MHI_ERR("Error setting to pm_state:%s from pm_state:%s\n",
+			to_mhi_pm_state_str(MHI_PM_M3),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		ret = -EIO;
+		goto error_suspend;
+	}
+
+	mhi_cntrl->dev_state = MHI_STATE_M3_FAST;
+	mhi_cntrl->M3_FAST++;
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/* now safe to check ctrl event ring */
+	tasklet_enable(&mhi_cntrl->mhi_event->task);
+	mhi_msi_handlr(0, mhi_cntrl->mhi_event);
+
+	if (!notify_client)
+		return 0;
+
+	/* notify any clients we enter lpm */
+	list_for_each_entry_safe(itr, tmp, &mhi_cntrl->lpm_chans, node) {
+		mutex_lock(&itr->mutex);
+		if (itr->mhi_dev)
+			mhi_notify(itr->mhi_dev, MHI_CB_LPM_ENTER);
+		mutex_unlock(&itr->mutex);
+	}
+
+	return 0;
+
+error_suspend:
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	/* check ctrl event ring for pending work */
+	tasklet_enable(&mhi_cntrl->mhi_event->task);
+	mhi_msi_handlr(0, mhi_cntrl->mhi_event);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_pm_fast_suspend);
+
+/**
+ * @brief resume device
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_pm_resume(struct mhi_controller *mhi_cntrl)
+{
+	enum MHI_PM_STATE cur_state;
+	int ret;
+	struct mhi_chan *itr, *tmp;
+
+	MHI_DEFAULT("Entered with pm_state:%s dev_state:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	if (mhi_cntrl->pm_state == MHI_PM_DISABLE)
+		return 0;
+
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	MHI_ASSERT(mhi_cntrl->pm_state != MHI_PM_M3, "mhi_pm_state != M3");
+
+	/* notify any clients we enter lpm */
+	list_for_each_entry_safe(itr, tmp, &mhi_cntrl->lpm_chans, node) {
+		mutex_lock(&itr->mutex);
+		if (itr->mhi_dev)
+			mhi_notify(itr->mhi_dev, MHI_CB_LPM_EXIT);
+		mutex_unlock(&itr->mutex);
+	}
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	cur_state = mhi_tryset_pm_state(mhi_cntrl, MHI_PM_M3_EXIT);
+	if (cur_state != MHI_PM_M3_EXIT) {
+		write_unlock_irq(&mhi_cntrl->pm_lock);
+		MHI_ERR("Error setting to pm_state:%s from pm_state:%s\n",
+			to_mhi_pm_state_str(MHI_PM_M3_EXIT),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		return -EIO;
+	}
+
+	/* set dev to M0 and wait for completion */
+	mhi_cntrl->wake_get(mhi_cntrl, true);
+	mhi_set_mhi_state(mhi_cntrl, MHI_STATE_M0);
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->dev_state == MHI_STATE_M0 ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_put(mhi_cntrl, false);
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("Did not enter M0 state, cur_state:%s pm_state:%s\n",
+			TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+
+		/*
+		 * It's possible device already in error state and we didn't
+		 * process it due to low power mode, force a check
+		 */
+		mhi_intvec_threaded_handlr(0, mhi_cntrl);
+		return -EIO;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(mhi_pm_resume);
+
+/**
+ * @brief fast resume device
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ * @param[in ]            notify_client 			if notify client
+ */
+int mhi_pm_fast_resume(struct mhi_controller *mhi_cntrl, bool notify_client)
+{
+	struct mhi_chan *itr, *tmp;
+	struct mhi_event *mhi_event;
+	int i;
+
+	MHI_DEFAULT("Entered with pm_state:%s dev_state:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	if (mhi_cntrl->pm_state == MHI_PM_DISABLE)
+		return 0;
+
+	if (MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state))
+		return -EIO;
+
+	MHI_ASSERT(mhi_cntrl->pm_state != MHI_PM_M3, "mhi_pm_state != M3");
+
+	/* notify any clients we're about to exit lpm */
+	if (notify_client) {
+		list_for_each_entry_safe(itr, tmp, &mhi_cntrl->lpm_chans,
+					 node) {
+			mutex_lock(&itr->mutex);
+			if (itr->mhi_dev)
+				mhi_notify(itr->mhi_dev, MHI_CB_LPM_EXIT);
+			mutex_unlock(&itr->mutex);
+		}
+	}
+
+	write_lock_irq(&mhi_cntrl->pm_lock);
+	/* restore the states */
+	mhi_cntrl->pm_state = mhi_cntrl->saved_pm_state;
+	mhi_cntrl->dev_state = mhi_cntrl->saved_dev_state;
+	write_unlock_irq(&mhi_cntrl->pm_lock);
+
+	switch (mhi_cntrl->pm_state) {
+	case MHI_PM_M0:
+		mhi_pm_m0_transition(mhi_cntrl);
+		goto pm_2;
+	case MHI_PM_M2:
+pm_2:	
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		/*
+		 * we're doing a double check of pm_state because by the time we
+		 * grab the pm_lock, device may have already initiate a M0 on
+		 * its own. If that's the case we should not be toggling device
+		 * wake.
+		 */
+		if (mhi_cntrl->pm_state == MHI_PM_M2) {
+			mhi_cntrl->wake_get(mhi_cntrl, true);
+			mhi_cntrl->wake_put(mhi_cntrl, true);
+		}
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+	}
+
+	/*
+	 * In fast suspend/resume case device is not aware host transition
+	 * to suspend state. So, device could be triggering a interrupt while
+	 * host not accepting MSI. We have to manually check each event ring
+	 * upon resume.
+	 */
+	mhi_event = mhi_cntrl->mhi_event;
+	for (i = 0; i < mhi_cntrl->total_ev_rings; i++, mhi_event++) {
+		if (mhi_event->offload_ev)
+			continue;
+
+		mhi_msi_handlr(0, mhi_event);
+	}
+
+	MHI_DEFAULT("Exit with pm_state:%s dev_state:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_STATE_STR(mhi_cntrl->dev_state));
+
+	return 0;
+}
+EXPORT_SYMBOL(mhi_pm_fast_resume);
+
+/**
+ * @brief idle device sync
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int __mhi_device_get_sync(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+
+	read_lock_bh(&mhi_cntrl->pm_lock);
+	mhi_cntrl->wake_get(mhi_cntrl, true);
+	if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+		pm_wakeup_event(&mhi_cntrl->mhi_dev->dev, 0);
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+	}
+	read_unlock_bh(&mhi_cntrl->pm_lock);
+
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->pm_state == MHI_PM_M0 ||
+				 MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state),
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+
+	if (!ret || MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)) {
+		MHI_ERR("Did not enter M0 state, cur_state:%s pm_state:%s\n",
+			TO_MHI_STATE_STR(mhi_cntrl->dev_state),
+			to_mhi_pm_state_str(mhi_cntrl->pm_state));
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		mhi_cntrl->wake_put(mhi_cntrl, false);
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * @brief idle device
+ *
+ * @param[in ]            mhi_dev 					mhi device
+ * @param[in ]            vote 						vote
+ */
+void mhi_device_get(struct mhi_device *mhi_dev, int vote)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+	if (vote & MHI_VOTE_DEVICE) {
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		mhi_cntrl->wake_get(mhi_cntrl, true);
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+		atomic_inc(&mhi_dev->dev_vote);
+	}
+
+	if (vote & MHI_VOTE_BUS) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		atomic_inc(&mhi_dev->bus_vote);
+	}
+}
+EXPORT_SYMBOL(mhi_device_get);
+
+/**
+ * @brief idle device
+ *
+ * @param[in ]            mhi_dev 					mhi device
+ * @param[in ]            vote 						vote
+ */
+int mhi_device_get_sync(struct mhi_device *mhi_dev, int vote)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+	int ret;
+
+	/*
+	 * regardless of any vote we will bring device out lpm and assert
+	 * device wake
+	 */
+	ret = __mhi_device_get_sync(mhi_cntrl);
+	if (ret)
+		return ret;
+
+	if (vote & MHI_VOTE_DEVICE) {
+		atomic_inc(&mhi_dev->dev_vote);
+	} else {
+		/* client did not requested device vote so de-assert dev_wake */
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		mhi_cntrl->wake_put(mhi_cntrl, false);
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+	}
+
+	if (vote & MHI_VOTE_BUS) {
+		mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+		atomic_inc(&mhi_dev->bus_vote);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(mhi_device_get_sync);
+
+/**
+ * @brief resume device
+ *
+ * @param[in ]            mhi_dev 					mhi device
+ * @param[in ]            vote 						vote
+ */
+void mhi_device_put(struct mhi_device *mhi_dev, int vote)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+	if (vote & MHI_VOTE_DEVICE) {
+		atomic_dec(&mhi_dev->dev_vote);
+		read_lock_bh(&mhi_cntrl->pm_lock);
+		if (MHI_PM_IN_SUSPEND_STATE(mhi_cntrl->pm_state)) {
+			mhi_cntrl->runtime_get(mhi_cntrl, mhi_cntrl->priv_data);
+			mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+		}
+		mhi_cntrl->wake_put(mhi_cntrl, false);
+		read_unlock_bh(&mhi_cntrl->pm_lock);
+	}
+
+	if (vote & MHI_VOTE_BUS) {
+		atomic_dec(&mhi_dev->bus_vote);
+		mhi_cntrl->runtime_put(mhi_cntrl, mhi_cntrl->priv_data);
+
+		/*
+		 * if counts reach 0, clients release all votes
+		 * send idle cb to to attempt suspend
+		 */
+		if (!atomic_read(&mhi_dev->bus_vote))
+			mhi_cntrl->status_cb(mhi_cntrl, mhi_cntrl->priv_data,
+					     MHI_CB_IDLE);
+	}
+}
+EXPORT_SYMBOL(mhi_device_put);
+
+/**
+ * @brief force device to enter rddm mode
+ *
+ * @param[in ]            mhi_cntrl 				mhi controller
+ */
+int mhi_force_rddm_mode(struct mhi_controller *mhi_cntrl)
+{
+	int ret;
+
+	MHI_LOG("Enter with pm_state:%s ee:%s\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_EXEC_STR(mhi_cntrl->ee));
+
+	/* device already in rddm */
+	if (mhi_cntrl->ee == MHI_EE_RDDM)
+		return 0;
+
+	MHI_LOG("Triggering SYS_ERR to force rddm state\n");
+	mhi_set_mhi_state(mhi_cntrl, MHI_STATE_SYS_ERR);
+
+	/* wait for rddm event */
+	MHI_LOG("Waiting for device to enter RDDM state\n");
+	ret = wait_event_timeout(mhi_cntrl->state_event,
+				 mhi_cntrl->ee == MHI_EE_RDDM,
+				 msecs_to_jiffies(mhi_cntrl->timeout_ms));
+	ret = ret ? 0 : -EIO;
+
+	MHI_LOG("Exiting with pm_state:%s ee:%s ret:%d\n",
+		to_mhi_pm_state_str(mhi_cntrl->pm_state),
+		TO_MHI_EXEC_STR(mhi_cntrl->ee), ret);
+
+	return ret;
+}
+EXPORT_SYMBOL(mhi_force_rddm_mode);
diff --git a/drivers/staging/em9190/devices/Makefile b/drivers/staging/em9190/devices/Makefile
new file mode 100644
index 000000000000..12f7ba4e9a98
--- /dev/null
+++ b/drivers/staging/em9190/devices/Makefile
@@ -0,0 +1,33 @@
+# Comment/uncomment the following line to disable/enable debugging
+#DEBUG = y
+
+# Add your debugging flag (or not) to CFLAGS
+ifeq ($(DEBUG),y)
+  DEBFLAGS = -O2 -g -DMHINET_DEBUG # "-O" is needed to expand inlines
+else
+  DEBFLAGS = -O2
+endif
+
+ccflags-$(DEBUG):=$(DEBFLAGS)
+
+obj-m	:= mhiuci.o mhinet.o mhitty.o
+mhiuci-objs := mhi_uci.o qmap.o
+mhinet-objs := mhi_netdev.o
+mhitty-objs := mhi_tty.o
+
+KERNELDIR ?= /lib/modules/$(shell uname -r)/build
+PWD       := $(shell pwd)
+
+all:
+	$(MAKE) -C $(KERNELDIR) M=$(PWD)
+
+clean:
+	rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions *.o.ur-safe *.symvers *.order .cache.mk
+
+install:
+	sudo cp ./mhinet.ko /lib/modules/`uname -r`/kernel/drivers/pci/mhinet.ko
+	sudo cp ./mhiuci.ko /lib/modules/`uname -r`/kernel/drivers/pci/mhiuci.ko
+	sudo cp ./mhiuci.ko /lib/modules/`uname -r`/kernel/drivers/pci/mhitty.ko
+	
+
+
diff --git a/drivers/staging/em9190/devices/mhi_netdev.c b/drivers/staging/em9190/devices/mhi_netdev.c
new file mode 100644
index 000000000000..750e6ae5277e
--- /dev/null
+++ b/drivers/staging/em9190/devices/mhi_netdev.c
@@ -0,0 +1,2286 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/in.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/ip.h>          /* struct iphdr */
+#include <linux/tcp.h>         /* struct tcphdr */
+#include <linux/skbuff.h>
+#include <linux/if_vlan.h>
+#include <linux/mii.h>
+#include <linux/in6.h>
+#include <linux/if_vlan.h>
+#include "../inc/msm_rmnet.h"
+#include <linux/if_arp.h>
+#include <linux/dma-mapping.h>
+#include <linux/debugfs.h>
+#include "../inc/ipc_logging.h"
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/of_device.h>
+#include <linux/rtnetlink.h>
+#include <linux/version.h>
+#include <linux/ipv6.h>
+#include <net/addrconf.h>
+#include <linux/hrtimer.h>
+#include <linux/usb.h>
+#include <linux/usb/usbnet.h>
+#include <linux/usb/cdc.h>
+#include <linux/usb/cdc_ncm.h>
+#include <linux/time64.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+
+#include "../inc/mhi.h"
+
+#include "qmap.h"
+
+#define MHI_NETDEV_DRIVER_NAME "mhi_netdev"
+#define WATCHDOG_TIMEOUT (30 * HZ)
+#define IPC_LOG_PAGES (100)
+#define MAX_NETBUF_SIZE (128)
+
+#define HEADER_RESERVE 32
+
+enum data_dir {
+	RX_DATA	= 0,
+	TX_DATA	= 1
+};
+
+#define DROP_PKT_IF_RING_FULL	0
+
+ /* alternative VLAN for IP session 0 if not untagged */
+#define MBIM_IPS0_VID	4094
+
+/* flags for the cdc_mbim_state.flags field */
+enum cdc_mbim_flags {
+	FLAG_IPS0_VLAN = 1 << 0,	/* IP session 0 is tagged  */
+};
+
+/* mac address */
+u8 mac_addr[6] = { 0x00, 0xa0, 0xd5, 0xff, 0xf0, 0xb0 };
+
+bool debug = false;
+
+module_param(debug, bool, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug,"enable/disable driver logging");
+
+int debug_level = MHI_MSG_LVL_INFO;
+module_param(debug_level, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug_level,"driver logging level");
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0)
+
+#ifndef ETH_P_MAP
+#define ETH_P_MAP 0xDA1A
+#endif
+
+#ifndef ARPHRD_RAWIP
+#define ARPHRD_RAWIP    519		/* Raw IP                       */
+#endif
+
+static inline void *skb_put_data(struct sk_buff *skb, const void *data,
+				 unsigned int len)
+{
+	void *tmp = skb_put(skb, len);
+
+	memcpy(tmp, data, len);
+
+	return tmp;
+}
+
+static inline int page_ref_count(struct page *page)
+{
+	return atomic_read(&page->_count);
+}
+
+#define page_ref_tracepoint_active(t) false
+
+static inline void __page_ref_mod(struct page *page, int v)
+{
+}
+
+static inline void page_ref_inc(struct page *page)
+{
+	atomic_inc(&page->_count);
+	if (page_ref_tracepoint_active(__tracepoint_page_ref_mod))
+		__page_ref_mod(page, 1);
+}
+
+static inline void page_ref_dec(struct page *page)
+{
+	atomic_dec(&page->_count);
+	if (page_ref_tracepoint_active(__tracepoint_page_ref_mod))
+		__page_ref_mod(page, -1);
+}
+#endif
+
+#ifdef CONFIG_MHI_DEBUG
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_VERBOSE)
+
+#define MHI_ASSERT(cond, msg) do { \
+	if (cond) \
+		panic(msg); \
+} while (0)
+
+#define MSG_VERB(fmt, ...) do { \
+	if (mhi_netdev->msg_lvl <= MHI_MSG_LVL_VERBOSE) \
+		pr_err("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+	if (mhi_netdev->ipc_log && (mhi_netdev->ipc_log_lvl <= \
+				    MHI_MSG_LVL_VERBOSE)) \
+		ipc_log_string(mhi_netdev->ipc_log, "[D][%s] " fmt, \
+			       __func__, ##__VA_ARGS__); \
+} while (0)
+
+#else
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_ERROR)
+
+#define MHI_ASSERT(cond, msg) do { \
+	if (cond) { \
+		MSG_ERR(msg); \
+		WARN_ON(cond); \
+	} \
+} while (0)
+
+#define MSG_VERB(fmt, ...)
+
+#endif
+
+extern bool debug;
+extern int debug_level;
+
+#define MSG_LOG(fmt, ...) do { \
+	if (debug && debug_level <= MHI_MSG_LVL_INFO) \
+		pr_err("[I][%s] " fmt, __func__, ##__VA_ARGS__);\
+	if (mhi_netdev->ipc_log && (mhi_netdev->ipc_log_lvl <= \
+				    MHI_MSG_LVL_INFO)) \
+		ipc_log_string(mhi_netdev->ipc_log, "[I][%s] " fmt, \
+			       __func__, ##__VA_ARGS__); \
+} while (0)
+
+#define MSG_ERR(fmt, ...) do { \
+	if (debug && debug_level <= MHI_MSG_LVL_ERROR) \
+		pr_err("[E][%s] " fmt, __func__, ##__VA_ARGS__); \
+	if (mhi_netdev->ipc_log && (mhi_netdev->ipc_log_lvl <= \
+				    MHI_MSG_LVL_ERROR)) \
+		ipc_log_string(mhi_netdev->ipc_log, "[E][%s] " fmt, \
+			       __func__, ##__VA_ARGS__); \
+} while (0)
+
+#define DEFAULT 1
+
+#if DEFAULT
+#define MSG_DEFAULT(fmt, ...) do { \
+		pr_info("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+#else
+#define MSG_DEFAULT(fmt, ...)
+#endif
+
+struct mhi_net_chain {
+	struct sk_buff *head, *tail; /* chained skb */
+};
+
+struct mhi_netdev {
+	int alias;
+	struct mhi_device *mhi_dev;
+	struct mhi_netdev *rsc_dev; /* rsc linked node */
+	bool is_rsc_dev;
+	int wake;
+
+	u32 mru;
+	u32 rx_order;
+	u32 mtu;
+	u32 tx_order;
+	const char *interface_name;
+	struct napi_struct *napi;
+	struct net_device *ndev;
+
+	/* for rx */
+	struct mhi_netbuf **rx_netbuf_pool;
+	int rx_pool_size; /* must be power of 2 */
+	int rx_current_index;
+	bool rx_chain_skb;
+	struct mhi_net_chain *rx_chain;
+
+	/* for tx */
+	struct mhi_netbuf **tx_netbuf_pool;
+	int tx_pool_size; /* must be power of 2 */
+	int tx_current_index;
+	bool tx_chain_skb;
+	struct mhi_net_chain *tx_chain;
+
+	struct mhi_netbuf *pending_netbuf;
+	u16 tx_offset;
+	u16 tx_num_pkt;
+
+	spinlock_t tx_lock;
+	struct hrtimer tx_timer;
+	struct tasklet_struct bh;
+
+	u32 timer_interval;
+	u16 tx_timer_pending;
+
+	atomic_t stop;
+
+	struct dentry *dentry;
+	enum MHI_DEBUG_LEVEL msg_lvl;
+	enum MHI_DEBUG_LEVEL ipc_log_lvl;
+	void *ipc_log;
+
+	unsigned long flags;
+
+#ifdef XDP_SUPPORT
+	struct bpf_prog *xdp_prog;
+#endif	
+};
+
+struct mhi_netdev_priv {
+	struct mhi_netdev *mhi_netdev;
+};
+
+/* Try not to make this structure bigger than 128 bytes, since this take space
+ * in payload packet.
+ * Example: If MRU = 16K, effective MRU = 16K - sizeof(mhi_netbuf)
+ */
+struct mhi_netbuf {
+	struct mhi_buf mhi_buf; /* this must be first element */
+	void (*unmap)(struct device *dev, dma_addr_t addr, size_t size,
+		      enum dma_data_direction dir);
+};
+
+static struct mhi_driver mhi_netdev_driver;
+static void mhi_netdev_create_debugfs(struct mhi_netdev *mhi_netdev);
+
+#if 0
+/**
+ * @brief map data type to protocol
+ *
+ * @param[in ]            data				data type
+ */
+static __be16 mhi_netdev_ip_type_trans(u8 data)
+{
+	__be16 protocol = 0;
+
+	/* determine L3 protocol */
+	switch (data & 0xf0) {
+	case 0x40:
+		protocol = htons(ETH_P_IP);
+		break;
+	case 0x60:
+		protocol = htons(ETH_P_IPV6);
+		break;
+	default:
+		/* default is QMAP */
+		protocol = htons(ETH_P_MAP);
+		break;
+	}
+	return protocol;
+}
+#endif
+
+/**
+ * @brief allocate and map pages for DMA 
+ *
+ * @param[in ]            dev				struct device
+ * @param[in ]            gfp				flag
+ * @param[in ]            order				page order
+ */
+static struct mhi_netbuf *mhi_netdev_alloc(struct device *dev,
+					   gfp_t gfp,
+					   unsigned int order,
+					   enum data_dir dir)
+{
+	struct page *page;
+	struct mhi_netbuf *netbuf;
+	struct mhi_buf *mhi_buf;
+	void *vaddr;
+
+	if (dir == RX_DATA)
+		page = __dev_alloc_pages(gfp, order);
+	else 
+		page = alloc_pages(gfp, order);
+
+	if (!page) {
+		return NULL;
+	}
+
+	vaddr = page_address(page);
+
+	/* we going to use the end of page to store cached data */
+	netbuf = vaddr + (PAGE_SIZE << order) - sizeof(*netbuf);
+
+	mhi_buf = (struct mhi_buf *)&netbuf->mhi_buf;
+	mhi_buf->page = page;
+	mhi_buf->buf = vaddr;
+	mhi_buf->len = (void *)netbuf - vaddr;
+
+	if (dir == RX_DATA) {
+
+		mhi_buf->dma_addr = dma_map_page(dev, page, 0, mhi_buf->len,
+										 DMA_FROM_DEVICE);
+		if (dma_mapping_error(dev, mhi_buf->dma_addr))
+		{
+			__free_pages(mhi_buf->page, order);
+			return NULL;
+		}
+	}
+
+	return netbuf;
+}
+
+/**
+ * @brief unmap pages for DMA
+ *
+ * @param[in ]            dev				struct device
+ * @param[in ]            dma_addr			dma address
+ * @param[in ]            len				dma buffer size
+ * @param[in ]            dir				dma direction
+ */
+static void mhi_netdev_unmap_page(struct device *dev,
+				  dma_addr_t dma_addr,
+				  size_t len,
+				  enum dma_data_direction dir)
+{
+	dma_unmap_page(dev, dma_addr, len, dir);
+}
+
+/**
+ * @brief queue data read requests
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ * @param[in ]            nr_tre			number of requests
+ */
+static int mhi_netdev_tmp_alloc(struct mhi_netdev *mhi_netdev, int nr_tre)
+{
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	struct device *dev = mhi_dev->dev.parent;
+	const u32 order = mhi_netdev->rx_order;
+	int i, ret;
+
+	MSG_LOG("Enter free_desc:%d\n", nr_tre);
+
+	for (i = 0; i < nr_tre; i++) {
+		struct mhi_buf *mhi_buf;
+		struct mhi_netbuf *netbuf = mhi_netdev_alloc(dev, GFP_ATOMIC,
+							     order, RX_DATA);
+		if (!netbuf)
+			return -ENOMEM;
+
+		mhi_buf = (struct mhi_buf *)netbuf;
+		netbuf->unmap = mhi_netdev_unmap_page;
+
+		ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE, mhi_buf,
+					 mhi_buf->len, MHI_EOT);
+		if (unlikely(ret)) {
+			MSG_ERR("Failed to queue transfer, ret:%d\n", ret);
+			mhi_netdev_unmap_page(dev, mhi_buf->dma_addr,
+					      mhi_buf->len, DMA_FROM_DEVICE);
+			__free_pages(mhi_buf->page, order);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * @brief queue data read requests
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static void mhi_netdev_queue(struct mhi_netdev *mhi_netdev)
+{
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	struct device *dev = mhi_dev->dev.parent;
+	struct mhi_netbuf *netbuf;
+	struct mhi_buf *mhi_buf = NULL;
+	struct mhi_netbuf **netbuf_pool = mhi_netdev->rx_netbuf_pool;
+	int nr_tre = mhi_get_no_free_descriptors(mhi_dev, DMA_FROM_DEVICE);
+	int i, peak, cur_index = 0, ret;
+	const int pool_size = mhi_netdev->rx_pool_size - 1, max_peak = 4;
+
+	MSG_VERB("Enter free_desc:%d\n", nr_tre);
+	MSG_LOG("Enter free_desc:%d\n", nr_tre);
+
+	if (!nr_tre)
+		return;
+
+	/* try going thru reclaim pool first */
+	for (i = 0; i < nr_tre; i++) {
+		/* peak for the next buffer, we going to peak several times,
+		 * and we going to give up if buffers are not yet free
+		 */
+	    MSG_LOG("i %d cur_index %d\n", i, cur_index);
+		cur_index = mhi_netdev->rx_current_index;
+		netbuf = NULL;
+		for (peak = 0; peak < max_peak; peak++) {
+			struct mhi_netbuf *tmp = netbuf_pool[cur_index];
+
+			mhi_buf = &tmp->mhi_buf;
+
+			cur_index = (cur_index + 1) & pool_size;
+	        
+            MSG_LOG("peak %d cur_index %d\n", peak, cur_index);
+
+			/* page == 1 idle, buffer is free to reclaim */
+			if (page_ref_count(mhi_buf->page) == 1) {
+				netbuf = tmp;
+				break;
+			}
+		}
+
+		/* could not find a free buffer */
+		if (!netbuf)
+			break;
+
+	    MSG_LOG("mhi_buf 0x%p\n", mhi_buf);
+
+		/* increment reference count so when network stack is done
+		 * with buffer, the buffer won't be freed
+		 */
+		page_ref_inc(mhi_buf->page);
+		dma_sync_single_for_device(dev, mhi_buf->dma_addr, mhi_buf->len,
+					   DMA_FROM_DEVICE);
+		ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE, mhi_buf,
+					 mhi_buf->len, MHI_EOT);
+		if (unlikely(ret)) {
+			MSG_ERR("Failed to queue buffer, ret:%d\n", ret);
+			netbuf->unmap(dev, mhi_buf->dma_addr, mhi_buf->len,
+				      DMA_FROM_DEVICE);
+			page_ref_dec(mhi_buf->page);
+			return;
+		}
+		mhi_netdev->rx_current_index = cur_index;
+	}
+
+	/* recyling did not work, buffers are still busy allocate temp pkts */
+	if (i < nr_tre)
+		mhi_netdev_tmp_alloc(mhi_netdev, nr_tre - i);
+}
+
+/**
+ * @brief allocating rx pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static int mhi_netdev_alloc_rx_pool(struct mhi_netdev *mhi_netdev)
+{
+	int i;
+	struct mhi_netbuf *netbuf, **netbuf_pool;
+	struct mhi_buf *mhi_buf;
+	const u32 rx_order = mhi_netdev->rx_order;
+	struct device *dev = mhi_netdev->mhi_dev->dev.parent;
+
+	netbuf_pool = kmalloc_array(mhi_netdev->rx_pool_size, sizeof(*netbuf_pool),
+				    GFP_KERNEL);
+	if (!netbuf_pool)
+		return -ENOMEM;
+
+	for (i = 0; i < mhi_netdev->rx_pool_size; i++) {
+		/* allocate paged data */
+		netbuf = mhi_netdev_alloc(dev, GFP_DMA32 | GFP_KERNEL, rx_order, RX_DATA);
+		if (!netbuf)
+			goto error_alloc_page;
+
+		netbuf->unmap = dma_sync_single_for_cpu;
+		netbuf_pool[i] = netbuf;
+	}
+
+	mhi_netdev->rx_netbuf_pool = netbuf_pool;
+
+	return 0;
+
+error_alloc_page:
+	for (--i; i >= 0; i--) {
+		netbuf = netbuf_pool[i];
+		mhi_buf = &netbuf->mhi_buf;
+		dma_unmap_page(dev, mhi_buf->dma_addr, mhi_buf->len,
+			       DMA_FROM_DEVICE);
+		__free_pages(mhi_buf->page, rx_order);
+	}
+
+	kfree(netbuf_pool);
+
+	return -ENOMEM;
+}
+
+/**
+ * @brief allocating tx pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static int mhi_netdev_alloc_tx_pool(struct mhi_netdev *mhi_netdev)
+{
+	int i;
+	struct mhi_netbuf *netbuf, **netbuf_pool;
+	struct mhi_buf *mhi_buf;
+	const u32 tx_order = mhi_netdev->tx_order;
+	struct device *dev = mhi_netdev->mhi_dev->dev.parent;
+
+	netbuf_pool = kmalloc_array(mhi_netdev->tx_pool_size, sizeof(*netbuf_pool),
+				    GFP_KERNEL);
+	if (!netbuf_pool) {
+		MSG_ERR("failed to alloc buffer for tx pool!\n");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < mhi_netdev->tx_pool_size; i++) {
+		/* allocate paged data */
+		netbuf = mhi_netdev_alloc(dev, GFP_DMA32 | GFP_KERNEL, tx_order, TX_DATA);
+		if (!netbuf) {
+			MSG_ERR("failed to alloc netbuf!\n");
+			goto error_alloc_page;
+		}
+
+		netbuf->unmap = NULL;
+		netbuf_pool[i] = netbuf;
+	}
+
+	mhi_netdev->tx_netbuf_pool = netbuf_pool;
+
+	MSG_LOG("success!\n");
+
+	return 0;
+
+error_alloc_page:
+	for (--i; i >= 0; i--) {
+		netbuf = netbuf_pool[i];
+		mhi_buf = &netbuf->mhi_buf;
+		__free_pages(mhi_buf->page, tx_order);
+	}
+
+	kfree(netbuf_pool);
+
+	return -ENOMEM;
+}
+
+/**
+ * @brief free RX pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static void mhi_netdev_free_rx_pool(struct mhi_netdev *mhi_netdev)
+{
+	int i;
+	struct mhi_netbuf *netbuf, **netbuf_pool = mhi_netdev->rx_netbuf_pool;
+	struct device *dev = mhi_netdev->mhi_dev->dev.parent;
+	struct mhi_buf *mhi_buf;
+
+	for (i = 0; i < mhi_netdev->rx_pool_size; i++) {
+		netbuf = netbuf_pool[i];
+		mhi_buf = &netbuf->mhi_buf;
+		dma_unmap_page(dev, mhi_buf->dma_addr, mhi_buf->len,
+			       DMA_FROM_DEVICE);
+		__free_pages(mhi_buf->page, mhi_netdev->rx_order);
+	}
+
+	kfree(mhi_netdev->rx_netbuf_pool);
+	mhi_netdev->rx_netbuf_pool = NULL;
+}
+
+/**
+ * @brief free TX pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static void mhi_netdev_free_tx_pool(struct mhi_netdev *mhi_netdev)
+{
+	int i;
+	struct mhi_netbuf *netbuf, **netbuf_pool = mhi_netdev->tx_netbuf_pool;
+	struct mhi_buf *mhi_buf;
+
+	for (i = 0; i < mhi_netdev->tx_pool_size; i++) {
+		netbuf = netbuf_pool[i];
+		mhi_buf = &netbuf->mhi_buf;
+		__free_pages(mhi_buf->page, mhi_netdev->tx_order);
+	}
+
+	kfree(mhi_netdev->tx_netbuf_pool);
+	mhi_netdev->tx_netbuf_pool = NULL;
+}
+
+/**
+ * @brief allocating pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static int mhi_netdev_alloc_pool(struct mhi_netdev *mhi_netdev)
+{
+	int ret;
+	
+	ret = mhi_netdev_alloc_rx_pool(mhi_netdev);
+	if (ret) 
+		return ret;
+
+	ret = mhi_netdev_alloc_tx_pool(mhi_netdev);
+	if (ret) 
+		mhi_netdev_free_rx_pool(mhi_netdev);	
+
+	return ret;
+}
+
+/**
+ * @brief free pool of memory
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static void mhi_netdev_free_pool(struct mhi_netdev *mhi_netdev)
+{
+	mhi_netdev_free_rx_pool(mhi_netdev);
+	mhi_netdev_free_tx_pool(mhi_netdev);
+}
+
+/**
+ * @brief poll the net device
+ *
+ * @param[in ]            napi		struct napi_struct
+ * @param[in ]            budget	poll budget
+ */
+static int mhi_netdev_poll(struct napi_struct *napi, int budget)
+{
+	struct net_device *dev = napi->dev;
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	struct mhi_netdev *rsc_dev = mhi_netdev->rsc_dev;
+	struct mhi_net_chain *chain = mhi_netdev->rx_chain;
+	int rx_work = 0;
+
+	//MSG_VERB("Entered\n");
+    MSG_LOG("Entered\n");
+
+	rx_work = mhi_poll(mhi_dev, budget);
+
+	/* chained skb, push it to stack */
+	if (chain && chain->head) {
+		netif_receive_skb(chain->head);
+		chain->head = NULL;
+	}
+
+	if (rx_work < 0) {
+		MSG_ERR("Error polling ret:%d\n", rx_work);
+		napi_complete(napi);
+		return 0;
+	}
+
+	/* queue new buffers */
+    MSG_LOG("mhi_netdev_queue\n");
+	mhi_netdev_queue(mhi_netdev);
+
+	if (rsc_dev)
+		mhi_netdev_queue(rsc_dev);
+
+	/* complete work if # of packet processed less than allocated budget */
+	if (rx_work < budget)
+		napi_complete(napi);
+
+	MSG_VERB("polled %d pkts\n", rx_work);
+    MSG_LOG("polled %d pkts\n", rx_work);
+
+	return rx_work;
+}
+
+/**
+ * @brief open the net device
+ *
+ * @param[in ]            dev		struct net_device
+ */
+static int mhi_netdev_open(struct net_device *dev)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+
+	MSG_LOG("Opened net dev interface\n");
+
+	memcpy(dev->dev_addr, mac_addr, ETH_ALEN);
+
+	/* tx queue may not necessarily be stopped already
+	 * so stop the queue if tx path is not enabled
+	 */
+	if (!mhi_dev->ul_chan)
+		netif_stop_queue(dev);
+	else
+		netif_start_queue(dev);
+
+	return 0;
+
+}
+
+/**
+ * @brief device stops
+ *
+ * @param[in ]		  dev		network device
+ */
+static int mhi_netdev_release(struct net_device *dev)
+{
+	netif_stop_queue(dev); 
+
+	return 0;
+}
+
+/**
+ * @brief change MTU for the net device
+ *
+ * @param[in ]            dev		struct net_device
+ * @param[in ]            new_mtu	new MTU value
+ */
+static int mhi_netdev_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+
+	if (new_mtu < 0 || mhi_dev->mtu < new_mtu)
+		return -EINVAL;
+
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+/**
+ * @brief verify that the ethernet protocol is IPv4 or IPv6
+ *
+ * @param[in ]        proto		protocol
+*/
+static bool is_ip_proto(__be16 proto)
+{
+	switch (proto) {
+	case htons(ETH_P_IP):
+	case htons(ETH_P_IPV6):
+		return true;
+	}
+	return false;
+}
+
+/**
+ * @brief get QMAP tci
+ *
+ * @param[in ]        dev_info		device information
+ * @param[in ]        skb			data packet
+ * @param[in,out ]    tci		    
+ */
+int mhi_netdev_get_qmap_tci(struct net_device *dev, struct sk_buff *skb, u8 *tci_ret)
+{
+	u16 tci = 0;
+	bool is_ip;
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+
+	/* Some applications using e.g. packet sockets will
+	 * bypass the VLAN acceleration and create tagged
+	 * ethernet frames directly.  We primarily look for
+	 * the accelerated out-of-band tag, but fall back if
+	 * required
+	 */
+	skb_reset_mac_header(skb);
+	if (vlan_get_tag(skb, &tci) < 0 && skb->len > VLAN_ETH_HLEN &&
+		__vlan_get_tag(skb, &tci) == 0) {
+		is_ip = is_ip_proto(vlan_eth_hdr(skb)->h_vlan_encapsulated_proto);
+		skb_pull(skb, VLAN_ETH_HLEN);
+	}
+	else {
+		is_ip = is_ip_proto(eth_hdr(skb)->h_proto);
+		skb_pull(skb, ETH_HLEN);
+	}
+
+	/* Is IP session <0> tagged too? */
+	if (mhi_netdev->flags & FLAG_IPS0_VLAN) {
+		/* drop all untagged packets */
+		if (!tci)
+			goto error;
+		/* map MBIM_IPS0_VID to IPS<0> */
+		if (tci == MBIM_IPS0_VID)
+			tci = 0;
+	}
+
+	/* mapping VLANs to QMAP mux_id id:
+	 *   no tag     => mux_id <0> if !FLAG_IPS0_VLAN
+	 *   1 - 127    => mux_id <vlanid>
+	 *   4094       => mux_id <0> if FLAG_IPS0_VLAN
+	 */
+
+	switch (tci & 0x0f00) {
+	case 0x0000: /* VLAN ID 1 - 127 */
+		if (!is_ip)
+			goto error;
+
+		MSG_LOG("tci %d\n", tci);
+
+		*tci_ret = tci; /* map VLAN ID to QMAP mux_id */
+		break;
+
+	default:
+		MSG_ERR("unsupported tci 0x%04x\n", tci);
+		goto error;
+	}
+
+	return 0;
+
+error:
+	return -EINVAL;
+}
+
+/**
+ * @brief tx packet timer start
+ *
+ * @param[in ]        timer		timer
+*/
+void mhi_netdev_tx_timeout_start(struct mhi_netdev *mhi_netdev)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,6,0)
+	ktime_t temp_time;
+	temp_time.tv64 = mhi_netdev->timer_interval;
+#endif
+	/* start timer, if not already started */
+	//MSG_LOG("active %d stop %d\n", hrtimer_active(&mhi_netdev->tx_timer), atomic_read(&mhi_netdev->stop));
+
+	if (!(hrtimer_active(&mhi_netdev->tx_timer) || atomic_read(&mhi_netdev->stop))) {
+
+		hrtimer_start(&mhi_netdev->tx_timer,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,6,0)
+			temp_time,
+#else
+			mhi_netdev->timer_interval,
+#endif
+			HRTIMER_MODE_REL);
+	
+		//MSG_LOG("tx timer started!\n");
+	}
+}
+
+/**
+ * @brief data tx BH
+ *
+ * @param[in ]        param		parameter
+*/
+void mhi_netdev_txpath_bh(unsigned long param)
+{
+	struct mhi_netdev *mhi_netdev = (struct mhi_netdev *)param;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	struct mhi_netbuf *netbuf = NULL;
+	struct mhi_buf *mhi_buf = NULL;
+	int res;
+
+	spin_lock_bh(&mhi_netdev->tx_lock);
+
+	MSG_LOG("tx_timer_pending 0x%x\n", mhi_netdev->tx_timer_pending);
+
+	if (mhi_netdev->tx_timer_pending != 0) {
+		mhi_netdev->tx_timer_pending--;
+		mhi_netdev_tx_timeout_start(mhi_netdev);
+	} else {
+		
+		netbuf = mhi_netdev->pending_netbuf;
+		mhi_netdev->pending_netbuf = NULL;
+
+		if (netbuf) {
+
+			mhi_buf = &netbuf->mhi_buf;	
+
+            MSG_LOG("send packets in mhi_netdev_txpath_bh 0x%p\n", netbuf);
+			
+			/* increment reference count so when network stack is done
+		 	* with buffer, the buffer won't be freed
+		 	*/
+			page_ref_inc(mhi_buf->page);
+
+			res = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, mhi_buf, mhi_buf->pkt_len,
+				 MHI_EOT);
+
+			if (res) {
+				MSG_VERB("Failed to queue with reason:%d\n", res);
+				page_ref_dec(mhi_buf->page);
+			}
+        }    
+	}
+
+	spin_unlock_bh(&mhi_netdev->tx_lock);
+}
+
+/**
+ * @brief tx packet timer callback
+ *
+ * @param[in ]        timer		timerdev_info
+*/
+enum hrtimer_restart mhi_netdev_tx_timer_cb(struct hrtimer *timer)
+{
+	struct mhi_netdev *mhi_netdev =
+		container_of(timer, struct mhi_netdev, tx_timer);
+
+	//MSG_LOG(KERN_INFO "stop %d", atomic_read(&mhi_netdev->stop));
+
+	if (!atomic_read(&mhi_netdev->stop))
+		tasklet_schedule(&mhi_netdev->bh);
+	
+	return HRTIMER_NORESTART;
+}
+
+/**
+ * @brief send data packet
+ *
+ * @param[in ]            skb		data packet
+ * @param[in ]            dev		struct net_device
+ */
+static int mhi_netdev_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	int res = NETDEV_TX_OK;
+    u32 len = 0;
+	u8 tci = 0;
+
+	struct mhi_netbuf *netbuf = NULL;
+	struct mhi_buf *mhi_buf = NULL;
+	struct mhi_netbuf **netbuf_pool = mhi_netdev->tx_netbuf_pool;
+
+	int i, space;
+	const int pool_size = mhi_netdev->tx_pool_size;
+	u16 offset = 0, index;
+    u8 header[4], *buf;
+
+	MSG_LOG("skb 0x%p len %d\n", skb, skb->len);
+
+#if 0
+    MSG_LOG("mhi_netdev_xmit buffer\n");
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        skb->data, skb->len, true);
+#endif
+
+	/* get tci from skb */
+	if (mhi_netdev_get_qmap_tci(dev, skb, &tci) < 0) {
+		MSG_ERR("mhi_netdev_get_qmap_tci failed!\n");
+
+		/* drop the packet */
+		dev->stats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+
+		return res;
+	}
+
+	if (mhi_queue_full(mhi_dev)) {
+		MSG_ERR("upload channel is full!\n");
+
+#if !defined(DROP_PKT_IF_RING_FULL) || (defined(DROP_PKT_IF_RING_FULL) && DROP_PKT_IF_RING_FULL == 0)
+		netif_stop_queue(dev);
+		res = NETDEV_TX_BUSY;
+#else
+		/* drop the packet */
+		dev->stats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+#endif
+		return res;
+	}
+
+	spin_lock_bh(&mhi_netdev->tx_lock);
+
+	/* first packet (no pending packets)? */
+	if (mhi_netdev->pending_netbuf == NULL) {
+
+		index = mhi_netdev->tx_current_index;
+
+		/* get the a free buffer from the pool */
+		for (i = 0; i < pool_size; i++) {
+			struct mhi_netbuf *tmp = netbuf_pool[index];
+
+			index = (index+1) % pool_size;
+
+			mhi_buf = &tmp->mhi_buf;
+
+	    	MSG_LOG("i %d mhi_buf 0x%px page ref count %d\n", 
+				i, mhi_buf, page_ref_count(mhi_buf->page));
+
+			/* page == 1 idle, buffer is free to reclaim */
+			if (page_ref_count(mhi_buf->page) == 1) {
+				netbuf = tmp;
+				mhi_netdev->tx_current_index = index;
+				break;
+			}
+		}
+
+		if (netbuf == NULL)	{
+			MSG_ERR("no free tx buffer!\n");
+			goto drop;
+		}
+
+		offset = 0;
+		mhi_netdev->tx_num_pkt = 0;
+
+	} else {
+		netbuf = mhi_netdev->pending_netbuf;
+		offset = mhi_netdev->tx_offset;
+	}
+
+	/* copy packet to mhi_buf */
+	len = skb->len;
+
+    /* compose the QMAP header */
+    header[0] = (len % 4) ? (4 - (len % 4)) : 0;
+    header[1] = tci;
+    header[2] = ((len + header[0]) & 0xff00) >> 8;
+    header[3] = ((len + header[0]) & 0x00ff);
+
+	len += header[0];
+
+	mhi_buf = &netbuf->mhi_buf;
+
+	buf = (u8 *)mhi_buf->buf;
+
+	/* copy the QMAP header */
+	memcpy((u8 *)&buf[offset], &header, sizeof(header));
+	offset += sizeof(header);
+
+	/* copy the packet */
+	memcpy((u8 *)&buf[offset], skb->data, skb->len);
+
+	offset += len;
+
+	/* save the offset */
+	mhi_netdev->tx_offset = offset;
+
+	mhi_buf->pkt_len = offset;
+
+#if 0
+    MSG_LOG("netdev_xmit buffer after adding QMAP header\n");
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        mhi_buf->buf[offset], offset, true);
+#endif
+
+	/* after copying, free the skb */
+	dev_kfree_skb_any(skb);
+
+	mhi_netdev->tx_num_pkt++;
+	mhi_buf->pkt_num = mhi_netdev->tx_num_pkt;
+
+	/* make sure the buffer is enough for another packet */
+	space = mhi_buf->len - offset - dev->mtu - sizeof(header);
+
+	MSG_LOG("pkt %d space %d\n", mhi_netdev->tx_num_pkt, space);
+
+	if ((mhi_netdev->tx_num_pkt >= MAX_UL_PKT_AGGR) || (space < 0))
+	{
+		MSG_LOG("accumulate %d pkt ready to send!\n", mhi_netdev->tx_num_pkt);
+
+		/* increment reference count so when network stack is done
+		 * with buffer, the buffer won't be freed
+		 */
+		page_ref_inc(mhi_buf->page);
+
+		res = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, mhi_buf, mhi_netdev->tx_offset,
+				 MHI_EOT);
+
+		if (res) {
+			MSG_VERB("Failed to queue with reason:%d\n", res);
+
+			page_ref_dec(mhi_buf->page);
+
+			/* drop the packet */
+			dev->stats.tx_dropped += mhi_netdev->tx_num_pkt;
+		}
+
+		mhi_netdev->pending_netbuf = NULL;
+
+		res = NETDEV_TX_OK;
+
+		goto exit;
+	}
+
+	mhi_netdev->pending_netbuf = netbuf;
+
+	if (mhi_netdev->tx_num_pkt < CDC_NCM_RESTART_TIMER_DATAGRAM_CNT)
+		mhi_netdev->tx_timer_pending = CDC_NCM_TIMER_PENDING_CNT;
+
+	/* start timer, if there is a buffer waiting */
+	if (mhi_netdev->pending_netbuf != NULL && mhi_netdev->tx_num_pkt > 0) {
+
+		MSG_LOG("start tx timer!\n");
+
+		mhi_netdev_tx_timeout_start(mhi_netdev);
+	}
+
+	goto exit;
+
+drop:
+	dev->stats.tx_dropped++;
+	dev_kfree_skb_any(skb);
+
+exit:
+	spin_unlock_bh(&mhi_netdev->tx_lock);
+
+	return res;	
+}	
+
+/**
+ * @brief ioctl extension handler
+ *
+ * @param[in ]            dev		struct net_device
+ * @param[in ]            ifreq		request
+ */
+static int mhi_netdev_ioctl_extended(struct net_device *dev, struct ifreq *ifr)
+{
+	struct rmnet_ioctl_extended_s ext_cmd;
+	int rc = 0;
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+
+	rc = copy_from_user(&ext_cmd, ifr->ifr_ifru.ifru_data,
+			    sizeof(struct rmnet_ioctl_extended_s));
+	if (rc)
+		return rc;
+
+	switch (ext_cmd.extended_ioctl) {
+	case RMNET_IOCTL_GET_SUPPORTED_FEATURES:
+		ext_cmd.u.data = 0;
+		break;
+	case RMNET_IOCTL_GET_DRIVER_NAME:
+		strlcpy(ext_cmd.u.if_name, mhi_netdev->interface_name,
+			sizeof(ext_cmd.u.if_name));
+		break;
+	case RMNET_IOCTL_SET_SLEEP_STATE:
+		if (ext_cmd.u.data && mhi_netdev->wake) {
+			/* Request to enable LPM */
+			MSG_VERB("Enable MHI LPM");
+			mhi_netdev->wake--;
+			mhi_device_put(mhi_dev, MHI_VOTE_DEVICE);
+		} else if (!ext_cmd.u.data && !mhi_netdev->wake) {
+			/* Request to disable LPM */
+			MSG_VERB("Disable MHI LPM");
+			mhi_netdev->wake++;
+			mhi_device_get(mhi_dev, MHI_VOTE_DEVICE);
+		}
+		break;
+	default:
+		rc = -EINVAL;
+		break;
+	}
+
+	rc = copy_to_user(ifr->ifr_ifru.ifru_data, &ext_cmd,
+			  sizeof(struct rmnet_ioctl_extended_s));
+	return rc;
+}
+
+/**
+ * @brief ioctl handler
+ *
+ * @param[in ]            dev		struct net_device
+ * @param[in ]            ifreq		request
+ * @param[in ]            cmd		command
+ */
+static int mhi_netdev_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	int rc = 0;
+	struct rmnet_ioctl_data_s ioctl_data;
+
+	switch (cmd) {
+	case RMNET_IOCTL_SET_LLP_IP: /* set RAWIP protocol */
+		break;
+	case RMNET_IOCTL_GET_LLP: /* get link protocol state */
+		ioctl_data.u.operation_mode = RMNET_MODE_LLP_IP;
+		if (copy_to_user(ifr->ifr_ifru.ifru_data, &ioctl_data,
+		    sizeof(struct rmnet_ioctl_data_s)))
+			rc = -EFAULT;
+		break;
+	case RMNET_IOCTL_GET_OPMODE: /* get operation mode */
+		ioctl_data.u.operation_mode = RMNET_MODE_LLP_IP;
+		if (copy_to_user(ifr->ifr_ifru.ifru_data, &ioctl_data,
+		    sizeof(struct rmnet_ioctl_data_s)))
+			rc = -EFAULT;
+		break;
+	case RMNET_IOCTL_SET_QOS_ENABLE:
+		rc = -EINVAL;
+		break;
+	case RMNET_IOCTL_SET_QOS_DISABLE:
+		rc = 0;
+		break;
+	case RMNET_IOCTL_OPEN:
+	case RMNET_IOCTL_CLOSE:
+		/* we just ignore them and return success */
+		rc = 0;
+		break;
+	case RMNET_IOCTL_EXTENDED:
+		rc = mhi_netdev_ioctl_extended(dev, ifr);
+		break;
+	default:
+		/* don't fail any IOCTL right now */
+		rc = 0;
+		break;
+	}
+
+	return rc;
+}
+
+/**
+ * @brief add VLAN id to network device
+ *
+ * @param[in ]        dev		network device
+ * @param[in ]        proto		protocol
+ * @param[in ]        vid		VLAN id
+ */
+static int mhi_netdev_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+
+	/* creation of this VLAN is a request to tag IP session 0 */
+	if (vid == MBIM_IPS0_VID)
+		mhi_netdev->flags |= FLAG_IPS0_VLAN;
+	else
+		if (vid > 127) {	/* we don't map these to QMAP mux_id */
+			MSG_ERR("VLAN id %d is out of range!\n", vid);
+			return -EINVAL;
+		}
+	return 0;
+}
+
+/**
+ * @brief remove VLAN id from network device
+ *
+ * @param[in ]        dev		network device
+ * @param[in ]        proto		protocol
+ * @param[in ]        vid		VLAN id
+ */
+static int mhi_netdev_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+
+	/* this is a request for an untagged IP session 0 */
+	if (vid == MBIM_IPS0_VID)
+		mhi_netdev->flags &= ~FLAG_IPS0_VLAN;
+
+	return 0;
+}
+
+#ifdef XDP_SUPPORT
+static int mhi_netdev_xdp_setup(struct net_device *dev, struct bpf_prog *prog)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+
+	struct bpf_prog *old_prog;
+
+	old_prog = xchg(&mhi_netdev->xdp_prog, prog);
+
+	if (old_prog)
+		bpf_prog_put(old_prog);
+
+	return 0;
+}
+
+static int mhi_netdev_xdp(struct net_device *dev, struct netdev_bpf *xdp)
+{
+	struct mhi_netdev_priv *mhi_netdev_priv = netdev_priv(dev);
+	struct mhi_netdev *mhi_netdev = mhi_netdev_priv->mhi_netdev;
+
+	MSG_LOG("Enter: xdp command %d\n", xdp->command);
+
+	switch (xdp->command) {
+	case XDP_SETUP_PROG:
+		MSG_LOG("XDP_SETUP_PROG\n");
+		return mhi_netdev_xdp_setup(dev, xdp->prog);
+	case XDP_QUERY_PROG:
+		MSG_LOG("XDP_QUERY_PROG\n");
+		xdp->prog_id = mhi_netdev->xdp_prog ?
+			mhi_netdev->xdp_prog->aux->id : 0;
+		return 0;
+
+	default:
+		return -EINVAL;
+	}
+}
+#endif
+
+static const struct net_device_ops mhi_netdev_ops_ip = {
+	.ndo_open = mhi_netdev_open,
+    .ndo_stop = mhi_netdev_release,
+	.ndo_start_xmit = mhi_netdev_xmit,
+	.ndo_do_ioctl = mhi_netdev_ioctl,
+	.ndo_change_mtu = mhi_netdev_change_mtu,
+	.ndo_set_mac_address = 0,
+	.ndo_validate_addr = 0,
+	.ndo_vlan_rx_add_vid = mhi_netdev_rx_add_vid,
+	.ndo_vlan_rx_kill_vid = mhi_netdev_rx_kill_vid,
+#ifdef XDP_SUPPORT
+	.ndo_bpf = mhi_netdev_xdp,
+#endif	
+};
+
+/**
+ * @brief initialize data structure for net device
+ *
+ * @param[in ]            dev		struct net_device
+ */
+static void mhi_netdev_setup(struct net_device *dev)
+{
+	ether_setup(dev);
+
+	dev->netdev_ops = &mhi_netdev_ops_ip;
+
+	/* keep the default flags, just add NOARP */
+	dev->flags |= IFF_NOARP;
+
+	/* no need to put the VLAN tci in the packet headers */
+	dev->features |= NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_FILTER;
+
+	memcpy(dev->dev_addr, mac_addr, ETH_ALEN);
+
+	//dev->watchdog_timeo = WATCHDOG_TIMEOUT;
+}
+
+static struct device_type wwan_type = {
+	.name = "wwan",
+};
+
+/**
+ * @brief enable mhi_netdev netdev, call only after grabbing mhi_netdev.mutex
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ */
+static int mhi_netdev_enable_iface(struct mhi_netdev *mhi_netdev)
+{
+	int ret = 0;
+	char ifalias[IFALIASZ];
+	char ifname[IFNAMSIZ];
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	//struct device_node *of_node = mhi_dev->dev.of_node;
+	struct mhi_netdev_priv *mhi_netdev_priv;
+
+	MSG_LOG("Enter\n");
+
+	mhi_netdev->interface_name = mhi_netdev_driver.driver.name;
+
+	snprintf(ifalias, sizeof(ifalias), "%s_%04x_%02u.%02u.%02u_%u",
+		 mhi_netdev->interface_name, mhi_dev->dev_id, mhi_dev->domain,
+		 mhi_dev->bus, mhi_dev->slot, mhi_netdev->alias);
+
+	snprintf(ifname, sizeof(ifname), "%s%%d", mhi_netdev->interface_name);
+
+	rtnl_lock();
+	mhi_netdev->ndev = alloc_netdev(sizeof(*mhi_netdev_priv),
+					ifname, NET_NAME_PREDICTABLE,
+					mhi_netdev_setup);
+	if (!mhi_netdev->ndev) {
+		rtnl_unlock();
+		return -ENOMEM;
+	}
+
+	mhi_netdev->ndev->mtu = 1500; // mhi_dev->mtu;
+	SET_NETDEV_DEV(mhi_netdev->ndev, &mhi_dev->dev);
+
+	SET_NETDEV_DEVTYPE(mhi_netdev->ndev, &wwan_type);
+#if 0
+	dev_set_alias(mhi_netdev->ndev, ifalias, strlen(ifalias));
+#endif    
+	mhi_netdev_priv = netdev_priv(mhi_netdev->ndev);
+	mhi_netdev_priv->mhi_netdev = mhi_netdev;
+	rtnl_unlock();
+
+	mhi_netdev->napi = devm_kzalloc(&mhi_dev->dev,
+					sizeof(*mhi_netdev->napi), GFP_KERNEL);
+	if (!mhi_netdev->napi) {
+		ret = -ENOMEM;
+		goto napi_alloc_fail;
+	}
+
+	netif_napi_add(mhi_netdev->ndev, mhi_netdev->napi,
+		       mhi_netdev_poll, NAPI_POLL_WEIGHT);
+	ret = register_netdev(mhi_netdev->ndev);
+	if (ret) {
+		MSG_ERR("Network device registration failed\n");
+		goto net_dev_reg_fail;
+	}
+
+	napi_enable(mhi_netdev->napi);
+
+	MSG_LOG("Exited.\n");
+
+	return 0;
+
+net_dev_reg_fail:
+	netif_napi_del(mhi_netdev->napi);
+
+napi_alloc_fail:
+	free_netdev(mhi_netdev->ndev);
+	mhi_netdev->ndev = NULL;
+
+	return ret;
+}
+
+/**
+ * @brief host to device transfer callback
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ * @param[in ]            mhi_result		struct mhi_result
+ */
+static void mhi_netdev_xfer_ul_cb(struct mhi_device *mhi_dev,
+				  struct mhi_result *mhi_result)
+{
+	struct mhi_netdev *mhi_netdev = mhi_device_get_devdata(mhi_dev);
+	struct mhi_buf *mhi_buf = mhi_result->buf_addr;
+	struct net_device *ndev = mhi_netdev->ndev;
+
+    MSG_LOG("mhi_buf 0x%p num %d len %ld\n", mhi_buf, mhi_buf->pkt_num, 
+		(long int)mhi_buf->pkt_len);
+
+	ndev->stats.tx_packets += mhi_buf->pkt_num;
+	ndev->stats.tx_bytes += mhi_buf->pkt_len;
+
+	/* drop the page ref count */
+	__free_pages(mhi_buf->page, mhi_netdev->tx_order);
+
+	if (netif_queue_stopped(ndev))
+		netif_wake_queue(ndev);
+}
+
+/**
+ * @brief handle neighbor solicitation
+ *
+ * Some devices are known to send Neigbor Solicitation messages and
+ * require Neigbor Advertisement replies.  The IPv6 core will not
+ * respond since IFF_NOARP is set, so we must handle them ourselves.
+ *
+ * @param[in ]        pdevice_info		device information
+ * @param[in ]        buf				buffer
+ * @param[in ]        tci
+*/
+static void do_neigh_solicit(struct net_device *dev, u8 *buf, u16 tci)
+{
+	struct ipv6hdr *iph = (void *)buf;
+	struct nd_msg *msg = (void *)(iph + 1);
+	struct net_device *netdev;
+	struct inet6_dev *in6_dev;
+	bool is_router;
+
+	/* we'll only respond to requests from unicast addresses to
+	 * our solicited node addresses.
+	 */
+	if (!ipv6_addr_is_solict_mult(&iph->daddr) ||
+		!(ipv6_addr_type(&iph->saddr) & IPV6_ADDR_UNICAST))
+		return;
+
+	/* need to send the NA on the VLAN dev, if any */
+	rcu_read_lock();
+	if (tci) {
+		netdev = __vlan_find_dev_deep_rcu(dev, htons(ETH_P_8021Q),
+			tci);
+		if (!netdev) {
+			rcu_read_unlock();
+			return;
+		}
+	}
+	else {
+		netdev = dev;
+	}
+	dev_hold(netdev);
+	rcu_read_unlock();
+
+	in6_dev = in6_dev_get(netdev);
+	if (!in6_dev)
+		goto out;
+	is_router = !!in6_dev->cnf.forwarding;
+	in6_dev_put(in6_dev);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,4,0)
+	/* ipv6_stub != NULL if in6_dev_get returned an inet6_dev */
+	ipv6_stub->ndisc_send_na(netdev, NULL, &iph->saddr, &msg->target,
+		is_router /* router */,
+		true /* solicited */,
+		false /* override */,
+		true /* inc_opt */);
+#else
+	/* ipv6_stub != NULL if in6_dev_get returned an inet6_dev */
+	ipv6_stub->ndisc_send_na(netdev, &iph->saddr, &msg->target,
+		is_router /* router */,
+		true /* solicited */,
+		false /* override */,
+		true /* inc_opt */);
+#endif
+
+out:
+	dev_put(netdev);
+}
+
+/**
+ * @brief check if packet is a neighbor solicitation request
+ *
+ * @param[in ]        buf		buffer
+ * @param[in ]        len		buffer length
+ */
+static bool is_neigh_solicit(u8 *buf, size_t len)
+{
+	struct ipv6hdr *iph = (void *)buf;
+	struct nd_msg *msg = (void *)(iph + 1);
+
+	return (len >= sizeof(struct ipv6hdr) + sizeof(struct nd_msg) &&
+		iph->nexthdr == IPPROTO_ICMPV6 &&
+		msg->icmph.icmp6_code == 0 &&
+		msg->icmph.icmp6_type == NDISC_NEIGHBOUR_SOLICITATION);
+}
+
+/**
+ * @brief pass data packet to stack
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ * @param[in ]            mhi_buf			struct mhi_buf
+ * @param[in ]            buf				buffer
+ * @param[in ]            len 				buffer size
+ * @param[in ]            offset			offset
+ */
+static void mhi_netdev_push_skb(struct mhi_netdev *mhi_netdev,
+				struct mhi_buf *mhi_buf,
+				u8 *buf,
+                u16 len,
+                u32 offset,
+				u16 tci)
+{
+	__be16 proto = htons(ETH_P_802_3);
+	struct sk_buff *skb;
+
+	MSG_LOG("tci %d\n", tci);
+
+	if (tci < 128 || tci == MBIM_IPS0_VID) { /* IPS session? */
+		if (len < sizeof(struct iphdr))
+			goto err;
+
+		switch (*buf & 0xf0) {
+		case 0x40:
+			proto = htons(ETH_P_IP);
+			break;
+		case 0x60:
+			if (is_neigh_solicit(buf, len))
+				do_neigh_solicit(mhi_netdev->ndev, buf, tci);
+			proto = htons(ETH_P_IPV6);
+			break;
+		default:
+			goto err;
+		}
+	}
+
+	MSG_LOG("proto 0x%x\n", proto);
+
+    /* reserve the head room and also the ethernet header */
+	skb = alloc_skb(len + HEADER_RESERVE + ETH_HLEN, GFP_ATOMIC);
+	if (!skb) {
+		MSG_ERR("Failed to allocate skb!\n");
+		return;
+	}
+
+	skb_reserve(skb, HEADER_RESERVE);
+
+	MSG_LOG("skb headroom: %d\n", skb_headroom(skb));
+
+	skb_put(skb, ETH_HLEN);
+	skb_reset_mac_header(skb);
+	eth_hdr(skb)->h_proto = proto;
+	eth_zero_addr(eth_hdr(skb)->h_source);
+	memcpy(eth_hdr(skb)->h_dest, mac_addr, ETH_ALEN);
+
+	/* add datagram */
+	skb_put_data(skb, buf, len);
+
+	skb->dev = mhi_netdev->ndev;
+	// skb->protocol = mhi_netdev_ip_type_trans(*(u8 *)buf);
+
+	if (skb->protocol == 0)
+		skb->protocol = eth_type_trans(skb, mhi_netdev->ndev);
+
+	MSG_LOG("eth_type_trans protocol 0x%x pkt_type %d!", skb->protocol, skb->pkt_type);
+	skb->pkt_type = PACKET_HOST;
+
+	/* map MBIM session to VLAN */
+	if (tci)
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tci);
+
+#if 0
+    MSG_LOG("before netif_receive_skb\n");
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        skb->data, skb->len, true);
+#endif
+
+	netif_receive_skb(skb);
+
+err: 
+	return;
+}
+
+/**
+ * @brief status callback
+ *
+ * @param[in ]            mhi_netdev		struct mhi_netdev
+ * @param[in ]            mhi_cb			enum MHI_CB
+ */
+static void mhi_netdev_status_cb(struct mhi_device *mhi_dev, enum MHI_CB mhi_cb)
+{
+	struct mhi_netdev *mhi_netdev = mhi_device_get_devdata(mhi_dev);
+
+	if (mhi_cb != MHI_CB_PENDING_DATA)
+		return;
+
+	napi_schedule(mhi_netdev->napi);
+}
+
+#ifdef XDP_SUPPORT
+/**
+ * @brief send data packet for XDP 
+ *
+ * @param[in ]            dev		struct net_device
+ * @param[in ]            xdp		struct xdp_buff
+ */
+static int mhi_netdev_xmit_xdp(struct mhi_netdev *mhi_netdev, 
+	struct xdp_buff *xdp)
+{
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+	int res = 0;
+    u32 len = 0;
+
+	struct mhi_netbuf *netbuf = NULL;
+	struct mhi_buf *mhi_buf = NULL;
+	struct mhi_netbuf **netbuf_pool = mhi_netdev->tx_netbuf_pool;
+
+	int i, space;
+	const int pool_size = mhi_netdev->tx_pool_size;
+	u16 offset = 0, index;
+    u8 *buf;
+
+	MSG_LOG("xdp 0x%p len %ld\n", xdp, xdp->data_end - xdp->data_hard_start);
+
+#if 0
+    MSG_LOG("mhi_netdev_xmit_xdp buffer\n");
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        xdp.data_hard_start, xdp->data_end - xdp->data + 4, true);
+#endif
+
+	spin_lock_bh(&mhi_netdev->tx_lock);
+
+	/* first packet (no pending packets)? */
+	if (mhi_netdev->pending_netbuf == NULL) {
+
+		index = mhi_netdev->tx_current_index;
+
+		/* get the a free buffer from the pool */
+		for (i = 0; i < pool_size; i++) {
+			struct mhi_netbuf *tmp = netbuf_pool[index];
+
+			index = (index+1) % pool_size;
+
+			mhi_buf = &tmp->mhi_buf;
+
+	    	MSG_LOG("i %d mhi_buf 0x%px page ref count %d\n", 
+				i, mhi_buf, page_ref_count(mhi_buf->page));
+
+			/* page == 1 idle, buffer is free to reclaim */
+			if (page_ref_count(mhi_buf->page) == 1) {
+				netbuf = tmp;
+				mhi_netdev->tx_current_index = index;
+				break;
+			}
+		}
+
+		if (netbuf == NULL)	{
+			MSG_ERR("no free tx buffer!\n");
+			goto drop;
+		}
+
+		offset = 0;
+		mhi_netdev->tx_num_pkt = 0;
+
+	} else {
+		netbuf = mhi_netdev->pending_netbuf;
+		offset = mhi_netdev->tx_offset;
+	}
+
+	/* copy packet (including QMAP header) to mhi_buf */
+	len = xdp->data_end - xdp->data_hard_start;
+
+	mhi_buf = &netbuf->mhi_buf;
+
+	buf = (u8 *)mhi_buf->buf;
+
+	/* copy the QMAP header & payload */
+	memcpy((u8 *)&buf[offset], xdp->data_hard_start, len);
+	offset += len;
+
+	/* save the offset */
+	mhi_netdev->tx_offset = offset;
+
+	mhi_buf->pkt_len = offset;
+
+#if 0
+    MSG_LOG("netdev_xmit buffer after adding QMAP header\n");
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        mhi_buf->buf[offset], len, true);
+#endif
+
+	mhi_netdev->tx_num_pkt++;
+	mhi_buf->pkt_num = mhi_netdev->tx_num_pkt;
+
+	/* make sure the buffer is enough for another packet */
+	space = mhi_buf->len - offset - mhi_netdev->ndev->mtu - 4;
+
+	MSG_LOG("pkt %d space %d\n", mhi_netdev->tx_num_pkt, space);
+
+	if ((mhi_netdev->tx_num_pkt >= MAX_UL_PKT_AGGR) || (space < 0))
+	{
+		MSG_LOG("accumulate %d pkt ready to send!\n", mhi_netdev->tx_num_pkt);
+
+		/* increment reference count so when network stack is done
+		 * with buffer, the buffer won't be freed
+		 */
+		page_ref_inc(mhi_buf->page);
+
+		res = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, mhi_buf, mhi_netdev->tx_offset,
+				 MHI_EOT);
+
+		if (res) {
+			MSG_VERB("Failed to queue with reason:%d\n", res);
+			page_ref_dec(mhi_buf->page);
+
+			/* drop the packet */
+			dev->stats.tx_dropped += mhi_netdev->tx_num_pkt;
+		}
+
+		mhi_netdev->pending_netbuf = NULL;
+
+		goto exit;
+	}
+
+	if (mhi_netdev->tx_num_pkt < CDC_NCM_RESTART_TIMER_DATAGRAM_CNT)
+		mhi_netdev->tx_timer_pending = CDC_NCM_TIMER_PENDING_CNT;
+
+	/* start timer, if there is a buffer waiting */
+	if (mhi_netdev->pending_netbuf != NULL && mhi_netdev->tx_num_pkt > 0) {
+
+		MSG_LOG("start tx timer!\n");
+
+		mhi_netdev_tx_timeout_start(mhi_netdev);
+	}
+
+	goto exit;
+
+drop:
+	mhi_netdev->ndev->stats.tx_dropped++;
+
+exit:
+	spin_unlock_bh(&mhi_netdev->tx_lock);
+
+	return res;	
+}	
+
+static u32 mhi_netdev_run_xdp(struct mhi_netdev *mhi_netdev,
+			struct xdp_buff *xdp)
+{
+	int result;
+	struct bpf_prog *xdp_prog;
+	u32 act = XDP_PASS;		/* default action */
+
+	xdp_prog = mhi_netdev->xdp_prog;
+
+	if (!xdp_prog)
+		goto xdp_out;
+
+	act = bpf_prog_run_xdp(xdp_prog, xdp);
+	
+	MSG_LOG("action: %d\n", act);
+	switch (act) {
+	case XDP_PASS:
+		MSG_LOG("action: XDP_PASS\n");
+		break;
+	case XDP_TX:
+		MSG_LOG("action: XDP_TX\n");
+		result = mhi_netdev_xmit_xdp(mhi_netdev, xdp);
+		break;
+	case XDP_REDIRECT:
+		MSG_LOG("action: XDP_REDIRECT\n");
+		result = xdp_do_redirect(mhi_netdev->ndev, xdp, xdp_prog);
+		break;
+	default:
+		bpf_warn_invalid_xdp_action(act);
+		/* fallthrough */
+	case XDP_ABORTED:
+		MSG_LOG("action: XDP_ABORTED\n");
+		//trace_xdp_exception(mhi_netdev->ndev, xdp_prog, act);
+		/* fallthrough -- handle aborts by dropping packet */
+	case XDP_DROP:
+		MSG_LOG("action: XDP_DROP\n");
+		break;
+	}
+xdp_out:
+	return act;
+}
+#endif 
+
+/**
+ * @brief device to host transfer callback
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ * @param[in ]            mhi_result		struct mhi_result
+ */
+static void mhi_netdev_xfer_dl_cb(struct mhi_device *mhi_dev,
+				  struct mhi_result *mhi_result)
+{
+	struct mhi_netdev *mhi_netdev = mhi_device_get_devdata(mhi_dev);
+	struct mhi_netbuf *netbuf = mhi_result->buf_addr;
+	struct mhi_buf *mhi_buf = &netbuf->mhi_buf;
+	//struct sk_buff *skb;
+	struct net_device *ndev = mhi_netdev->ndev;
+	struct device *dev = mhi_dev->dev.parent;
+	//struct mhi_net_chain *chain = mhi_netdev->chain;
+	u8 *header, *buf, *rx_buf = (u8 *)mhi_buf->buf;
+	u32 offset = 0;
+	u16 len = 0, tci;
+
+#ifdef XDP_SUPPORT
+	struct xdp_buff xdp;
+	u32 act;
+#endif
+
+	netbuf->unmap(dev, mhi_buf->dma_addr, mhi_buf->len, DMA_FROM_DEVICE);
+
+	/* modem is down, drop the buffer */
+	if (mhi_result->transaction_status == -ENOTCONN) {
+		__free_pages(mhi_buf->page, mhi_netdev->rx_order);
+		return;
+	}
+
+	MSG_LOG("rx_bytes: %ld\n", (long)mhi_result->bytes_xferd);
+
+#if 0
+    print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+        mhi_buf->buf, mhi_result->bytes_xferd, true);
+#endif
+
+next_packet:
+    buf = (u8 *)&rx_buf[offset];
+	header = (u8 *)&rx_buf[offset];
+
+	/* max pad is 3 and C/D bit should be 0 */
+	if (header[0] > 3) {
+		MSG_ERR("invalid QMAP pad or c/d bit 0x%x!", header[0]);
+		goto error;
+	}
+
+	/* range of max_id is 1 to 127, 128 to 255 are reserved */
+	if (header[1] > 127) {
+		MSG_ERR("invalid QMAP mux_id 0x%x!", header[1]);
+		goto error;
+	}
+	
+	tci = header[1] & 0x7f;
+
+	if (!tci && mhi_netdev->flags & FLAG_IPS0_VLAN)
+		tci = MBIM_IPS0_VID;
+
+	len = (((u16)header[2]) << 8) + header[3];
+
+	if ((len < header[0]) || (len > mhi_result->bytes_xferd)) {
+		MSG_ERR("length invalid %d!", len);
+        goto error;
+	}
+
+	offset += 4;
+    buf += 4;
+
+	ndev->stats.rx_packets++;
+	ndev->stats.rx_bytes += (len - header[0]);
+
+#ifdef XDP_SUPPORT
+	xdp.data = buf;
+	xdp.data_end = buf + len - header[0];
+	xdp.data_meta = xdp.data;
+	xdp.data_hard_start = buf - 4;
+	xdp.handle = 0;
+	xdp.rxq = NULL;
+
+	act = mhi_netdev_run_xdp(mhi_netdev, &xdp);
+
+	if (act == XDP_PASS) {
+
+		MSG_LOG("pkt buf 0x%p len %d\n", buf, len - header[0]);
+		mhi_netdev_push_skb(mhi_netdev, mhi_buf, buf, len - header[0], offset, tci);
+	}
+#else
+	mhi_netdev_push_skb(mhi_netdev, mhi_buf, buf, len - header[0], offset, tci);
+#endif
+
+#if 0
+	if (unlikely(!chain)) {
+		mhi_netdev_push_skb(mhi_netdev, mhi_buf, buf, len, offset);
+		goto next;
+	}
+
+	/* we support chaining */
+	skb = alloc_skb(0, GFP_ATOMIC);
+	if (likely(skb)) {
+		skb_add_rx_frag(skb, 0, mhi_buf->page, offset,
+				len, mhi_netdev->mru);
+		/* this is first on list */
+		if (!chain->head) {
+			skb->dev = ndev;
+			skb->protocol =
+				mhi_netdev_ip_type_trans(*(u8 *)buf);
+			chain->head = skb;
+		} else {
+			skb_shinfo(chain->tail)->frag_list = skb;
+		}
+
+		chain->tail = skb;
+	} else goto error;
+#endif
+
+//next:
+	offset += len;
+	MSG_LOG("offset %d\n", offset);
+
+	if ((offset + 1) < mhi_result->bytes_xferd)
+		goto next_packet;
+
+    /* send more pend data read requests */
+    mhi_netdev_status_cb(mhi_dev, MHI_CB_PENDING_DATA);
+    
+error:
+    __free_pages(mhi_buf->page, mhi_netdev->rx_order);
+    return;
+}
+
+#ifdef CONFIG_DEBUG_FS
+
+struct dentry *dentry;
+
+static void mhi_netdev_create_debugfs(struct mhi_netdev *mhi_netdev)
+{
+	char node_name[32];
+	struct mhi_device *mhi_dev = mhi_netdev->mhi_dev;
+
+	/* Both tx & rx client handle contain same device info */
+	snprintf(node_name, sizeof(node_name), "%s_%04x_%02u.%02u.%02u_%u",
+		 mhi_netdev->interface_name, mhi_dev->dev_id, mhi_dev->domain,
+		 mhi_dev->bus, mhi_dev->slot, mhi_netdev->alias);
+
+	if (IS_ERR_OR_NULL(dentry))
+		return;
+
+	mhi_netdev->dentry = debugfs_create_dir(node_name, dentry);
+	if (IS_ERR_OR_NULL(mhi_netdev->dentry))
+		return;
+}
+
+static void mhi_netdev_create_debugfs_dir(void)
+{
+	dentry = debugfs_create_dir(MHI_NETDEV_DRIVER_NAME, 0);
+}
+
+#else
+
+static void mhi_netdev_create_debugfs(struct mhi_netdev_private *mhi_netdev)
+{
+}
+
+static void mhi_netdev_create_debugfs_dir(void)
+{
+}
+
+#endif
+
+
+/**
+ * @brief remove net device
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ */
+static void mhi_netdev_remove(struct mhi_device *mhi_dev)
+{
+	struct mhi_netdev *mhi_netdev = mhi_device_get_devdata(mhi_dev);
+
+	MSG_DEFAULT("enter\n");
+
+   	/* stop the channel */
+	MSG_LOG("stop channel!\n");
+	mhi_unprepare_from_transfer(mhi_netdev->mhi_dev);
+
+	/* stop the tx timer */
+	atomic_set(&mhi_netdev->stop, 1);
+
+	if (hrtimer_active(&mhi_netdev->tx_timer))
+		hrtimer_cancel(&mhi_netdev->tx_timer);
+
+	tasklet_kill(&mhi_netdev->bh);
+
+	/* rsc parent takes cares of the cleanup */
+	if (mhi_netdev->is_rsc_dev) {
+		mhi_netdev_free_pool(mhi_netdev);
+		return;
+	}
+
+	netif_stop_queue(mhi_netdev->ndev);
+	napi_disable(mhi_netdev->napi);
+	unregister_netdev(mhi_netdev->ndev);
+	netif_napi_del(mhi_netdev->napi);
+	free_netdev(mhi_netdev->ndev);
+	mhi_netdev_free_pool(mhi_netdev);
+
+#ifdef CONFIG_DEBUG_FS
+	if (!IS_ERR_OR_NULL(mhi_netdev->dentry))
+		debugfs_remove_recursive(mhi_netdev->dentry);
+#endif		
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,3,0)
+static int mhi_netdev_match(struct device *dev, void *data)
+#else
+static int mhi_netdev_match(struct device *dev, const void *data)
+#endif
+{
+	/* if phandle dt == device dt, we found a match */
+	return (dev->of_node == data);
+}
+
+static void mhi_netdev_clone_dev(struct mhi_netdev *mhi_netdev,
+				 struct mhi_netdev *parent)
+{
+	mhi_netdev->ndev = parent->ndev;
+	mhi_netdev->napi = parent->napi;
+	mhi_netdev->ipc_log = parent->ipc_log;
+	mhi_netdev->msg_lvl = parent->msg_lvl;
+	mhi_netdev->ipc_log_lvl = parent->ipc_log_lvl;
+	mhi_netdev->is_rsc_dev = true;
+	mhi_netdev->rx_chain = parent->rx_chain;
+}
+
+/**
+ * @brief probe net device
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ * @param[in ]            id				struct mhi_device_id
+ */
+static int mhi_netdev_probe(struct mhi_device *mhi_dev,
+			    const struct mhi_device_id *id)
+{
+	int ret;
+	struct mhi_netdev *mhi_netdev, *p_netdev = NULL;
+	struct device_node *of_node = mhi_dev->dev.of_node;
+	int nr_tre;
+	char node_name[32];
+	struct device_node *phandle;
+	bool no_chain;
+
+    MSG_DEFAULT("enter\n");
+
+	mhi_netdev = devm_kzalloc(&mhi_dev->dev, sizeof(*mhi_netdev),
+				  GFP_KERNEL);
+	if (!mhi_netdev)
+		return -ENOMEM;
+
+	mhi_netdev->mhi_dev = mhi_dev;
+	mhi_device_set_devdata(mhi_dev, mhi_netdev);
+
+    mhi_netdev->mru = QMAP_RX_BUFFER_SIZE;
+	mhi_netdev->mtu = QMAP_TX_BUFFER_SIZE;
+
+	/* MRU must be multiplication of page size */
+	mhi_netdev->rx_order = __ilog2_u32(mhi_netdev->mru / PAGE_SIZE);
+	if ((PAGE_SIZE << mhi_netdev->rx_order) < mhi_netdev->mru) {
+		return -EINVAL;
+    }
+
+	/* MTU must be multiplication of page size */
+	mhi_netdev->tx_order = __ilog2_u32(mhi_netdev->mtu / PAGE_SIZE);
+	if ((PAGE_SIZE << mhi_netdev->tx_order) < mhi_netdev->mtu) {
+		return -EINVAL;
+    }
+
+	/* check if this device shared by a parent device */
+	phandle = of_parse_phandle(of_node, "mhi,rsc-parent", 0);
+	if (phandle) {
+		struct device *dev;
+		struct mhi_device *pdev;
+		/* find the parent device */
+		dev = driver_find_device(mhi_dev->dev.driver, NULL, phandle,
+					 mhi_netdev_match);
+		if (!dev)
+			return -ENODEV;
+
+		/* this device is shared with parent device. so we won't be
+		 * creating a new network interface. Clone parent
+		 * information to child node
+		 */
+		pdev = to_mhi_device(dev);
+		p_netdev = mhi_device_get_devdata(pdev);
+		mhi_netdev_clone_dev(mhi_netdev, p_netdev);
+		put_device(dev);
+	} else {
+        if (debug) {
+		    mhi_netdev->msg_lvl = MHI_MSG_LVL_INFO;
+        } else {     
+		    mhi_netdev->msg_lvl = MHI_MSG_LVL_MASK_ALL;
+        }
+        
+        no_chain = of_property_read_bool(of_node,
+						 "mhi,disable-chain-skb");
+		if (!no_chain) {
+			mhi_netdev->rx_chain = devm_kzalloc(&mhi_dev->dev,
+						sizeof(*mhi_netdev->rx_chain),
+						GFP_KERNEL);
+			if (!mhi_netdev->rx_chain)
+				return -ENOMEM;
+		}
+
+		ret = mhi_netdev_enable_iface(mhi_netdev);
+		if (ret)
+			return ret;
+
+		/* create ipc log buffer */
+		snprintf(node_name, sizeof(node_name),
+			 "%s_%04x_%02u.%02u.%02u_%u",
+			 mhi_netdev->interface_name, mhi_dev->dev_id,
+			 mhi_dev->domain, mhi_dev->bus, mhi_dev->slot,
+			 mhi_netdev->alias);
+		mhi_netdev->ipc_log = ipc_log_context_create(IPC_LOG_PAGES,
+							     node_name, 0);
+		mhi_netdev->ipc_log_lvl = IPC_LOG_LVL;
+
+		mhi_netdev_create_debugfs(mhi_netdev);
+	}
+
+	/* move mhi channels to start state */
+	ret = mhi_prepare_for_transfer(mhi_dev);
+	if (ret) {
+		MSG_ERR("Failed to start channels ret %d\n", ret);
+		goto error_start;
+	}
+
+	/* setup pool size ~2x ring length*/
+	nr_tre = mhi_get_no_free_descriptors(mhi_dev, DMA_FROM_DEVICE);
+	mhi_netdev->rx_pool_size = 1 << __ilog2_u32(nr_tre);
+	if (nr_tre > mhi_netdev->rx_pool_size)
+		mhi_netdev->rx_pool_size <<= 1;
+	//mhi_netdev->pool_size <<= 1;
+	MSG_LOG("rx pool size %d\n", mhi_netdev->rx_pool_size);
+
+	/* setup pool size ~2x ring length*/
+	nr_tre = mhi_get_no_free_descriptors(mhi_dev, DMA_TO_DEVICE);
+	mhi_netdev->tx_pool_size = 1 << __ilog2_u32(nr_tre);
+	if (nr_tre > mhi_netdev->tx_pool_size)
+		mhi_netdev->tx_pool_size <<= 1;
+	//mhi_netdev->pool_size <<= 1;
+	MSG_LOG("tx pool size %d\n", mhi_netdev->tx_pool_size);
+
+	/* allocate memory pool */
+	ret = mhi_netdev_alloc_pool(mhi_netdev);
+	if (ret)
+		goto error_start;
+
+	/* link child node with parent node if it's children dev */
+	if (p_netdev)
+		p_netdev->rsc_dev = mhi_netdev;
+
+	/* now we have a pool of buffers allocated, queue to hardware
+	 * by triggering a napi_poll
+	 */
+	napi_schedule(mhi_netdev->napi);
+
+	spin_lock_init(&mhi_netdev->tx_lock);
+
+	mhi_netdev->timer_interval = CDC_NCM_TIMER_INTERVAL_USEC * NSEC_PER_USEC;
+
+	atomic_set(&mhi_netdev->stop, 0);
+
+	/* set the tx timer */
+	hrtimer_init(&mhi_netdev->tx_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	mhi_netdev->tx_timer.function = &mhi_netdev_tx_timer_cb;
+
+    tasklet_init(&mhi_netdev->bh, mhi_netdev_txpath_bh, (unsigned long)mhi_netdev);
+
+    MSG_DEFAULT("success\n");
+
+	return 0;
+
+error_start:
+	if (phandle)
+		return ret;
+
+	netif_stop_queue(mhi_netdev->ndev);
+	napi_disable(mhi_netdev->napi);
+	unregister_netdev(mhi_netdev->ndev);
+	netif_napi_del(mhi_netdev->napi);
+	free_netdev(mhi_netdev->ndev);
+
+    MSG_DEFAULT("fail!\n");
+	return ret;
+}
+
+static const struct mhi_device_id mhi_netdev_match_table[] = {
+	{ .chan = "IP_HW0" },
+	{ .chan = "IP_HW0_RSC" },
+	{},
+};
+MODULE_DEVICE_TABLE(mhi, mhi_netdev_match_table);
+
+static struct mhi_driver mhi_netdev_driver = {
+	.id_table = mhi_netdev_match_table,
+	.probe = mhi_netdev_probe,
+	.remove = mhi_netdev_remove,
+	.ul_xfer_cb = mhi_netdev_xfer_ul_cb,
+	.dl_xfer_cb = mhi_netdev_xfer_dl_cb,
+	.status_cb = mhi_netdev_status_cb,
+	.driver = {
+		.name = "mhi_netdev",
+		.owner = THIS_MODULE,
+	}
+};
+
+/**
+ * @brief module init
+ */
+static int __init mhi_netdev_init(void)
+{
+    int ret;
+
+	BUILD_BUG_ON(sizeof(struct mhi_netbuf) > MAX_NETBUF_SIZE);
+
+    MSG_DEFAULT("enter\n");
+
+	mhi_netdev_create_debugfs_dir();
+
+    ret = mhi_driver_register(&mhi_netdev_driver);
+
+	return ret;
+}
+
+/**
+ * @brief module exit
+ */
+static void __exit mhi_netdev_exit(void)
+{
+    MSG_DEFAULT("enter\n");
+
+    mhi_driver_unregister(&mhi_netdev_driver);
+}
+
+module_init(mhi_netdev_init);
+module_exit(mhi_netdev_exit);
+
+MODULE_DESCRIPTION("MHI NETDEV Network Interface");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION("1.9.2101.1");
diff --git a/drivers/staging/em9190/devices/mhi_tty.c b/drivers/staging/em9190/devices/mhi_tty.c
new file mode 100644
index 000000000000..19ae555543ee
--- /dev/null
+++ b/drivers/staging/em9190/devices/mhi_tty.c
@@ -0,0 +1,1210 @@
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/tty.h>
+#include <linux/tty_driver.h>
+#include <linux/tty_flip.h>
+#include <linux/serial.h>
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <linux/version.h>
+#include <linux/dma-direction.h>
+#include <linux/dma-mapping.h>
+#include <linux/list.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,12,0)
+#include <linux/sched/signal.h>
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+
+#include "../inc/ipc_logging.h"
+#include "../inc/mhi.h"
+#include "../core/mhi_internal.h"
+#include "../inc/msm_mhi_dev.h"
+
+#include "mhi_tty.h"
+
+bool debug = false;
+
+module_param(debug, bool, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug,"enable/disable driver logging");
+
+int debug_level = MHI_MSG_LVL_INFO;
+module_param(debug_level, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug_level,"driver logging level");
+
+static struct mhi_tty_drv mhi_tty_drv;
+
+#define MAX_MHITTY_DEVICES (256)
+
+static DECLARE_BITMAP(mhitty_seq, MAX_MHITTY_DEVICES);
+static DECLARE_BITMAP(mhisahara_seq, MAX_MHITTY_DEVICES);
+
+struct tty_dev *mhi_tty_get_dev(struct mhi_device *mhi_dev)
+{
+    struct tty_dev *tty_dev = NULL;
+
+	mutex_lock(&mhi_tty_drv.lock);
+    
+    tty_dev = mhi_device_get_devdata(mhi_dev);
+    if (tty_dev == NULL) {
+       	MSG_LOG("device was removed!\n");
+        mutex_unlock(&mhi_tty_drv.lock);
+        return NULL;
+    }
+
+    mutex_unlock(&mhi_tty_drv.lock);
+
+    return tty_dev;
+}    
+
+/**
+ * @brief queue income messages 
+ *
+ * @param[in ]            tty_dev				struct tty_dev
+ */
+static int mhi_queue_inbound(struct tty_dev *tty_dev)
+{
+	struct mhi_device *mhi_dev = tty_dev->mhi_dev;
+	int nr_trbs = mhi_get_no_free_descriptors(mhi_dev, DMA_FROM_DEVICE);
+	size_t mtu = tty_dev->mtu;
+	void *buf;
+	struct tty_buf *tty_buf;
+	int ret = -EIO, i;
+
+	MSG_LOG("nr_trbs %d mtu %ld\n", nr_trbs, (unsigned long)mtu);
+
+	for (i = 0; i < nr_trbs; i++) {
+		buf = kmalloc(mtu + sizeof(*tty_buf), GFP_KERNEL);
+		if (!buf) {
+	        MSG_ERR("failed to allocate memory %ld\n", (long)(mtu + sizeof(*tty_buf)));
+			return -ENOMEM;
+        }
+
+		tty_buf = buf + mtu;
+		tty_buf->data = buf;
+
+		MSG_LOG("Allocated buf %d of %d size %lu\n", i, nr_trbs, (unsigned long)mtu);
+
+		ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE, buf, mtu,
+					 MHI_EOT);
+	    
+        MSG_LOG("mhi_queue_transfer i %d ret 0x%x\n", i, ret);
+		
+        if (ret) {
+			kfree(buf);
+			MSG_ERR("Failed to queue buffer %d\n", i);
+			return ret;
+		}
+	}
+
+	MSG_LOG("ret 0x%x\n", ret);
+	return ret;
+}
+
+/**
+ * @brief return rx data to TTY
+ *
+ * @param[in ]        tty_dev        TTY device
+ * @param[in ]        data           data buffer
+ * @param[in ]        data_size      data length
+ */
+void mhi_tty_return_data(struct tty_dev *tty_dev, u8 *data, u32 data_size)
+{
+	struct mhi_serial *tiny = NULL;
+	struct tty_struct *tty;
+	int copied = 0;
+
+    MSG_LOG("mhi_tty_return_data %p len %d", data, data_size);
+
+    if ((data == NULL) || (data_size == 0)) {
+        
+		MSG_LOG("no data return!");
+        
+        return;
+    }
+
+	tiny = &tty_dev->tty_table;
+
+	if (!tiny)
+		return;
+
+#if 0
+	print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+		data, data_size, true);
+#endif
+
+	tty = tiny->tty;
+
+	/* check if the tty open or not*/
+	if ((tty == NULL) || (tty->port == NULL))
+		return;
+
+	/* send the data to the tty layer for users to read.  This doesn't
+	 * actually push the data through unless tty->low_latency is set */
+	tty_buffer_request_room(tty->port, data_size);
+	copied = tty_insert_flip_string(tty->port, data, data_size);
+	// MSG_DEFAULT("copied %d data_size %\n", copied, data_size);
+	if (copied != data_size) {
+		MSG_ERR("copied %d != data_size %d!\n", copied, data_size);
+	}
+	tty_flip_buffer_push(tty->port);
+}
+
+/**
+ * @brief open TTY device
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        file			file handle
+ */
+static int tiny_open(struct tty_struct *tty, struct file *file)
+{
+	struct mhi_serial *tiny;
+	struct tty_dev *tty_dev;
+    int ret, count;
+	struct tty_chan *dl_chan;
+	struct tty_buf *buf_itr, *tmp;
+
+	tty_dev = (struct tty_dev *)dev_get_drvdata(tty->dev);
+
+	tty->driver_data = NULL;
+    if (tty_dev->removed) {
+       	MSG_LOG("device was removed!\n");
+        return -ENODEV;
+    }
+
+	/* get the serial object associated with this tty pointer */
+	tiny = &tty_dev->tty_table;
+
+	count = atomic_inc_return(&tiny->open_count);
+
+#if 0
+    /* opened? */
+    if (count != 1) {
+       	MSG_ERR("device was opened!\n");
+        return -EBUSY;
+    }     
+
+	/* initialize the pointer in case something fails */
+	tty->driver_data = NULL;
+#endif
+
+	MSG_LOG("0x%p\n", tty->dev);
+
+	sema_init(&tiny->sem, 1);
+
+	down(&tiny->sem);
+
+	/* save our structure within the tty structure */
+	tty->driver_data = tiny;
+	tiny->tty = tty;
+
+	tty_dev->tport.tty = tty;
+	tty->port = &tty_dev->tport;
+    
+	if (count == 1) {
+        
+        if (tty_dev->sahara) {
+            /* for SAHARA fw downloading, without this, TTY chops the data in 2K each */
+            set_bit(TTY_NO_WRITE_SPLIT, &tty->flags);
+
+	    tty->port->low_latency = 1;	
+        }
+
+		/* this is the first time this port is opened */
+		/* do any hardwarelist initialization needed here */
+		MSG_LOG("Starting channel\n");
+		if (tty_dev->mhi_dev == NULL) {
+			ret = -ENOTCONN;
+			goto error_open_chan;
+		}
+		
+		ret = mhi_prepare_for_transfer(tty_dev->mhi_dev);
+		if (ret) {
+			MSG_ERR("Error starting transfer channels\n");
+			goto error_open_chan;
+		}
+        
+		MSG_LOG("mhi_queue_inbound\n");
+		ret = mhi_queue_inbound(tty_dev);
+		if (ret)
+			goto error_rx_queue;
+	}
+
+	up(&tiny->sem);
+	return 0;
+
+error_rx_queue:
+	dl_chan = &tty_dev->dl_chan;
+	mhi_unprepare_from_transfer(tty_dev->mhi_dev);
+	list_for_each_entry_safe(buf_itr, tmp, &dl_chan->pending, node) {
+		list_del(&buf_itr->node);
+		kfree(buf_itr->data);
+	}
+
+error_open_chan:
+	atomic_dec(&tiny->open_count);
+	up(&tiny->sem);
+	return ret;
+}
+
+/**
+ * @brief close TTY device helper
+ *
+ * @param[in ]        tiny          TTY device
+ */
+static void do_close(struct mhi_serial *tiny, struct tty_dev *tty_dev)
+{
+    int count;
+
+   	MSG_LOG("Enter!\n");
+
+	down(&tiny->sem);
+
+	if (!atomic_read(&tiny->open_count)) {
+		/* port was never opened */
+		goto exit;
+	}
+
+	count = atomic_dec_return(&tiny->open_count);
+
+   	MSG_LOG("open_count %d\n", count);
+
+	if (count == 0) {
+
+		if (tty_dev->removed) {
+   			MSG_LOG("free tty_dev\n");
+
+			kfree(tty_dev);
+		} else {
+   			MSG_LOG("mhi_unprepare_from_transfer!\n");
+			/* The port is being closed by the last user. */
+			/* Do any hardware specific stuff here */
+	    	mhi_unprepare_from_transfer(tty_dev->mhi_dev);
+		}
+
+	}
+exit:
+	up(&tiny->sem);
+}
+
+/**
+ * @brief close TTY device
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        file			file handle
+ */
+static void tiny_close(struct tty_struct *tty, struct file *file)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+	struct tty_dev *tty_dev = NULL;
+
+   	MSG_LOG("Enter!\n");
+
+	tty_dev = (struct tty_dev *)dev_get_drvdata(tty->dev);
+
+	if (tiny) 
+		do_close(tiny, tty_dev);
+}	
+
+/**
+ * @brief write TTY device
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        file			file handle
+ */
+static int tiny_write(struct tty_struct *tty,
+	const unsigned char *buffer, int count)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+    struct mhi_device *mhi_dev;
+	struct tty_dev *tty_dev = NULL;
+	struct tty_chan *tty_chan;
+	size_t bytes_xfered = 0;
+	int ret = -EINVAL; 
+	int nr_avail;
+
+	tty_dev = (struct tty_dev *)dev_get_drvdata(tty->dev);
+
+	MSG_LOG("enter tty_dev 0x%px\n", tty_dev);
+
+    if (tty_dev->removed) {
+       	MSG_LOG("device was removed!\n");
+        return -ENODEV;
+    }
+
+	mhi_dev = tty_dev->mhi_dev;
+
+	tty_chan = &tty_dev->ul_chan;
+
+	if (!buffer || !count)
+		return -EINVAL;
+
+	if (!tiny)
+		return -ENODEV;
+
+	if (tty->index >= MHI_TTY_MINORS)
+		return -ENODEV;
+
+	down(&tiny->sem);
+
+	if (!atomic_read(&tiny->open_count)) {
+       	MSG_LOG("device not opened!\n");
+
+		/* port was not opened */
+		goto exit;
+	}
+
+	/* confirm channel is active */
+	spin_lock_bh(&tty_chan->lock);
+	if (!tty_dev->enabled) {
+		spin_unlock_bh(&tty_chan->lock);
+		ret = -ERESTARTSYS;
+		goto exit;
+	}
+
+	MSG_LOG("Enter: to xfer:%u bytes\n", count);
+
+	while (count) {
+		size_t xfer_size;
+		void *kbuf;
+		enum MHI_FLAGS flags;
+
+		spin_unlock_bh(&tty_chan->lock);
+
+		/* wait for free descriptors */
+		ret = wait_event_interruptible(tty_chan->wq,
+			(!tty_dev->enabled) ||
+			(nr_avail = mhi_get_no_free_descriptors(mhi_dev,
+				DMA_TO_DEVICE)) > 0);
+
+		if (ret == -ERESTARTSYS || !tty_dev->enabled) {
+			MSG_LOG("Exit signal caught for node or not enabled\n");
+			ret = -ERESTARTSYS;
+			goto exit;
+		}
+
+		xfer_size = min_t(size_t, count, tty_dev->mtu);
+		MSG_LOG("xfer_size:%lu bytes\n", (unsigned long)xfer_size);
+		kbuf = kmalloc(xfer_size, GFP_KERNEL);
+		if (!kbuf) {
+			MSG_ERR("Failed to allocate memory %lu\n", (unsigned long)xfer_size);
+			ret = -ENOMEM;
+			goto exit;
+		}
+
+	    memcpy(kbuf, buffer, count);
+
+		spin_lock_bh(&tty_chan->lock);
+
+		/* if ring is full after this force EOT */
+		if (nr_avail > 1 && (count - xfer_size))
+			flags = MHI_CHAIN;			   
+
+		else
+			flags = MHI_EOT;
+
+		MSG_LOG("dev 0x%p enabled %d\n", tty_dev, tty_dev->enabled);
+
+		if (tty_dev->enabled) {
+#if 0
+			MSG_LOG("write buffer\n");
+			print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+				kbuf, xfer_size, true);
+#endif
+			ret = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, kbuf,
+				xfer_size, flags);
+			MSG_LOG("mhi_queue_transfer flags %d ret 0x%x\n", flags, ret);
+		}
+		else
+			ret = -ERESTARTSYS;
+
+		if (ret) {
+			kfree(kbuf);
+			goto sys_interrupt;
+		}
+
+		bytes_xfered += xfer_size;
+		count -= xfer_size;
+		buffer += xfer_size;
+	}
+
+	spin_unlock_bh(&tty_chan->lock);
+	MSG_LOG("Exit: Number of bytes xferred:%lu\n", (unsigned long)bytes_xfered);
+
+	up(&tiny->sem);
+
+	return bytes_xfered;
+
+sys_interrupt:
+	spin_unlock_bh(&tty_chan->lock);
+
+exit:
+	up(&tiny->sem);
+	return ret;
+}
+
+/**
+ * @brief get writing space for TTY device
+ *
+ * @param[in ]        tty           TTY device
+ */
+static int tiny_write_room(struct tty_struct *tty) 
+{
+	struct mhi_serial *tiny = tty->driver_data;
+    struct mhi_device *mhi_dev;
+	struct tty_dev *tty_dev;
+	int room = -EINVAL;
+    int nr_trbs = 0;
+
+	tty_dev = (struct tty_dev *)dev_get_drvdata(tty->dev);
+	MSG_LOG("tty_dev 0x%px removed %d\n", tty_dev, tty_dev->removed);
+
+    if (tty_dev->removed) {
+       	MSG_LOG("device was removed!\n");
+        return -ENODEV;
+    }
+
+    mhi_dev = tty_dev->mhi_dev;
+
+	if (!tiny)
+		return -ENODEV;
+
+	down(&tiny->sem);
+	
+	if (!atomic_read(&tiny->open_count)) {
+		/* port was not opened */
+		goto exit;
+	}
+	nr_trbs = mhi_get_no_free_descriptors(mhi_dev, DMA_TO_DEVICE);
+	MSG_LOG("nr_trbs %d\n", nr_trbs);
+
+    if (nr_trbs) {
+	    /* calculate how much room is left in the device */
+	    room = tty_dev->mtu;
+    } else {
+        room = 0;
+    }    
+exit:
+	up(&tiny->sem);
+	MSG_LOG("room: %d\n", room);
+	return room;
+}
+
+#define RELEVANT_IFLAG(iflag) ((iflag) & (IGNBRK|BRKINT|IGNPAR|PARMRK|INPCK))
+
+/**
+ * @brief set properties for TTY
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        old_termios   properties
+ */
+static void tiny_set_termios(struct tty_struct *tty, struct ktermios *old_termios)
+{
+	unsigned int cflag;
+
+	cflag = tty->termios.c_cflag;
+
+	/* check that they really want us to change something */
+	if (old_termios) {
+		if ((cflag == old_termios->c_cflag) &&
+		    (RELEVANT_IFLAG(tty->termios.c_iflag) == 
+		     RELEVANT_IFLAG(old_termios->c_iflag))) {
+			MSG_LOG(" - nothing to change...\n");
+			return;
+		}
+	}
+
+	/* get the byte size */
+	switch (cflag & CSIZE) {
+		case CS5:
+			MSG_LOG(" - data bits = 5\n");
+			break;
+		case CS6:
+			MSG_LOG(" - data bits = 6\n");
+			break;
+		case CS7:
+			MSG_LOG(" - data bits = 7\n");
+			break;
+		default:
+		case CS8:
+			MSG_LOG(" - data bits = 8\n");
+			break;
+	}
+	
+	/* determine the parity */
+	if (cflag & PARENB)
+		if (cflag & PARODD) {
+			MSG_LOG(" - parity = odd\n");
+        }    
+		else {
+			MSG_LOG(" - parity = even\n");
+        }     
+	else {
+		MSG_LOG(" - parity = none\n");
+    }
+
+	/* figure out the stop bits requested */
+	if (cflag & CSTOPB) {
+		MSG_LOG(" - stop bits = 2\n");
+    }
+    else {
+		MSG_LOG(" - stop bits = 1\n");
+    }
+
+	/* figure out the hardware flow control settings */
+	if (cflag & CRTSCTS) {
+		MSG_LOG(" - RTS/CTS is enabled\n");
+    }     
+	else {
+		MSG_LOG(" - RTS/CTS is disabled\n");
+    }
+
+	/* determine software flow control */
+	/* if we are implementing XON/XOFF, set the start and 
+	 * stop character in the device */
+	if (I_IXOFF(tty) || I_IXON(tty)) {
+		//unsigned char stop_char  = STOP_CHAR(tty);
+		//unsigned char start_char = START_CHAR(tty);
+
+		/* if we are implementing INBOUND XON/XOFF */
+		if (I_IXOFF(tty)) {
+			MSG_LOG(" - INBOUND XON/XOFF is enabled");
+        }
+        else {
+			MSG_LOG(" - INBOUND XON/XOFF is disabled");
+        }
+
+		/* if we are implementing OUTBOUND XON/XOFF */
+		if (I_IXON(tty)) {
+			MSG_LOG(" - OUTBOUND XON/XOFF is enabled");
+        }
+        else {
+			MSG_LOG(" - OUTBOUND XON/XOFF is disabled");
+        }    
+	}
+
+	/* get the baud rate wanted */
+	MSG_LOG(" - baud rate = %d", tty_get_baud_rate(tty));
+}
+
+/* Our fake UART values */
+#define MCR_DTR		0x01
+#define MCR_RTS		0x02
+#define MCR_LOOP	0x04
+#define MSR_CTS		0x08
+#define MSR_CD		0x10
+#define MSR_RI		0x20
+#define MSR_DSR		0x40
+
+/**
+ * @brief get signal settings for TTY
+ *
+ * @param[in ]        tty           TTY device
+ */
+static int tiny_tiocmget(struct tty_struct *tty)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+
+	unsigned int result = 0;
+	unsigned int msr = tiny->msr;
+	unsigned int mcr = tiny->mcr;
+
+	result = ((mcr & MCR_DTR)  ? TIOCM_DTR  : 0) |	/* DTR is set */
+             ((mcr & MCR_RTS)  ? TIOCM_RTS  : 0) |	/* RTS is set */
+             ((mcr & MCR_LOOP) ? TIOCM_LOOP : 0) |	/* LOOP is set */
+             ((msr & MSR_CTS)  ? TIOCM_CTS  : 0) |	/* CTS is set */
+             ((msr & MSR_CD)   ? TIOCM_CAR  : 0) |	/* Carrier detect is set*/
+             ((msr & MSR_RI)   ? TIOCM_RI   : 0) |	/* Ring Indicator is set */
+             ((msr & MSR_DSR)  ? TIOCM_DSR  : 0);	/* DSR is set */
+
+	return result;
+}
+
+/**
+ * @brief set/clear signal for TTY
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        set           set
+ * @param[in ]        clear         clear
+ */
+static int tiny_tiocmset(struct tty_struct *tty,
+                         unsigned int set, unsigned int clear)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+	unsigned int mcr = tiny->mcr;
+
+	if (set & TIOCM_RTS)
+		mcr |= MCR_RTS;
+	if (set & TIOCM_DTR)
+		mcr |= MCR_RTS;
+
+	if (clear & TIOCM_RTS)
+		mcr &= ~MCR_RTS;
+	if (clear & TIOCM_DTR)
+		mcr &= ~MCR_RTS;
+
+	/* set the new MCR value in the device */
+	tiny->mcr = mcr;
+	return 0;
+}
+
+/**
+ * @brief device I/O control
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        cmd           command
+ * @param[in ]        arg           argument
+ */
+#define tiny_ioctl tiny_ioctl_tiocgserial
+static int tiny_ioctl(struct tty_struct *tty, 
+                      unsigned int cmd, unsigned long arg)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+
+	if (cmd == TIOCGSERIAL) {
+		struct serial_struct tmp;
+
+		if (!arg)
+			return -EFAULT;
+
+		memset(&tmp, 0, sizeof(tmp));
+
+		tmp.type		= tiny->serial.type;
+		tmp.line		= tiny->serial.line;
+		tmp.port		= tiny->serial.port;
+		tmp.irq			= tiny->serial.irq;
+		tmp.flags		= ASYNC_SKIP_TEST | ASYNC_AUTO_IRQ;
+		tmp.xmit_fifo_size	= tiny->serial.xmit_fifo_size;
+		tmp.baud_base		= tiny->serial.baud_base;
+		tmp.close_delay		= 5*HZ;
+		tmp.closing_wait	= 30*HZ;
+		tmp.custom_divisor	= tiny->serial.custom_divisor;
+		tmp.hub6		= tiny->serial.hub6;
+		tmp.io_type		= tiny->serial.io_type;
+
+		if (copy_to_user((void __user *)arg, &tmp, sizeof(struct serial_struct)))
+			return -EFAULT;
+		return 0;
+	}
+	return -ENOIOCTLCMD;
+}
+#undef tiny_ioctl
+
+/**
+ * @brief device I/O control
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        cmd           command
+ * @param[in ]        arg           argument
+ */
+#define tiny_ioctl tiny_ioctl_tiocmiwait
+static int tiny_ioctl(struct tty_struct *tty, 
+                      unsigned int cmd, unsigned long arg)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+
+	if (cmd == TIOCMIWAIT) {
+		DECLARE_WAITQUEUE(wait, current);
+		struct async_icount cnow;
+		struct async_icount cprev;
+
+		cprev = tiny->icount;
+		while (1) {
+			add_wait_queue(&tiny->wait, &wait);
+			set_current_state(TASK_INTERRUPTIBLE);
+			schedule();
+			remove_wait_queue(&tiny->wait, &wait);
+
+			/* see if a signal woke us up */
+			if (signal_pending(current))
+				return -ERESTARTSYS;
+
+			cnow = tiny->icount;
+			if (cnow.rng == cprev.rng && cnow.dsr == cprev.dsr &&
+			    cnow.dcd == cprev.dcd && cnow.cts == cprev.cts)
+				return -EIO; /* no change => error */
+			if (((arg & TIOCM_RNG) && (cnow.rng != cprev.rng)) ||
+			    ((arg & TIOCM_DSR) && (cnow.dsr != cprev.dsr)) ||
+			    ((arg & TIOCM_CD)  && (cnow.dcd != cprev.dcd)) ||
+			    ((arg & TIOCM_CTS) && (cnow.cts != cprev.cts)) ) {
+				return 0;
+			}
+			cprev = cnow;
+		}
+
+	}
+	return -ENOIOCTLCMD;
+}
+#undef tiny_ioctl
+
+/**
+ * @brief device I/O control
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        cmd           command
+ * @param[in ]        arg           argument
+ */
+#define tiny_ioctl tiny_ioctl_tiocgicount
+static int tiny_ioctl(struct tty_struct *tty, 
+                      unsigned int cmd, unsigned long arg)
+{
+	struct mhi_serial *tiny = tty->driver_data;
+
+	if (cmd == TIOCGICOUNT) {
+		struct async_icount cnow = tiny->icount;
+		struct serial_icounter_struct icount;
+
+		icount.cts	= cnow.cts;
+		icount.dsr	= cnow.dsr;
+		icount.rng	= cnow.rng;
+		icount.dcd	= cnow.dcd;
+		icount.rx	= cnow.rx;
+		icount.tx	= cnow.tx;
+		icount.frame	= cnow.frame;
+		icount.overrun	= cnow.overrun;
+		icount.parity	= cnow.parity;
+		icount.brk	= cnow.brk;
+		icount.buf_overrun = cnow.buf_overrun;
+
+		if (copy_to_user((void __user *)arg, &icount, sizeof(icount)))
+			return -EFAULT;
+		return 0;
+	}
+	return -ENOIOCTLCMD;
+}
+#undef tiny_ioctl
+
+/**
+ * @brief device I/O control
+ *
+ * the real tiny_ioctl function.  The above is done to get the small functions in the book
+ *
+ * @param[in ]        tty           TTY device
+ * @param[in ]        cmd           command
+ * @param[in ]        arg           argument
+ */
+static int tiny_ioctl(struct tty_struct *tty, 
+                      unsigned int cmd, unsigned long arg)
+{
+
+	switch (cmd) {
+	case TIOCGSERIAL:
+		return tiny_ioctl_tiocgserial(tty, cmd, arg);
+	case TIOCMIWAIT:
+		return tiny_ioctl_tiocmiwait(tty, cmd, arg);
+	case TIOCGICOUNT:
+		return tiny_ioctl_tiocgicount(tty, cmd, arg);
+	}
+
+	return -ENOIOCTLCMD;
+}
+
+/**
+ * @brief remove TTY device
+ *
+ * @param[in ]            mhi_dev				struct mhi_device
+ */
+static void mhi_tty_remove(struct mhi_device *mhi_dev)
+{
+	struct tty_dev *tty_dev;
+	struct tty_driver *mhi_tty_driver;
+	struct mhi_serial *tiny = NULL;
+
+	MSG_DEFAULT("enter\n");
+
+    tty_dev = mhi_tty_get_dev(mhi_dev);    
+
+    if (tty_dev == NULL || tty_dev->removed) {
+       	MSG_LOG("device was removed!\n");
+        return;
+    }
+
+	mutex_lock(&mhi_tty_drv.lock);
+	mutex_lock(&tty_dev->mutex);
+    
+    mhi_tty_driver = tty_dev->mhi_tty_driver;
+
+	/* disable the node */
+	spin_lock_irq(&tty_dev->dl_chan.lock);
+	spin_lock_irq(&tty_dev->ul_chan.lock);
+	tty_dev->enabled = false;
+	spin_unlock_irq(&tty_dev->ul_chan.lock);
+	spin_unlock_irq(&tty_dev->dl_chan.lock);
+
+	wake_up(&tty_dev->dl_chan.wq);
+	wake_up(&tty_dev->ul_chan.wq);
+
+	/* delete the node to prevent new opens */
+	tty_dev->dev = NULL;
+	list_del(&tty_dev->node);
+
+	MSG_LOG("mhi_tty_driver 0x%px\n", tty_dev->mhi_tty_driver);
+	if (tty_dev->mhi_tty_driver) {
+
+        tty_port_destroy(&tty_dev->tport);
+		tty_unregister_device(mhi_tty_driver, 0);
+
+		tty_unregister_driver(tty_dev->mhi_tty_driver);
+
+		MSG_LOG("mhi_unprepare_from_transfer!\n");
+		mhi_unprepare_from_transfer(mhi_dev);
+
+		tty_dev->mhi_tty_driver = NULL;
+	}
+
+    if (tty_dev->sahara) {
+    	clear_bit(tty_dev->dev_seq, mhisahara_seq);
+    } else {
+    	clear_bit(tty_dev->dev_seq, mhitty_seq);
+    }
+
+	/* set the removed flag */
+	MSG_LOG("tty_dev 0x%px removed is true!\n", tty_dev);
+	tty_dev->removed = true;
+
+    mhi_device_set_devdata(mhi_dev, NULL);
+
+	/* safe to free memory only if all file nodes are closed */
+	tiny = &tty_dev->tty_table;
+	MSG_LOG("open_count %d\n", atomic_read(&tiny->open_count));
+	if (!atomic_read(&tiny->open_count)) {
+
+		mutex_unlock(&tty_dev->mutex);
+		mutex_destroy(&tty_dev->mutex);
+		MSG_LOG("free tty_dev!\n");
+		kfree(tty_dev);
+		mutex_unlock(&mhi_tty_drv.lock);
+		goto exit;
+	}
+
+	mutex_unlock(&tty_dev->mutex);
+	mutex_unlock(&mhi_tty_drv.lock);
+
+exit:
+	MSG_DEFAULT("exit\n");
+}
+
+static struct tty_operations serial_ops = {
+	.open = tiny_open,
+	.close = tiny_close,
+	.write = tiny_write,
+	.write_room = tiny_write_room,
+	.set_termios = tiny_set_termios,
+	.tiocmget = tiny_tiocmget,
+	.tiocmset = tiny_tiocmset,
+	.ioctl = tiny_ioctl,
+};
+
+/**
+ * @brief probe TTY device
+ *
+ * @param[in ]            mhi_dev				struct mhi_device
+ * @param[in ]            id					struct mhi_device_id
+ */
+static int mhi_tty_probe(struct mhi_device *mhi_dev,
+	const struct mhi_device_id *id)
+{
+	struct tty_dev *tty_dev;
+	struct tty_driver *mhi_tty_driver;
+	int dir, ret;
+	struct device *dev;
+    int seq;
+	struct device *parent = mhi_dev->dev.parent;
+
+	MSG_DEFAULT("enter\n");
+
+	MSG_LOG("dev_id: 0x%x domain: 0x%x bus: 0x%x slot: 0x%x\n",
+        mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus, mhi_dev->slot);
+
+	tty_dev = kzalloc(sizeof(*tty_dev), GFP_KERNEL);
+	if (!tty_dev) {
+		MSG_ERR("no memory!\n");
+        ret = -ENOMEM;
+		goto exit0;
+	}
+
+	mutex_init(&tty_dev->mutex);
+	tty_dev->mhi_dev = mhi_dev;
+
+    if (strcmp(id->chan, "SAHARA") == 0) { 
+        tty_dev->sahara = true;
+    }
+
+    /* try to get a sequence number for device */
+	mutex_lock(&tty_dev->mutex);
+    if (tty_dev->sahara) {
+	    seq = find_first_zero_bit(mhisahara_seq, MAX_MHITTY_DEVICES);
+    } else {
+	    seq = find_first_zero_bit(mhitty_seq, MAX_MHITTY_DEVICES);
+    }
+    
+	if (seq >= MAX_MHITTY_DEVICES) {
+		MSG_ERR("No available tty device number %d!\n", seq);
+        mutex_unlock(&tty_dev->mutex);
+		ret = -ENOSPC;
+        goto exit1;
+	}
+
+    tty_dev->dev_seq = seq;
+    if (tty_dev->sahara) {
+    	set_bit(seq, mhisahara_seq);
+    } else {
+    	set_bit(seq, mhitty_seq);
+    }
+	mutex_unlock(&tty_dev->mutex);
+
+	mutex_lock(&tty_dev->mutex);
+	mutex_lock(&mhi_tty_drv.lock);
+
+	for (dir = 0; dir < 2; dir++) {
+		struct tty_chan *tty_chan = (dir) ?
+			&tty_dev->ul_chan : &tty_dev->dl_chan;
+		spin_lock_init(&tty_chan->lock);
+		init_waitqueue_head(&tty_chan->wq);
+		INIT_LIST_HEAD(&tty_chan->pending);
+	};
+
+	tty_dev->mtu = min_t(size_t, id->driver_data, mhi_dev->mtu);
+	mhi_device_set_devdata(mhi_dev, tty_dev);
+	tty_dev->enabled = true;
+
+	MSG_LOG("dev 0x%p enabled %d\n", tty_dev, tty_dev->enabled);
+
+	list_add(&tty_dev->node, &mhi_tty_drv.head);
+	mutex_unlock(&mhi_tty_drv.lock);
+	mutex_unlock(&tty_dev->mutex);
+
+	/* allocate the tty driver */
+	mhi_tty_driver = tty_alloc_driver(MHI_TTY_MINORS, TTY_DRIVER_DYNAMIC_DEV);
+
+	if (IS_ERR(mhi_tty_driver)) {
+		MSG_ERR("failed to alloc memory for tty driver!\n");
+		ret = -ENOMEM;
+        goto exit2;
+	}
+
+	tty_dev->mhi_tty_driver = mhi_tty_driver;
+
+	/* initialize the tty driver */
+	mhi_tty_driver->owner = THIS_MODULE;
+	mhi_tty_driver->driver_name = "mhi_tty";
+	mhi_tty_driver->name = "mhitty";
+    if (tty_dev->sahara)
+	    mhi_tty_driver->name = "mhiqdl";
+	mhi_tty_driver->subtype = SERIAL_TYPE_NORMAL;
+	mhi_tty_driver->flags |= TTY_DRIVER_REAL_RAW;
+	mhi_tty_driver->name_base = tty_dev->dev_seq;
+	mhi_tty_driver->init_termios = tty_std_termios;
+	mhi_tty_driver->init_termios.c_lflag &= ~ECHO;
+	mhi_tty_driver->init_termios.c_cflag = B9600 | CS8 | CREAD | HUPCL | CLOCAL;
+	tty_set_operations(mhi_tty_driver, &serial_ops);
+
+	tty_port_init(&tty_dev->tport);
+
+	/* set the limit to higher value as the recent kernel versions*/
+	tty_buffer_set_limit(&tty_dev->tport, (640 * 1024UL));
+
+	tty_port_link_device(&tty_dev->tport, mhi_tty_driver, 0);
+
+	/* register the tty driver */
+	ret = tty_register_driver(mhi_tty_driver);
+	if (ret) {
+		MSG_ERR("failed to register tiny tty driver 0x%x", ret);
+		put_tty_driver(mhi_tty_driver);
+		mhi_tty_driver = NULL;
+		tty_dev->mhi_tty_driver = mhi_tty_driver;
+		goto exit2;
+	}
+
+	sema_init(&tty_dev->tty_table.sem, 1);
+
+    atomic_set(&tty_dev->tty_table.open_count, 0);
+
+	dev = tty_register_device(mhi_tty_driver, 0, NULL);
+
+	dev_set_drvdata(dev, tty_dev);
+
+    ret = sysfs_create_link(&dev->kobj, &parent->kobj, mhi_dev->dev.kobj.name);
+
+    MSG_LOG("sysfs_create_link 0x%x\n", ret);
+
+    MSG_DEFAULT("probe chan %s successful!\n", mhi_dev->chan_name);
+
+	return 0;
+
+exit2:
+    if (tty_dev->sahara) {
+    	clear_bit(tty_dev->dev_seq, mhisahara_seq);
+    } else {
+    	clear_bit(tty_dev->dev_seq, mhitty_seq);
+    }
+exit1:
+    kfree(tty_dev);
+exit0:
+    return ret;
+};
+
+static void mhi_ul_xfer_cb(struct mhi_device *mhi_dev,
+	struct mhi_result *mhi_result)
+{
+	struct tty_dev *tty_dev = NULL;
+	struct tty_chan *tty_chan;
+
+    tty_dev = mhi_device_get_devdata(mhi_dev);
+    if (tty_dev == NULL) {
+       	MSG_LOG("device was removed!\n");
+	    kfree(mhi_result->buf_addr);
+        return;
+    }
+
+    tty_chan = &tty_dev->ul_chan;
+	MSG_LOG("status:%d xfer_len:%zu\n", mhi_result->transaction_status,
+		mhi_result->bytes_xferd);
+
+	kfree(mhi_result->buf_addr);
+	if (!mhi_result->transaction_status)
+		wake_up(&tty_chan->wq);
+}
+
+/**
+ * @brief device to host transfer callback
+ *
+ * @param[in ]            mhi_dev				struct mhi_device
+ * @param[in ]            mhi_result			struct mhi_result
+ */
+static void mhi_dl_xfer_cb(struct mhi_device *mhi_dev,
+	struct mhi_result *mhi_result)
+{
+	struct tty_dev *tty_dev;
+	//struct tty_chan *tty_chan = &tty_dev->dl_chan;
+	//unsigned long flags;
+	//struct tty_buf *buf;
+    int ret;
+
+	if ((mhi_dev == NULL) || (mhi_result == NULL))
+		return;
+
+    tty_dev = mhi_device_get_devdata(mhi_dev);
+    if (tty_dev == NULL) {
+       	MSG_LOG("device was removed!\n");
+        return;
+    }
+
+	MSG_LOG("status:%d receive_len:%zu\n", mhi_result->transaction_status,
+		mhi_result->bytes_xferd);
+
+	if (mhi_result->transaction_status == -ENOTCONN) {
+		kfree(mhi_result->buf_addr);
+		return;
+	}
+
+	MSG_LOG("chan %d\n", mhi_dev->dl_chan->chan);
+
+	MSG_LOG("mhi_tty_return_data: buf 0x%px len %ld", mhi_result->buf_addr, 
+		(long)mhi_result->bytes_xferd);
+
+	if (mhi_result->buf_addr == NULL)
+		return;
+
+    if (tty_dev) {
+        /* return data */
+        mhi_tty_return_data(tty_dev, mhi_result->buf_addr, mhi_result->bytes_xferd);
+
+        /* send read request */
+        if (tty_dev->enabled)
+        {
+            ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE,
+                                     mhi_result->buf_addr, tty_dev->mtu,
+                                     MHI_EOT);
+
+            MSG_LOG("mhi_queue_transfer 0x%x\n", ret);
+        }
+    } else {
+		kfree(mhi_result->buf_addr);
+        MSG_LOG("incoming tty data dropped!\n");
+    }    
+
+    //spin_lock_irqsave(&tty_chan->lock, flags);
+	//list_add_tail(&buf->node, &tty_chan->pending);
+	//spin_unlock_irqrestore(&tty_chan->lock, flags);
+
+	if (mhi_dev->dev.power.wakeup)
+		__pm_wakeup_event(mhi_dev->dev.power.wakeup, 0);
+
+	//wake_up(&tty_chan->wq);
+}
+
+/* .driver_data stores max mtu */
+static const struct mhi_device_id mhi_tty_match_table[] = {
+	{.chan = "DIAG",.driver_data = 0x1000 },
+	{.chan = "DUN",.driver_data = 0x1000 },
+	{.chan = "SAHARA",.driver_data = 0x8000 },
+	{},
+};
+MODULE_DEVICE_TABLE(mhi, mhi_tty_match_table);
+
+static struct mhi_driver mhi_tty_driver = {
+	.id_table = mhi_tty_match_table,
+	.remove = mhi_tty_remove,
+	.probe = mhi_tty_probe,
+	.ul_xfer_cb = mhi_ul_xfer_cb,
+	.dl_xfer_cb = mhi_dl_xfer_cb,
+	.driver = {
+		.name = MHI_TTY_DRIVER_NAME,
+		.owner = THIS_MODULE,
+	},
+};
+
+/**
+ * @brief initialize TTY 
+ *
+ * @param[in ]        pdev           PCIe device
+ */
+static int __init
+mhitty_init(void)
+{
+	int ret;
+
+	mutex_init(&mhi_tty_drv.lock);
+	INIT_LIST_HEAD(&mhi_tty_drv.head);
+
+	MSG_DEFAULT("Enter\n");
+
+	ret = mhi_driver_register(&mhi_tty_driver);
+
+	MSG_DEFAULT("mhi_driver_register 0x%x\n", ret);
+
+	return ret;
+}
+
+/**
+ * @brief exit TTY
+ *
+ * @param[in ]        pdev           PCIe device
+ */
+static void __exit
+mhitty_exit(void)
+{
+    mhi_driver_unregister(&mhi_tty_driver);
+
+	MSG_DEFAULT("exit\n");
+}
+
+module_init(mhitty_init);
+module_exit(mhitty_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("MHI_TTY");
+MODULE_DESCRIPTION("MHI TTY Driver");
+MODULE_VERSION("1.9.2101.1");
\ No newline at end of file
diff --git a/drivers/staging/em9190/devices/mhi_tty.h b/drivers/staging/em9190/devices/mhi_tty.h
new file mode 100644
index 000000000000..28c29e4146fb
--- /dev/null
+++ b/drivers/staging/em9190/devices/mhi_tty.h
@@ -0,0 +1,117 @@
+/* mhitty.h */
+
+#ifndef MHITTY_H_
+#define MHITTY_H_
+
+#include <linux/tty.h>
+#include <linux/tty_driver.h>
+#include <linux/tty_flip.h>
+#include <linux/serial.h>
+
+#define DELAY_TIME		HZ * 2	/* 2 seconds per character */
+#define TINY_DATA_CHARACTER	't'
+
+#define MHI_TTY_MAJOR		240	/* experimental range */
+#define MHI_TTY_MINORS		1	/* only have 1 device */
+
+#define MHI_TTY_DRIVER_NAME "mhi_tty"
+
+struct tty_buf {
+	void *data;
+	size_t len;
+	struct list_head node;
+};
+
+struct tty_chan {
+	wait_queue_head_t wq;
+	spinlock_t lock;
+	struct list_head pending; /* user space waiting to read */
+	struct tty_buf *cur_buf; /* current buffer user space reading */
+	size_t rx_size;
+};
+
+struct mhi_serial {
+	struct tty_struct *tty;		/* pointer to the tty for this device */
+	atomic_t open_count;	    /* number of times this port has been opened */
+	struct semaphore sem;		/* locks this structure */
+
+	/* for tiocmget and tiocmset functions */
+	int			msr;		/* MSR shadow */
+	int			mcr;		/* MCR shadow */
+
+	/* for ioctl fun */
+	struct serial_struct	serial;
+	wait_queue_head_t	wait;
+	struct async_icount	icount;
+};
+
+struct tty_dev {
+	struct list_head node;
+	dev_t devt;
+	struct device *dev;
+	struct mhi_device *mhi_dev;
+	const char *chan;
+	struct mutex mutex; /* sync open and close */
+	struct tty_chan ul_chan;
+	struct tty_chan dl_chan;
+	size_t mtu;
+	bool enabled;
+	void *ipc_log;
+    bool sahara; 
+
+	struct tty_driver *mhi_tty_driver;
+
+	struct mhi_serial tty_table;	/* initially all NULL */
+	struct tty_port tport;
+
+    int dev_seq;
+	bool removed;
+};
+
+struct mhi_tty_drv {
+	struct list_head head;
+	struct mutex lock;
+};
+
+void mhi_tty_return_data(struct tty_dev *tty_dev, u8 *data, u32 data_size);
+
+extern enum MHI_DEBUG_LEVEL msg_lvl;
+
+#ifdef CONFIG_MHI_DEBUG
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_VERBOSE)
+#define MHI_UCI_IPC_LOG_PAGES (25)
+
+#else
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_ERROR)
+#define MHI_UCI_IPC_LOG_PAGES (1)
+
+#endif
+
+#define MSG_VERB(fmt, ...)
+
+extern bool debug;
+extern int debug_level;
+
+#define MSG_LOG(fmt, ...) do { \
+		if (debug && debug_level <= MHI_MSG_LVL_INFO) \
+			pr_err("[I][%s] " fmt, __func__, ##__VA_ARGS__); \
+	} while (0)
+
+#define MSG_ERR(fmt, ...) do { \
+		if (debug && debug_level <= MHI_MSG_LVL_ERROR) \
+			pr_err("[E][%s] " fmt, __func__, ##__VA_ARGS__); \
+	} while (0)
+
+#define DEFAULT 1
+
+#if DEFAULT
+#define MSG_DEFAULT(fmt, ...) do { \
+		pr_info("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+#else
+#define MSG_DEFAULT(fmt, ...)
+#endif
+
+#endif /* MHITTY_H_ */
diff --git a/drivers/staging/em9190/devices/mhi_uci.c b/drivers/staging/em9190/devices/mhi_uci.c
new file mode 100644
index 000000000000..918b77b52ca9
--- /dev/null
+++ b/drivers/staging/em9190/devices/mhi_uci.c
@@ -0,0 +1,892 @@
+/* Copyright (c) 2018, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/dma-direction.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include "../inc/ipc_logging.h"
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+#include <linux/uaccess.h>
+
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/ctype.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/workqueue.h>
+#include <linux/mii.h>
+#include <linux/crc32.h>
+#include <linux/usb.h>
+#include <linux/hrtimer.h>
+#include <linux/atomic.h>
+#include <linux/usb/usbnet.h>
+#include <linux/usb/cdc.h>
+#include <linux/usb/cdc_ncm.h>
+#include <net/ipv6.h>
+#include <net/addrconf.h>
+//#include <linux/hrtimer.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5,6,0)
+#include <linux/mod_devicetable.h>
+#else
+#include "../inc/devicetable.h"
+#endif
+
+#include "../inc/mhi.h"
+#include "../core/mhi_internal.h"
+#include "../inc/msm_mhi_dev.h"
+
+#include "mhi_uci.h"
+#include "qmap.h"
+
+#ifdef CONFIG_MHI_DEBUG
+enum MHI_DEBUG_LEVEL msg_lvl = MHI_MSG_LVL_INFO;
+#endif 
+
+bool debug = false;
+
+module_param(debug, bool, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug,"enable/disable driver logging");
+
+int debug_level = MHI_MSG_LVL_INFO;
+module_param(debug_level, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH );
+MODULE_PARM_DESC(debug_level,"driver logging level");
+
+static DECLARE_BITMAP(uci_minors, MAX_UCI_DEVICES);
+static struct mhi_uci_drv mhi_uci_drv;
+
+/**
+ * @brief queue read requests for incoming messages
+ *
+ * @param[in ]            uci_dev				struct uci_dev	
+ */
+static int mhi_queue_inbound(struct uci_dev *uci_dev)
+{
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	int nr_trbs = mhi_get_no_free_descriptors(mhi_dev, DMA_FROM_DEVICE);
+	size_t mtu = uci_dev->mtu;
+	void *buf;
+	struct uci_buf *uci_buf;
+	int ret = -EIO, i;
+
+	MSG_LOG("nr_trbs %d mtu %ld\n", nr_trbs, (long)mtu);
+
+	for (i = 0; i < nr_trbs; i++) {
+		buf = kmalloc(mtu + sizeof(*uci_buf), GFP_KERNEL);
+		if (!buf) {
+	        MSG_ERR("failed to allocate memory %ld\n", (unsigned long)(mtu + sizeof(*uci_buf)));
+			return -ENOMEM;
+        }
+
+		uci_buf = buf + mtu;
+		uci_buf->data = buf;
+
+		//MSG_VERB("Allocated buf %d of %d size %u\n", i, nr_trbs, mtu);
+		MSG_LOG("Allocated buf %d of %d size %lu\n", i, nr_trbs, (unsigned long)mtu);
+
+		ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE, buf, mtu,
+					 MHI_EOT);
+	    
+        MSG_LOG("mhi_queue_transfer i %d ret 0x%x\n", i, ret);
+		
+        if (ret) {
+			kfree(buf);
+			MSG_ERR("Failed to queue buffer %d\n", i);
+			return ret;
+		}
+	}
+
+	return ret;
+}
+
+/**
+ * @brief device ioctl handler
+ *
+ * @param[in ]            file				struct file
+ * @param[in ]            cmd				command
+ * @param[in ]            arg				argument
+ */
+static long mhi_uci_ioctl(struct file *file,
+			  unsigned int cmd,
+			  unsigned long arg)
+{
+	struct uci_dev *uci_dev = file->private_data;
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	long ret = -ERESTARTSYS;
+
+	mutex_lock(&uci_dev->mutex);
+	if (uci_dev->enabled)
+		ret = mhi_ioctl(mhi_dev, cmd, arg);
+	mutex_unlock(&uci_dev->mutex);
+
+	return ret;
+}
+
+/**
+ * @brief release the char device
+ *
+ * @param[in ]            inode				struct inode
+ * @param[in ]            file				struct file
+ */
+static int mhi_uci_release(struct inode *inode, struct file *file)
+{
+	struct uci_dev *uci_dev = file->private_data;
+
+	mutex_lock(&uci_dev->mutex);
+	uci_dev->ref_count--;
+	if (!uci_dev->ref_count) {
+		struct uci_buf *itr, *tmp;
+		struct uci_chan *uci_chan;
+
+		MSG_LOG("Last client left, closing node\n");
+
+		if (uci_dev->enabled)
+			mhi_unprepare_from_transfer(uci_dev->mhi_dev);
+
+		/* clean inbound channel */
+		uci_chan = &uci_dev->dl_chan;
+		list_for_each_entry_safe(itr, tmp, &uci_chan->pending, node) {
+			list_del(&itr->node);
+			kfree(itr->data);
+		}
+		if (uci_chan->cur_buf)
+			kfree(uci_chan->cur_buf->data);
+
+		uci_chan->cur_buf = NULL;
+
+		if (!uci_dev->enabled) {
+			MSG_LOG("Node is deleted, freeing dev node\n");
+			mutex_unlock(&uci_dev->mutex);
+			mutex_destroy(&uci_dev->mutex);
+			clear_bit(MINOR(uci_dev->devt), uci_minors);
+			kfree(uci_dev);
+			return 0;
+		}
+	}
+
+	MSG_LOG("exit: ref_count:%d\n", uci_dev->ref_count);
+
+	mutex_unlock(&uci_dev->mutex);
+
+	return 0;
+}
+
+/**
+ * @brief poll the char device
+ *
+ * @param[in ]            file				struct file
+ * @param[in ]            wait				poll_table
+ */
+static unsigned int mhi_uci_poll(struct file *file, poll_table *wait)
+{
+	struct uci_dev *uci_dev = file->private_data;
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	struct uci_chan *uci_chan;
+	unsigned int mask = 0;
+
+	poll_wait(file, &uci_dev->dl_chan.wq, wait);
+	poll_wait(file, &uci_dev->ul_chan.wq, wait);
+
+	uci_chan = &uci_dev->dl_chan;
+	spin_lock_bh(&uci_chan->lock);
+	if (!uci_dev->enabled) {
+		mask = POLLERR;
+	} else if (!list_empty(&uci_chan->pending) || uci_chan->cur_buf) {
+		MSG_VERB("Client can read from node\n");
+		mask |= POLLIN | POLLRDNORM;
+	}
+	spin_unlock_bh(&uci_chan->lock);
+
+	uci_chan = &uci_dev->ul_chan;
+	spin_lock_bh(&uci_chan->lock);
+	if (!uci_dev->enabled) {
+		mask |= POLLERR;
+	} else if (mhi_get_no_free_descriptors(mhi_dev, DMA_TO_DEVICE) > 0) {
+		MSG_VERB("Client can write to node\n");
+		mask |= POLLOUT | POLLWRNORM;
+	}
+	spin_unlock_bh(&uci_chan->lock);
+
+	MSG_LOG("Client attempted to poll, returning mask 0x%x\n", mask);
+
+	return mask;
+}
+
+/**
+ * @brief write to the char device
+ *
+ * @param[in ]            file				struct file
+ * @param[in ]            buf				buffer
+ * @param[in ]            count				buffer size
+ * @param[in ]            offp				offset
+ */
+static ssize_t mhi_uci_write_single(struct file *file,
+							 const char __user *buf,
+							 size_t count,
+							 loff_t *offp)
+{
+	struct uci_dev *uci_dev = file->private_data;
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	struct uci_chan *uci_chan = &uci_dev->ul_chan;
+	size_t bytes_xfered = 0;
+	int ret, nr_avail;
+
+	if (!buf || !count)
+		return -EINVAL;
+
+	/* confirm channel is active */
+	spin_lock_bh(&uci_chan->lock);
+	if (!uci_dev->enabled) {
+		spin_unlock_bh(&uci_chan->lock);
+		return -ERESTARTSYS;
+	}
+
+	MSG_LOG("Enter: to xfer:%lu bytes\n", (unsigned long)count);
+	MSG_VERB("Enter: to xfer:%lu bytes\n", count);
+
+	while (count) {
+		size_t xfer_size;
+		void *kbuf;
+		enum MHI_FLAGS flags;
+
+		spin_unlock_bh(&uci_chan->lock);
+
+		/* wait for free descriptors */
+		ret = wait_event_interruptible(uci_chan->wq,
+			(!uci_dev->enabled) ||
+			(nr_avail = mhi_get_no_free_descriptors(mhi_dev,
+							DMA_TO_DEVICE)) > 0);
+
+		if (ret == -ERESTARTSYS || !uci_dev->enabled) {
+			MSG_LOG("Exit signal caught for node or not enabled\n");
+			return -ERESTARTSYS;
+		}
+
+		xfer_size = min_t(size_t, count, uci_dev->mtu);
+	    MSG_LOG("xfer_size:%lu bytes\n", (unsigned long)xfer_size);
+		kbuf = kmalloc(xfer_size, GFP_KERNEL);
+		if (!kbuf) {
+			MSG_ERR("Failed to allocate memory %lu\n", (unsigned long)xfer_size);
+			return -ENOMEM;
+		}
+
+		ret = copy_from_user(kbuf, buf, xfer_size);
+		if (unlikely(ret)) {
+			kfree(kbuf);
+			return ret;
+		}
+
+		spin_lock_bh(&uci_chan->lock);
+
+		/* if ring is full after this force EOT */
+		if (nr_avail > 1 && (count - xfer_size))
+			flags = MHI_CHAIN;
+		else
+			flags = MHI_EOT;
+
+		if (uci_dev->enabled) {
+#if 0
+	        MSG_LOG("write buffer\n");
+	        print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+		        kbuf, xfer_size, true);
+#endif
+			ret = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, kbuf,
+						 xfer_size, flags);
+	        MSG_LOG("mhi_queue_transfer flags %d ret 0x%x\n", flags, ret);
+        }                         
+		else
+			ret = -ERESTARTSYS;
+
+		if (ret) {
+			kfree(kbuf);
+			goto sys_interrupt;
+		}
+
+		bytes_xfered += xfer_size;
+		count -= xfer_size;
+		buf += xfer_size;
+	}
+
+	spin_unlock_bh(&uci_chan->lock);
+	MSG_LOG("Exit: Number of bytes xferred:%lu\n", (unsigned long)bytes_xfered);
+	MSG_VERB("Exit: Number of bytes xferred:%lu\n", bytes_xfered);
+
+	return bytes_xfered;
+
+sys_interrupt:
+	spin_unlock_bh(&uci_chan->lock);
+
+	return ret;
+}
+
+/**
+ * @brief write to the char device
+ *
+ * @param[in ]            file				struct file
+ * @param[in ]            buf				buffer
+ * @param[in ]            count				buffer size
+ * @param[in ]            offp				offset
+ */
+static ssize_t mhi_uci_write(struct file *file,
+							 const char __user *buf,
+							 size_t count,
+							 loff_t *offp)
+{
+	int ret;
+	
+#ifdef ADB_SUPPORT	
+
+	struct uci_dev *uci_dev = file->private_data;
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	int ret1;
+
+	/* for ADB UL channle, need to send ADB header and payload separately */
+	if ((mhi_dev->ul_chan_id == MHI_CLIENT_ADB_OUT) && (count > ADB_HDR_LEN)) {
+		ret = mhi_uci_write_single(file, buf, ADB_HDR_LEN, offp);
+		if (ret < 0)
+			return ret;
+
+		ret1 = mhi_uci_write_single(file, &buf[ADB_HDR_LEN], count - ADB_HDR_LEN, offp);
+		if (ret1 < 0)
+			return ret1;
+
+		return (ret + ret1);
+	}		
+#endif
+
+	ret = mhi_uci_write_single(file, buf, count, offp);
+
+	return ret;
+}
+
+/**
+ * @brief read from the char device
+ *
+ * @param[in ]            file				struct file
+ * @param[in ]            buf				buffer
+ * @param[in ]            count				buffer size
+ * @param[in ]            ppos				offset
+ */
+static ssize_t mhi_uci_read(struct file *file,
+			    char __user *buf,
+			    size_t count,
+			    loff_t *ppos)
+{
+	struct uci_dev *uci_dev = file->private_data;
+	struct mhi_device *mhi_dev = uci_dev->mhi_dev;
+	struct uci_chan *uci_chan = &uci_dev->dl_chan;
+	struct uci_buf *uci_buf;
+	char *ptr;
+	size_t to_copy;
+	int ret = 0;
+
+	if (!buf)
+		return -EINVAL;
+
+	MSG_VERB("Client provided buf len:%lu\n", count);
+
+	/* confirm channel is active */
+	spin_lock_bh(&uci_chan->lock);
+	if (!uci_dev->enabled) {
+		spin_unlock_bh(&uci_chan->lock);
+		return -ERESTARTSYS;
+	}
+
+	/* No data available to read, wait */
+	if (!uci_chan->cur_buf && list_empty(&uci_chan->pending)) {
+		MSG_VERB("No data available to read waiting\n");
+
+		spin_unlock_bh(&uci_chan->lock);
+		ret = wait_event_interruptible(uci_chan->wq,
+				(!uci_dev->enabled ||
+				 !list_empty(&uci_chan->pending)));
+		if (ret == -ERESTARTSYS) {
+			MSG_LOG("Exit signal caught for node\n");
+			return -ERESTARTSYS;
+		}
+
+		spin_lock_bh(&uci_chan->lock);
+		if (!uci_dev->enabled) {
+			MSG_LOG("node is disabled\n");
+			ret = -ERESTARTSYS;
+			goto read_error;
+		}
+	}
+
+	/* new read, get the next descriptor from the list */
+	if (!uci_chan->cur_buf) {
+		uci_buf = list_first_entry_or_null(&uci_chan->pending,
+						   struct uci_buf, node);
+		if (unlikely(!uci_buf)) {
+			ret = -EIO;
+			goto read_error;
+		}
+
+		list_del(&uci_buf->node);
+		uci_chan->cur_buf = uci_buf;
+		uci_chan->rx_size = uci_buf->len;
+		uci_buf->sent = false;
+		MSG_VERB("Got pkt of size:%zu\n", uci_chan->rx_size);
+	}
+
+	uci_buf = uci_chan->cur_buf;
+	spin_unlock_bh(&uci_chan->lock);
+
+	mutex_lock(&uci_chan->mutex_read);
+
+	/* Copy the buffer to user space */
+	to_copy = min_t(size_t, count, uci_chan->rx_size);
+	ptr = uci_buf->data + (uci_buf->len - uci_chan->rx_size);
+	ret = copy_to_user(buf, ptr, to_copy);
+	if (ret) {
+		mutex_unlock(&uci_chan->mutex_read);
+		return ret;
+	}
+
+	MSG_VERB("Copied %lu of %lu bytes\n", to_copy, uci_chan->rx_size);
+	uci_chan->rx_size -= to_copy;
+
+	mutex_unlock(&uci_chan->mutex_read);
+
+	/* we finished with this buffer, queue it back to hardware */
+	if (!uci_chan->rx_size) {
+		spin_lock_bh(&uci_chan->lock);
+		uci_chan->cur_buf = NULL;
+
+		if (!uci_buf->sent) { 
+
+			uci_buf->sent = true;
+			
+			if (uci_dev->enabled) {
+
+				ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE,
+					uci_buf->data, uci_dev->mtu,
+					MHI_EOT);
+
+				
+				MSG_LOG("buf 0x%px ret 0x%x\n", uci_buf->data, ret);
+			}
+			else {
+				ret = -ERESTARTSYS;
+				MSG_LOG("device disabled! ret 0x%x\n", ret);
+			}
+
+			if (ret) {
+				MSG_ERR("Failed to recycle element\n");
+				kfree(uci_buf->data);
+				goto read_error;
+			}
+		}
+
+		spin_unlock_bh(&uci_chan->lock);
+	}
+
+#if 0
+    MSG_LOG("read buffer\n");
+	print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+	    ptr, to_copy, true);
+#endif
+
+	MSG_VERB("Returning %lu bytes\n", to_copy);
+
+	return to_copy;
+
+read_error:
+	spin_unlock_bh(&uci_chan->lock);
+
+	return ret;
+}
+
+/**
+ * @brief open the char device
+ *
+ * @param[in ]            inode				struct inode
+ * @param[in ]            file				struct file
+ */
+static int mhi_uci_open(struct inode *inode, struct file *filp)
+{
+	struct uci_dev *uci_dev = NULL, *tmp_dev;
+	int ret = -EIO;
+	struct uci_buf *buf_itr, *tmp;
+	struct uci_chan *dl_chan;
+
+	mutex_lock(&mhi_uci_drv.lock);
+	list_for_each_entry(tmp_dev, &mhi_uci_drv.head, node) {
+		if (tmp_dev->devt == inode->i_rdev) {
+			uci_dev = tmp_dev;
+		    MSG_LOG("found device!\n");
+			break;
+		}
+	}
+
+	/* could not find a minor node */
+	if (!uci_dev) {
+		MSG_ERR("cannot find device!\n");
+		goto error_exit;
+    }
+
+	mutex_lock(&uci_dev->mutex);
+	if (!uci_dev->enabled) {
+		MSG_ERR("Node exist, but not in active state!\n");
+		goto error_open_chan;
+	}
+
+	uci_dev->ref_count++;
+
+	MSG_LOG("Node open, ref counts %u\n", uci_dev->ref_count);
+
+	if (uci_dev->ref_count == 1) {
+		MSG_LOG("Starting channel\n");
+		ret = mhi_prepare_for_transfer(uci_dev->mhi_dev);
+		if (ret) {
+			MSG_ERR("Error starting transfer channels\n");
+			uci_dev->ref_count--;
+			goto error_open_chan;
+		}
+
+		MSG_LOG("mhi_queue_inbound\n");
+		ret = mhi_queue_inbound(uci_dev);
+		if (ret)
+			goto error_rx_queue;
+	}
+
+	filp->private_data = uci_dev;
+	mutex_unlock(&uci_dev->mutex);
+	mutex_unlock(&mhi_uci_drv.lock);
+
+	return 0;
+
+ error_rx_queue:
+	dl_chan = &uci_dev->dl_chan;
+	mhi_unprepare_from_transfer(uci_dev->mhi_dev);
+	list_for_each_entry_safe(buf_itr, tmp, &dl_chan->pending, node) {
+		list_del(&buf_itr->node);
+		kfree(buf_itr->data);
+	}
+
+ error_open_chan:
+	mutex_unlock(&uci_dev->mutex);
+
+error_exit:
+	mutex_unlock(&mhi_uci_drv.lock);
+
+	return ret;
+}
+
+static const struct file_operations mhidev_fops = {
+	.owner = THIS_MODULE,
+	.open = mhi_uci_open,
+	.release = mhi_uci_release,
+	.read = mhi_uci_read,
+	.write = mhi_uci_write,
+	.poll = mhi_uci_poll,
+	.unlocked_ioctl = mhi_uci_ioctl,
+};
+
+/**
+ * @brief remove the char device
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ */
+static void mhi_uci_remove(struct mhi_device *mhi_dev)
+{
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+
+	MSG_DEFAULT("Enter\n");
+
+	mutex_lock(&mhi_uci_drv.lock);
+	mutex_lock(&uci_dev->mutex);
+
+	/* disable the node */
+	spin_lock_irq(&uci_dev->dl_chan.lock);
+	spin_lock_irq(&uci_dev->ul_chan.lock);
+	uci_dev->enabled = false;
+	spin_unlock_irq(&uci_dev->ul_chan.lock);
+	spin_unlock_irq(&uci_dev->dl_chan.lock);
+	wake_up(&uci_dev->dl_chan.wq);
+	wake_up(&uci_dev->ul_chan.wq);
+
+	/* delete the node to prevent new opens */
+	device_destroy(mhi_uci_drv.class, uci_dev->devt);
+	uci_dev->dev = NULL;
+	list_del(&uci_dev->node);
+
+	/* safe to free memory only if all file nodes are closed */
+	MSG_LOG("ref_count %d\n", uci_dev->ref_count);
+	if (!uci_dev->ref_count) {
+		mutex_unlock(&uci_dev->mutex);
+		mutex_destroy(&uci_dev->mutex);
+		clear_bit(MINOR(uci_dev->devt), uci_minors);
+		kfree(uci_dev);
+		mutex_unlock(&mhi_uci_drv.lock);
+		return;
+	}
+
+	mutex_unlock(&uci_dev->mutex);
+	mutex_unlock(&mhi_uci_drv.lock);
+
+	MSG_DEFAULT("Exit\n");
+}
+
+/**
+ * @brief probe the char device
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ * @param[in ]            id				struct mhi_device_id
+ */
+static int mhi_uci_probe(struct mhi_device *mhi_dev,
+			 const struct mhi_device_id *id)
+{
+	struct uci_dev *uci_dev;
+	int minor;
+	char node_name[32];
+	int dir;
+
+    MSG_DEFAULT("enter\n");
+
+	uci_dev = kzalloc(sizeof(*uci_dev), GFP_KERNEL);
+	if (!uci_dev)
+		return -ENOMEM;
+
+	mutex_init(&uci_dev->mutex);
+	uci_dev->mhi_dev = mhi_dev;
+
+	minor = find_first_zero_bit(uci_minors, MAX_UCI_DEVICES);
+	if (minor >= MAX_UCI_DEVICES) {
+		kfree(uci_dev);
+		return -ENOSPC;
+	}
+
+	mutex_lock(&uci_dev->mutex);
+	mutex_lock(&mhi_uci_drv.lock);
+
+	uci_dev->devt = MKDEV(mhi_uci_drv.major, minor);
+	uci_dev->dev = device_create(mhi_uci_drv.class, &mhi_dev->dev,
+				     uci_dev->devt, uci_dev,
+				     DEVICE_NAME "_%04x_%02u.%02u.%02u%s%d",
+				     mhi_dev->dev_id, mhi_dev->domain,
+				     mhi_dev->bus, mhi_dev->slot, "_pipe_",
+				     mhi_dev->ul_chan_id);
+	set_bit(minor, uci_minors);
+
+	/* create debugging buffer */
+	snprintf(node_name, sizeof(node_name), "mhi_uci_%04x_%02u.%02u.%02u_%d",
+		 mhi_dev->dev_id, mhi_dev->domain, mhi_dev->bus, mhi_dev->slot,
+		 mhi_dev->ul_chan_id);
+	uci_dev->ipc_log = ipc_log_context_create(MHI_UCI_IPC_LOG_PAGES,
+						  node_name, 0);
+
+	for (dir = 0; dir < 2; dir++) {
+		struct uci_chan *uci_chan = (dir) ?
+			&uci_dev->ul_chan : &uci_dev->dl_chan;
+		spin_lock_init(&uci_chan->lock);
+		init_waitqueue_head(&uci_chan->wq);
+		INIT_LIST_HEAD(&uci_chan->pending);
+		mutex_init(&uci_chan->mutex_read);
+	};
+
+	uci_dev->mtu = min_t(size_t, id->driver_data, mhi_dev->mtu);
+	mhi_device_set_devdata(mhi_dev, uci_dev);
+	uci_dev->enabled = true;
+
+	list_add(&uci_dev->node, &mhi_uci_drv.head);
+	mutex_unlock(&mhi_uci_drv.lock);
+	mutex_unlock(&uci_dev->mutex);
+
+	MSG_DEFAULT("channel:%s successfully probed\n", mhi_dev->chan_name);
+
+	return 0;
+};
+
+/**
+ * @brief host to device transfer callback
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ * @param[in ]            mhi_result		struct mhi_result
+ */
+static void mhi_ul_xfer_cb(struct mhi_device *mhi_dev,
+			   struct mhi_result *mhi_result)
+{
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+	struct uci_chan *uci_chan = &uci_dev->ul_chan;
+
+	MSG_VERB("status:%d xfer_len:%zu\n", mhi_result->transaction_status,
+		 mhi_result->bytes_xferd);
+
+	kfree(mhi_result->buf_addr);
+	if (!mhi_result->transaction_status)
+		wake_up(&uci_chan->wq);
+}
+
+/**
+ * @brief device to host transfer callback
+ *
+ * @param[in ]            mhi_dev			struct mhi_device
+ * @param[in ]            mhi_result		struct mhi_result
+ */
+static void mhi_dl_xfer_cb(struct mhi_device *mhi_dev,
+			   struct mhi_result *mhi_result)
+{
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+	struct uci_chan *uci_chan = &uci_dev->dl_chan;
+	unsigned long flags;
+	struct uci_buf *buf;
+	bool ignore;
+	int ret = 0;
+
+	MSG_VERB("status:%d receive_len:%zu\n", mhi_result->transaction_status,
+		 mhi_result->bytes_xferd);
+
+	if (mhi_result->transaction_status == -ENOTCONN) {
+		kfree(mhi_result->buf_addr);
+		return;
+	}
+
+	MSG_LOG("chan %d\n", mhi_dev->dl_chan->chan);
+
+	buf = mhi_result->buf_addr + uci_dev->mtu;
+	buf->data = mhi_result->buf_addr;
+	buf->len = mhi_result->bytes_xferd;
+
+	ignore = mbim_rx_cid(mhi_dev, buf->data, buf->len);
+
+	if (ignore) {
+		spin_lock_irqsave(&uci_chan->lock, flags);
+
+		if (uci_dev->enabled) {
+
+			ret = mhi_queue_transfer(mhi_dev, DMA_FROM_DEVICE,
+				buf->data, uci_dev->mtu,
+				MHI_EOT);
+
+				
+			MSG_LOG("buf 0x%px ret 0x%x\n", buf->data, ret);
+
+		} else {
+			MSG_LOG("device disabled!\n");
+			ret = -ERESTARTSYS;
+		}
+
+		if (ret) {
+			MSG_ERR("Failed to recycle element\n");
+			kfree(buf->data);
+		}	
+		
+		spin_unlock_irqrestore(&uci_chan->lock, flags);
+
+		return;
+	}
+
+	spin_lock_irqsave(&uci_chan->lock, flags);
+	list_add_tail(&buf->node, &uci_chan->pending);
+	spin_unlock_irqrestore(&uci_chan->lock, flags);
+
+	if (mhi_dev->dev.power.wakeup)
+		__pm_wakeup_event(mhi_dev->dev.power.wakeup, 0);
+
+	wake_up(&uci_chan->wq);
+}
+
+/* .driver_data stores max mtu */
+static const struct mhi_device_id mhi_uci_match_table[] = {
+	{.chan = "LOOPBACK", .driver_data = 0x1000},
+	{.chan = "EFS", .driver_data = 0x1000},
+	{.chan = "MBIM", .driver_data = 0x1000},
+	{.chan = "QMI0", .driver_data = 0x1000},
+	{.chan = "QMI1", .driver_data = 0x1000},
+#ifdef ADB_SUPPORT
+	{.chan = "ADB", .driver_data = 0x1000},
+#endif	
+	{.chan = "TF", .driver_data = 0x1000},
+	{},
+};
+MODULE_DEVICE_TABLE(mhi, mhi_uci_match_table);
+
+static struct mhi_driver mhi_uci_driver = {
+	.id_table = mhi_uci_match_table,
+	.remove = mhi_uci_remove,
+	.probe = mhi_uci_probe,
+	.ul_xfer_cb = mhi_ul_xfer_cb,
+	.dl_xfer_cb = mhi_dl_xfer_cb,
+	.driver = {
+		.name = MHI_UCI_DRIVER_NAME,
+		.owner = THIS_MODULE,
+	},
+};
+
+/**
+ * @brief module init
+ */
+static int mhi_uci_init(void)
+{
+	int ret;
+
+    MSG_DEFAULT("enter\n");
+
+	ret = register_chrdev(0, MHI_UCI_DRIVER_NAME, &mhidev_fops);
+	if (ret < 0)
+		return ret;
+
+	mhi_uci_drv.major = ret;
+	mhi_uci_drv.class = class_create(THIS_MODULE, MHI_UCI_DRIVER_NAME);
+	if (IS_ERR(mhi_uci_drv.class)) {
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	mutex_init(&mhi_uci_drv.lock);
+	INIT_LIST_HEAD(&mhi_uci_drv.head);
+
+	ret = mhi_driver_register(&mhi_uci_driver);
+	if (ret)
+		class_destroy(mhi_uci_drv.class);
+
+	return ret;
+
+exit:
+	unregister_chrdev(mhi_uci_drv.major, MHI_UCI_DRIVER_NAME);
+
+	return ret;
+}
+
+/**
+ * @brief module exit
+ */
+static void __exit mhi_uci_exit(void)
+{
+    MSG_DEFAULT("enter\n");
+
+	unregister_chrdev(mhi_uci_drv.major, MHI_UCI_DRIVER_NAME);
+
+    mhi_driver_unregister(&mhi_uci_driver);
+
+	class_destroy(mhi_uci_drv.class);
+}
+
+module_init(mhi_uci_init);
+module_exit(mhi_uci_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("MHI_UCI");
+MODULE_DESCRIPTION("MHI UCI Driver");
+MODULE_VERSION("1.9.2101.1");
\ No newline at end of file
diff --git a/drivers/staging/em9190/devices/mhi_uci.h b/drivers/staging/em9190/devices/mhi_uci.h
new file mode 100644
index 000000000000..07b8f451996e
--- /dev/null
+++ b/drivers/staging/em9190/devices/mhi_uci.h
@@ -0,0 +1,96 @@
+#ifndef _MHI_UCI_H
+#define _MHI_UCI_H
+
+struct uci_chan {
+	wait_queue_head_t wq;
+	spinlock_t lock;
+	struct list_head pending; /* user space waiting to read */
+	struct uci_buf *cur_buf; /* current buffer user space reading */
+	size_t rx_size;
+	struct mutex mutex_read;
+};
+
+struct uci_buf {
+	void *data;
+	size_t len;
+	struct list_head node;
+	bool sent;
+};
+
+struct uci_dev {
+	struct list_head node;
+	dev_t devt;
+	struct device *dev;
+	struct mhi_device *mhi_dev;
+	const char *chan;
+	struct mutex mutex; /* sync open and close */
+	struct uci_chan ul_chan;
+	struct uci_chan dl_chan;
+	size_t mtu;
+	int ref_count;
+	bool enabled;
+	void *ipc_log;
+	u32 set_data_fmt_tid;
+};
+
+struct mhi_uci_drv {
+	struct list_head head;
+	struct mutex lock;
+	struct class *class;
+	int major;
+	dev_t dev_t;
+};
+
+#define DEVICE_NAME "mhi"
+#define MHI_UCI_DRIVER_NAME "mhi_uci"
+
+extern enum MHI_DEBUG_LEVEL msg_lvl;
+
+#ifdef CONFIG_MHI_DEBUG
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_VERBOSE)
+#define MHI_UCI_IPC_LOG_PAGES (25)
+
+#else
+
+#define IPC_LOG_LVL (MHI_MSG_LVL_ERROR)
+#define MHI_UCI_IPC_LOG_PAGES (1)
+
+#endif
+
+extern bool debug;
+extern int debug_level;
+
+#define MSG_VERB(fmt, ...)
+
+#define MSG_LOG(fmt, ...) do { \
+		if (debug && debug_level <= MHI_MSG_LVL_INFO) \
+			pr_err("[I][%s] " fmt, __func__, ##__VA_ARGS__); \
+		if (uci_dev->ipc_log && (IPC_LOG_LVL <= MHI_MSG_LVL_INFO)) \
+			ipc_log_string(uci_dev->ipc_log, "[I][%s] " fmt, \
+				       __func__, ##__VA_ARGS__); \
+	} while (0)
+
+#define MSG_ERR(fmt, ...) do { \
+		if (debug && debug_level <= MHI_MSG_LVL_ERROR) \
+			pr_err("[E][%s] " fmt, __func__, ##__VA_ARGS__); \
+		if (uci_dev->ipc_log && (IPC_LOG_LVL <= MHI_MSG_LVL_ERROR)) \
+			ipc_log_string(uci_dev->ipc_log, "[E][%s] " fmt, \
+				       __func__, ##__VA_ARGS__); \
+	} while (0)
+
+#define DEFAULT 1
+
+#if DEFAULT
+#define MSG_DEFAULT(fmt, ...) do { \
+		pr_info("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+#else
+#define MSG_DEFAULT(fmt, ...)
+#endif
+
+#define MAX_UCI_DEVICES (64)
+
+#define ADB_HDR_LEN			24		/* ADB message header length */
+
+#endif
diff --git a/drivers/staging/em9190/devices/qmap.c b/drivers/staging/em9190/devices/qmap.c
new file mode 100644
index 000000000000..906c5f93fb4d
--- /dev/null
+++ b/drivers/staging/em9190/devices/qmap.c
@@ -0,0 +1,350 @@
+#include <linux/delay.h>
+#include <linux/kthread.h>
+
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/ctype.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/workqueue.h>
+#include <linux/mii.h>
+#include <linux/crc32.h>
+#include <linux/usb.h>
+#include <linux/hrtimer.h>
+#include <linux/atomic.h>
+#include <linux/usb/usbnet.h>
+#include <linux/usb/cdc.h>
+#include <linux/usb/cdc_ncm.h>
+#include <net/ipv6.h>
+#include <net/addrconf.h>
+#include <linux/hrtimer.h>
+
+#include "../inc/ipc_logging.h"
+#include "../inc/mhi.h"
+#include "../core/mhi_internal.h"
+#include "mhi_uci.h"
+#include "qmap.h"
+
+/* This is in preferred order from top to bottom */
+QMAP_DATA_FMT data_fmts_supported[] = 
+{
+    QMAPV1,
+    QMAPV4,
+    QMAPV5
+};
+
+const u8 mbim_extensibility_guid[16] = {
+	0x2d, 0x0c, 0x12, 0xc9, 0x0e, 0x6a, 0x49, 0x5a, 0x91, 0x5c, 0x8d, 0x17, 0x4f, 0xe5, 0xd6, 0x3c
+};
+
+const u8 mbim_extensibility_guid_new[16] = {
+	0xcf, 0xd4, 0x97, 0xea, 0xf0, 0xae, 0x4f, 0x71, 0x9a, 0xca, 0x14, 0x26, 0x6e, 0xae, 0xf1, 0x98
+};
+
+/**
+ * @brief Utility to find the max supported QMAP Aggregation formats
+ */
+u32 qmap_get_max_supproted_data_fmts(void)
+{
+    u32 num_supported_aggr = sizeof(data_fmts_supported) / sizeof(data_fmts_supported[0]);
+    return num_supported_aggr;
+}
+
+/**
+ * @brief Qmap API to get the Information buffer size needed for DataFmt list
+ */
+u32 qmap_get_buf_size_reqd_for_data_fmt(struct mhi_device *mhi_dev)
+{
+    u32 num_supported_aggr = qmap_get_max_supproted_data_fmts();
+    u32 buf_size = (num_supported_aggr * sizeof(QMAP_DATA_FMT_PARAMS)) +
+           sizeof(QMAP_DATA_FMT_REQ_OFFSET_LEN)* num_supported_aggr +
+           sizeof(QMAP_DATA_FMT_REQ);
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+
+	MSG_LOG("bufSize = %d\n", buf_size);
+
+	return buf_size;
+}
+
+/**
+ * @brief Qmap API to get the list of DataFormats supported
+ *
+ * @param[out]  buf       Buffer address of the elements stored consecutively
+ */
+void qmap_get_data_format_list(u8 *buf)
+{
+    u8 *info_buf = buf;
+    u32 num_supported_aggr = qmap_get_max_supproted_data_fmts();
+    PQMAP_DATA_FMT_REQ_OFFSET_LEN offset_len_pair;
+    u32 data_buf_offset;
+    PQMAP_DATA_FMT_PARAMS data_fmt_params = NULL;
+    u32 idx;
+
+    /* Preference order of QMAP aggregation list */
+    /*u32 buf_size = (num_supported_aggr * sizeof(QMAP_DATA_FMT_PARAMS)) +
+        sizeof(QMAP_DATA_FMT_REQ_OFFSET_LEN) * num_supported_aggr +
+        sizeof(QMAP_DATA_FMT_REQ); */
+#define RAW_IP_MODE (1)
+
+    /* Begin Info buffer update */
+    PQMAP_DATA_FMT_REQ req = (PQMAP_DATA_FMT_REQ)info_buf;
+
+	req->version = 1;     /* Support version 1 */
+    req->element_count = num_supported_aggr;
+
+    /* ProviderReflist */
+    offset_len_pair = (PQMAP_DATA_FMT_REQ_OFFSET_LEN)(info_buf + sizeof(QMAP_DATA_FMT_REQ));
+    data_buf_offset = (sizeof(QMAP_DATA_FMT_REQ_OFFSET_LEN) * num_supported_aggr) + sizeof(QMAP_DATA_FMT_REQ);
+
+    for (idx = 0; idx < num_supported_aggr; idx++)
+    {
+		offset_len_pair->offset = data_buf_offset;
+		offset_len_pair->length = sizeof(QMAP_DATA_FMT_PARAMS);
+
+        /* Fill DataFmt */
+		data_fmt_params = (PQMAP_DATA_FMT_PARAMS)(info_buf + data_buf_offset);
+		data_fmt_params->dl_data_agg_max_datagrams = MAX_DL_PKT_AGGR;
+		data_fmt_params->dl_data_agg_max_size = QMAP_RX_BUFFER_SIZE;
+		data_fmt_params->ul_data_agg_max_datagrams = MAX_UL_PKT_AGGR;
+		data_fmt_params->ul_data_agg_max_size = QMAP_TX_BUFFER_SIZE;
+		//data_fmt_params->ul_data_agg_max_datagrams = ;
+		//data_fmt_params->ul_data_agg_max_size = 0;
+		data_fmt_params->link_prot = RAW_IP_MODE;
+		data_fmt_params->tcp_udp_coalescing = 0;
+
+		data_fmt_params->ul_data_agg_protocol = data_fmts_supported[idx];
+		data_fmt_params->dl_data_agg_protocol = data_fmts_supported[idx];
+
+        /* Update the data buffer offset & OL pair provider offset */
+        data_buf_offset += sizeof(QMAP_DATA_FMT_PARAMS);
+		offset_len_pair = (PQMAP_DATA_FMT_REQ_OFFSET_LEN)((u8 *)offset_len_pair + sizeof(QMAP_DATA_FMT_REQ_OFFSET_LEN));
+    }
+
+    return;
+}
+
+/**
+ * @brief handle MBIM incoming message
+ *
+ * @param[in]  device_info       device information
+ * @param[in]  buf               MBIM messsage buffer
+ * @param[in]  len               MBIM messsage length
+ */
+bool
+mbim_rx_cid(struct mhi_device *mhi_dev, u8 *buf, unsigned long len)
+{
+	struct mbim_message_header *header;
+	struct mbim_open_done_message *done_msg;
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+
+	if (len < sizeof(struct mbim_message_header)) {
+		MSG_LOG("invalid mbim message len %ld\n", len);
+		return false;
+	}
+
+	header = (struct mbim_message_header *)buf;
+
+	switch (header->message_type) {
+	case MBIM_OPEN_DONE:
+		MSG_LOG("MBIM_OPEN_DONE message received!\n");
+		if (len != sizeof(struct mbim_open_done_message)) {
+			MSG_LOG("invalid mbim open done message len %ld\n", len);
+			return false;
+		}
+
+		done_msg = (struct mbim_open_done_message *)buf;
+
+		if (done_msg->status == MBIM_STATUS_SUCCESS) {
+			MSG_LOG("set the data format as QMAP!\n");
+			mbim_set_data_format(mhi_dev, header->transaction_id, false);
+		}
+
+		break;
+
+	case MBIM_COMMAND_DONE:
+	{
+		struct mbim_command_done_msg *cmd_msg_done = NULL;
+
+		MSG_LOG("MBIM_COMMAND_DONE message received!\n");
+		if (NULL == buf || len < sizeof(struct mbim_command_done_msg))
+		{
+			MSG_ERR("Invalid argument buf %p len %lu\n", buf, len);
+			return false;
+		}
+
+		MSG_LOG("dump MBIM_COMMAND_DONE msg\n");
+#if 0
+        print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+            buf, len, true);
+#endif
+
+		cmd_msg_done = (struct mbim_command_done_msg *)buf;
+
+		if ((memcmp(cmd_msg_done->device_service_id, mbim_extensibility_guid, sizeof(cmd_msg_done->device_service_id)) == 0) &&
+			(MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT == cmd_msg_done->cid) &&
+            (uci_dev->set_data_fmt_tid == cmd_msg_done->message_header.transaction_id)) {
+
+			MSG_LOG("Process MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT response\n");
+
+			data_format_packet_handling(mhi_dev, buf, len, false);
+
+            return true;
+		}
+
+		if ((memcmp(cmd_msg_done->device_service_id, mbim_extensibility_guid_new, sizeof(cmd_msg_done->device_service_id)) == 0) &&
+			(MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT_NEW == cmd_msg_done->cid) &&
+            (uci_dev->set_data_fmt_tid == cmd_msg_done->message_header.transaction_id)) {
+
+			MSG_LOG("Process MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT_NEW response\n");
+
+			data_format_packet_handling(mhi_dev, buf, len, true);
+
+            return true;
+		}
+	}
+	break;
+	}
+
+	return false;
+}
+
+/**
+ * @brief compose and send set data format message to modem
+ *
+ * @param[in]  device_info       device information
+ * @param[in]  transaction_id    MBIM message tid
+ */
+int mbim_set_data_format(struct mhi_device *mhi_dev, u32 transaction_id, bool new)
+{
+	u32 payload = 0;
+	u32 alloc = 0;
+	u8 *buf;
+	unsigned long buf_len;
+	u32 tid = transaction_id;
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+    int ret = -EIO;
+
+	MSG_LOG("MBIM_OPEN_DONE_MSG received, sending MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT to modem\n");
+
+	payload = qmap_get_buf_size_reqd_for_data_fmt(mhi_dev);
+	if (0 == payload) {
+		MSG_ERR("returned 0 bytes transaction ID %u\n",	transaction_id);
+		return -EINVAL;
+	}
+	else {
+		alloc = payload + sizeof(struct mbim_command_msg);
+
+		buf = kzalloc(alloc, GFP_KERNEL);
+
+		if (buf == NULL) {
+			MSG_ERR("mbim_set_data_format failed to allocate %u bytes\n", alloc);
+			return -ENOMEM;
+		}
+		else {
+			qmap_get_data_format_list(buf + sizeof(struct mbim_command_msg));
+
+			if (!new)
+ 				tid = transaction_id ^ 0x80000000;
+				
+			if (tid == 0)
+				tid = 1;
+
+            uci_dev->set_data_fmt_tid = tid;
+
+            add_data_format_header(mhi_dev, buf, alloc, tid, new);
+
+            buf_len = alloc;
+
+            MSG_LOG("dump qmap set data format msg\n");
+#if 0
+            print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+	            buf, buf_len, true);
+#endif
+            ret = mhi_queue_transfer(mhi_dev, DMA_TO_DEVICE, buf,
+                                     buf_len, MHI_EOT);
+
+            MSG_LOG("mhi_queue_transfer flags %d ret 0x%x\n", MHI_EOT, ret);
+        }
+    }
+
+	return 0;
+}
+
+/**
+ * @brief add message header for set data format message
+ *
+ * @param[in]  buf               MBIM messsage buffer
+ * @param[in]  len               MBIM messsage length
+ * @param[in]  transaction_id    MBIM message tid
+ */
+int add_data_format_header(struct mhi_device *mhi_dev, u8 *buf, u32 len, u32 transaction_id, bool new)
+{
+	struct mbim_command_msg *header = NULL;
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+
+	if (NULL == buf || len < sizeof(struct mbim_command_msg))
+	{
+		MSG_ERR("add_data_format_header invalid argument!\n");
+		return -EINVAL;
+	}
+
+	header = (struct mbim_command_msg *)buf;
+
+	header->message_header.message_type = MBIM_COMMAND;
+	header->message_header.transaction_id = transaction_id;
+	header->message_header.message_length = len;
+
+    header->fragment_header.total_fragments = 1;
+    header->fragment_header.current_fragment = 0;
+
+	if (!new) 
+		header->cid = MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT;
+    else
+		header->cid = MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT_NEW;
+
+	header->command_type = MBIM_SET_OPERATION;
+
+	if (!new) 
+		memcpy(&header->device_service_id, mbim_extensibility_guid, sizeof(header->device_service_id));
+	else 
+		memcpy(&header->device_service_id, mbim_extensibility_guid_new, sizeof(header->device_service_id));
+
+	header->information_buffer_length = len - sizeof(struct mbim_command_msg);
+
+	return 0;
+}
+
+/**
+ * @brief hanlder for set data format response
+ *
+ * @param[in]  buf               MBIM messsage buffer
+ * @param[in]  len               MBIM messsage length
+ * @param[in]  transaction_id    MBIM message tid
+ */
+int data_format_packet_handling(struct mhi_device *mhi_dev, u8 *buf, unsigned long len, bool new)
+{
+	struct mbim_command_done_msg *cmd_done = NULL;
+	struct uci_dev *uci_dev = mhi_device_get_devdata(mhi_dev);
+
+	cmd_done = (struct mbim_command_done_msg *)buf;
+
+	if (MBIM_STATUS_SUCCESS == cmd_done->status) {
+
+		MSG_LOG("set data format request success!\n");
+
+		return 0;
+	} else {
+
+		MSG_ERR("set data format request failed 0x%x!\n", cmd_done->status);
+
+        if (!new && MBIM_STATUS_NO_DEVICE_SUPPORT == cmd_done->status) {
+		    MSG_LOG("device does not support, try the new one!\n");
+
+            mbim_set_data_format(mhi_dev, cmd_done->message_header.transaction_id + 1, true);
+        }
+    }
+
+	return -EINVAL;
+}
diff --git a/drivers/staging/em9190/devices/qmap.h b/drivers/staging/em9190/devices/qmap.h
new file mode 100644
index 000000000000..0b2c2189edc0
--- /dev/null
+++ b/drivers/staging/em9190/devices/qmap.h
@@ -0,0 +1,287 @@
+
+#ifndef QMAP_H_
+#define QMAP_H_
+
+
+// ============================================================================
+// Internal Use
+// ============================================================================
+#define QUALCOMM_DEVICE_TYPE_QMAP       (0x40008)
+#define QMAP_FUNCTION_CODE_BASE         (0x800)
+
+#define HW_BUFFER_ALIGNMENT             (0x0000000f)
+#define QMAP_CKSUM_TRAILER              (8)
+#define QMAP_LEN_ALIGN                  (4)
+#define QMAP_CLIENT_MAX                 (17)
+#define MAX_DATA_FRAGMENT_SIZE          (0x600)
+
+#define MAX_OS_NET_PACKETS              (512)
+#define MAX_OS_NET_PACKETS_UL           (MAX_OS_NET_PACKETS)
+#define MAX_OS_NET_PACKETS_DL           (MAX_OS_NET_PACKETS)
+
+#define MHI_RING_LENGTH                 (256)
+#define MHI_UL_RING_LENGTH              (MHI_RING_LENGTH)
+#define MHI_DL_RING_LENGTH              (MHI_RING_LENGTH)
+
+#define MAX_MHI_DESCRIPTORS             (250)
+#define MAX_MHI_UL_DESCRIPTORS          (MAX_MHI_DESCRIPTORS)
+#define MAX_MHI_DL_DESCRIPTORS          (MAX_MHI_DESCRIPTORS)
+
+#define MAX_DL_PKT_AGGR                 (10)
+#define MAX_UL_PKT_AGGR                 (10)
+#define QMAP_TX_BUFFER_SIZE             0x4000	// 16K must be multiplication of page size (MAX_DATA_FRAGMENT_SIZE * MAX_UL_PKT_AGGR)
+#define QMAP_RX_BUFFER_SIZE             0x4000	// 16K must be multiplication of page size (MAX_DATA_FRAGMENT_SIZE * MAX_DL_PKT_AGGR)
+#define MAX_CLIENT_PKT_CONTEXT          (MAX_MHI_DL_DESCRIPTORS * MAX_DL_PKT_AGGR)
+
+#define QUALCOMM_MAC_0                 (0x00)
+#define QUALCOMM_MAC_1                 (0xA0)
+#define QUALCOMM_MAC_2                 (0xC6)
+#define QUALCOMM_MAC_3                 (0x00)
+#define QUALCOMM_MAC_4                 (0x00)
+// 5 - this need either be static or global for multiple adapter support. 
+#define QUALCOMM_MAC_5                 (0x01)
+
+#define QMAPV1_TX_HDR_LEN                  (4)
+#define QMAPV1_RX_HDR_LEN                  (4)
+#define QMAPV5_TX_HDR_LEN                  (8)
+#define QMAPV5_RX_HDR_LEN                  (8)
+#define QMAPV4_TX_HDR_LEN                  (8)
+#define QMAPV4_RX_HDR_LEN                  (4)
+#define QMAPV4_RX_TRL_LEN                  (QMAP_CKSUM_TRAILER)
+#define QMAP_TX_CKSUM_OFFSET               (4)
+
+#define QMAP_V4_FRAG_BITS      (0x02FFF) //includes the MF bit
+#define QMAP_V4_FRAG_MSB       (6)
+#define QMAP_V4_FRAG_LSB       (7)
+#define QMAP_V4_PROTO_BYTE     (9)
+
+#define QMAP_V4_FRAG_PKT(pkt) (0 != (QMAP_V4_FRAG_BITS & (((pkt)[QMAP_V4_FRAG_MSB] << 8) | (pkt)[QMAP_V4_FRAG_LSB])))
+
+#define QMAP_V6_FRAG_EXT_HDR   (44)
+#define QMAP_V6_NEXT_HDR_BYTE  (6)
+
+#define QMAP_V6_FRAG_PKT(pkt) (QMAP_V6_FRAG_EXT_HDR == (pkt)[QMAP_V6_NEXT_HDR_BYTE])
+
+#define QMAP_V4_NON_FRAG_PKT(pkt) (0 == (QMAP_V4_FRAG_BITS & (((pkt)[QMAP_V4_FRAG_MSB] << 8) | (pkt)[QMAP_V4_FRAG_LSB])))
+#define QMAP_V4_TCP_PKT(pkt) (QMAP_IPPROTO_TCP == (pkt)[QMAP_V4_PROTO_BYTE])
+#define QMAP_V4_UDP_PKT(pkt) (QMAP_IPPROTO_UDP == (pkt)[QMAP_V4_PROTO_BYTE])
+#define QMAP_V4_XSUM_OFFLOAD_PKT_TYPE(pkt)  ((QMAP_V4_TCP_PKT(pkt) || QMAP_V4_UDP_PKT(pkt)) && QMAP_V4_NON_FRAG_PKT(pkt))
+
+#define QMAP_V6_TCP_PKT(pkt) (QMAP_IPPROTO_TCP == (pkt)[QMAP_V6_NEXT_HDR_BYTE])
+#define QMAP_V6_UDP_PKT(pkt) (QMAP_IPPROTO_UDP == (pkt)[QMAP_V6_NEXT_HDR_BYTE])
+#define QMAP_V6_NON_FRAG_PKT(pkt) (QMAP_V6_FRAG_EXT_HDR != (pkt)[QMAP_V6_NEXT_HDR_BYTE])
+#define QMAP_V6_XSUM_OFFLOAD_PKT_TYPE(pkt)  ((QMAP_V6_TCP_PKT(pkt) || QMAP_V6_UDP_PKT(pkt)) && QMAP_V6_NON_FRAG_PKT(pkt))
+
+#define MAX_RX_CLIENT_DESC             (512)
+#define DEFAULT_UL_INDEX               (MHI_UL_RING_LENGTH - 1) /* To start at index 0 when it increments */
+#define DEFAULT_DL_INDEX               (MHI_DL_RING_LENGTH - 1) /* To start at index 0 when it increments */
+
+#define MAX_RSS_INDIRECTION_ENTRIES    (128)
+#define MAX_QUEUES_PER_DIR             (4)
+#define MAX_DIR                        (2) /* UL and DL */
+
+#define PEND_NOTIFY_ERR_MARGIN  (0xFFFFFFFD)
+
+#define QMAP_IPPROTO_TCP  (6)
+#define QMAP_IPPROTO_UDP  (17)
+#define QMAP_IPPROTO_IPV6_HDR_LEN (40)
+#define QMAP_IPPROTO_IPV4 (4)
+#define QMAP_IPPROTO_IPV6 (6)
+
+#define QMAP_MIN_IPV4_HDR_LEN            (20)
+#define QMAP_MIN_IPV6_HDR_LEN            (40)
+
+#define TCP_CKSUM_OFFSET 16
+#define UDP_CKSUM_OFFSET 6
+#define NUMBER_OF_32BIT_WORDS 4
+
+#define IP_VER(packet)                  ((packet[0] & 0xF0) >> 4)
+#define IPV4_HDR_LEN(packet)            ((packet[0] & 0x0F) * 4);
+
+/*! @brief QMAP header struct definition after ntohl conversion */
+typedef struct _QMAP_HEADER
+{
+    u8       packet_info;
+    u8       mux_id; // this should be same as channel
+    u16      length;
+    /*
+    Bit 0       =   C/D bit
+    1 = QMAP control command
+    0 = Data packet
+
+    Bit 1       =   Reserved (Set to 0)
+    Must be ignored by the receiver
+
+
+    Bit 2-7     =   PAD (6 bits; Indicates number of bytes padded to achieve a minimum of 4 byte alignment)
+    Padded bytes may or may not be set to 0. The receiver must ignore these bytes regardless.
+
+    Bit 8-15    =   MUX_ID - 8 bits; Indicates MUX channel ID
+    If MUX is not negotiated, this field is set to 0 and must be ignored by the receiver
+    If MUX is negotiated, MUX_ID can be in the range of 1 to 0xF0. MUX_IDs 0 and 0xFA - 0xFF are reserved.
+
+    Bit 16-31   =   PAYLOAD_LEN_WITH_PADDING: 16 bits; Total payload length in bytes including padding
+    Doesnt include QMAP header
+    The receiver must ignore a packet with this field set to 0 as hardware may use such special QMAP packet to indicate end of aggregation.
+    QMAP packet will not carry any payload in this case
+    */
+} QMAP_HEADER, *PQMAP_HEADER;
+
+/* Data format negotiation parameters */
+#define AGGR_NOT_SUPPORTED      (0)
+
+/* This should always be defined in order of least preferred to most preferred Aggr type */
+typedef enum
+{
+    QMAPV1 = 1,
+    QMAPV2 = AGGR_NOT_SUPPORTED,
+    QMAPV3 = AGGR_NOT_SUPPORTED,
+    QMAPV4 = 4,
+    QMAPV5 = 5,
+    QMAP_MAX_SUPP_AGGR
+}QMAP_DATA_FMT;
+
+// All structures will be 1-byte aligned.
+#pragma pack(push, 1)
+typedef struct 
+{
+    u32 offset;
+    u32 length;
+} QMAP_DATA_FMT_REQ_OFFSET_LEN, * PQMAP_DATA_FMT_REQ_OFFSET_LEN;
+
+typedef struct
+{
+    u32 version;
+    u32 element_count;
+    /*! @note Followed by element_count instances of QMAP_DATA_FMT_REQ_OFFSET_LEN */
+} QMAP_DATA_FMT_REQ, * PQMAP_DATA_FMT_REQ;
+
+typedef struct 
+{
+    u32 link_prot;
+    u32 dl_data_agg_protocol;
+    u32 dl_data_agg_max_datagrams;
+    u32 dl_data_agg_max_size;
+	u32 ul_data_agg_protocol;
+	u32 ul_data_agg_max_datagrams;
+	u32 ul_data_agg_max_size;
+	u32 tcp_udp_coalescing;
+} QMAP_DATA_FMT_PARAMS, * PQMAP_DATA_FMT_PARAMS;
+#pragma pack(pop)
+
+typedef enum
+{
+	MBIM_INVALID_MSG = 0,
+	MBIM_OPEN = 0x1,
+	MBIM_CLOSE = 0x2,
+	MBIM_COMMAND = 0x3,
+	MBIM_HOST_ERROR = 0x4,
+	MBIM_OPEN_DONE = 0x80000001,
+	MBIM_CLOSE_DONE = 0x80000002,
+	MBIM_COMMAND_DONE = 0x80000003,
+	MBIM_FUNCTION_ERROR = 0x80000004,
+	MBIM_INDICATE_STATUS = 0x80000007
+} MBIM_MSG_ID;
+
+typedef enum
+{
+	MBIM_CID_MBIM_EXTENSIBILITY_DIAG_CONFIG = 1,
+	MBIM_CID_MBIM_EXTENSIBILITY_DIAG_DATA = 2,
+	MBIM_CID_MBIM_EXTENSIBILITY_DEVICE_LOG = 3,
+	MBIM_CID_MBIM_EXTENSIBILITY_SIMLOCK = 4,
+	MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT = 5
+} MBIM_SERVICE_MBIM_EXTENSIBILITY_CID_ID;
+
+typedef enum
+{
+	MBIM_CID_MBIM_EXTENSIBILITY_QNET_DATA_FORMAT_NEW = 1
+} MBIM_SERVICE_MBIM_EXTENSIBILITY_CID_ID_NEW;
+
+typedef enum
+{
+	MBIM_QUERY_OPERATION = 0,
+	MBIM_SET_OPERATION = 1
+} MBIM_COMMAND_TYPE;
+
+typedef enum
+{
+	MBIM_STATUS_SUCCESS = 0,
+	MBIM_STATUS_BUSY = 1,
+	MBIM_STATUS_FAILURE = 2,
+	MBIM_STATUS_SIM_NOT_INSERTED = 3,
+	MBIM_STATUS_BAD_SIM = 4,
+	MBIM_STATUS_PIN_REQUIRED = 5,
+	MBIM_STATUS_PIN_DISABLED = 6,
+	MBIM_STATUS_NOT_REGISTERED = 7,
+	MBIM_STATUS_PROVIDERS_NOT_FOUND = 8,
+	MBIM_STATUS_NO_DEVICE_SUPPORT = 9,
+	MBIM_STATUS_PROVIDER_NOT_VISIBLE = 10,
+	MBIM_STATUS_DATA_CLASS_NOT_AVAILABLE = 11,
+	MBIM_STATUS_PACKET_SERVICE_DETACHED = 12,
+	MBIM_STATUS_MAX_ACTIVATED_CONTEXTS = 13,
+	MBIM_STATUS_NOT_INITIALIZED = 14,
+	MBIM_STATUS_VOICE_CALL_IN_PROGRESS = 15,
+	MBIM_STATUS_CONTEXT_NOT_ACTIVATED = 16,
+	MBIM_STATUS_SERVICE_NOT_ACTIVATED = 17,
+	MBIM_STATUS_INVALID_ACCESS_STRING = 18,
+	MBIM_STATUS_INVALID_USER_NAME_PWD = 19,
+	MBIM_STATUS_RADIO_POWER_OFF = 20,
+	MBIM_STATUS_INVALID_PARAMETERS = 21,
+	MBIM_STATUS_READ_FAILURE = 22,
+	MBIM_STATUS_WRITE_FAILURE = 23,
+	MBIM_STATUS_NO_PHONEBOOK = 25,
+	MBIM_STATUS_PARAMETER_TOO_LONG = 26,
+	MBIM_STATUS_STK_BUSY = 27,
+	MBIM_STATUS_OPERATION_NOT_ALLOWED = 28
+} MBIM_ERROR_CODE;
+
+struct mbim_message_header {
+	u32 message_type;
+	u32 message_length;
+	u32 transaction_id;
+} __attribute__((packed));
+
+struct mbim_open_done_message {
+	struct mbim_message_header message_header;
+	u32 status;
+} __attribute__((packed));
+
+struct mbim_fragment_header {
+	u32 total_fragments;
+	u32 current_fragment;
+} __attribute__((packed));
+
+struct mbim_command_msg
+{
+	struct mbim_message_header      message_header;
+	struct mbim_fragment_header     fragment_header;
+	u8                      device_service_id[16];
+	u32                      cid;
+	u32                      command_type;
+	u32                      information_buffer_length;
+	// InformationBuffer follows after this;
+} __attribute__((packed));
+
+struct mbim_command_done_msg
+{
+	struct mbim_message_header      message_header;
+	struct mbim_fragment_header     fragment_header;
+	u8							device_service_id[16];
+	u32							cid;
+	u32							status;
+	u32                      information_buffer_length;
+	// InformationBuffer follows after this;
+} __attribute__((packed));
+
+bool
+mbim_rx_cid(struct mhi_device *mhi_dev, u8 *buf, unsigned long len);
+
+u32 qmap_get_max_supproted_data_fmts(void);
+
+int mbim_set_data_format(struct mhi_device *mhi_dev, u32 transaction_id, bool new);
+
+int add_data_format_header(struct mhi_device *mhi_dev, u8 *buf, u32 len, u32 transaction_id, bool new);
+
+int data_format_packet_handling(struct mhi_device *mhi_dev, u8 *buf, unsigned long len, bool new);
+
+#endif
\ No newline at end of file
diff --git a/drivers/staging/em9190/inc/define_trace.h b/drivers/staging/em9190/inc/define_trace.h
new file mode 100644
index 000000000000..71d3d2a13863
--- /dev/null
+++ b/drivers/staging/em9190/inc/define_trace.h
@@ -0,0 +1,127 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Trace files that want to automate creation of all tracepoints defined
+ * in their file should include this file. The following are macros that the
+ * trace file may define:
+ *
+ * TRACE_SYSTEM defines the system the tracepoint is for
+ *
+ * TRACE_INCLUDE_FILE if the file name is something other than TRACE_SYSTEM.h
+ *     This macro may be defined to tell define_trace.h what file to include.
+ *     Note, leave off the ".h".
+ *
+ * TRACE_INCLUDE_PATH if the path is something other than core kernel include/trace
+ *     then this macro can define the path to use. Note, the path is relative to
+ *     define_trace.h, not the file including it. Full path names for out of tree
+ *     modules must be used.
+ */
+
+#ifdef CREATE_TRACE_POINTS
+
+/* Prevent recursion */
+#undef CREATE_TRACE_POINTS
+
+#include <linux/stringify.h>
+
+#undef TRACE_EVENT
+#define TRACE_EVENT(name, proto, args, tstruct, assign, print)	\
+	DEFINE_TRACE(name)
+
+#undef TRACE_EVENT_CONDITION
+#define TRACE_EVENT_CONDITION(name, proto, args, cond, tstruct, assign, print) \
+	TRACE_EVENT(name,						\
+		PARAMS(proto),						\
+		PARAMS(args),						\
+		PARAMS(tstruct),					\
+		PARAMS(assign),						\
+		PARAMS(print))
+
+#undef TRACE_EVENT_FN
+#define TRACE_EVENT_FN(name, proto, args, tstruct,		\
+		assign, print, reg, unreg)			\
+	DEFINE_TRACE_FN(name, reg, unreg)
+
+#undef TRACE_EVENT_FN_COND
+#define TRACE_EVENT_FN_COND(name, proto, args, cond, tstruct,		\
+		assign, print, reg, unreg)			\
+	DEFINE_TRACE_FN(name, reg, unreg)
+
+#undef DEFINE_EVENT
+#define DEFINE_EVENT(template, name, proto, args) \
+	DEFINE_TRACE(name)
+
+#undef DEFINE_EVENT_FN
+#define DEFINE_EVENT_FN(template, name, proto, args, reg, unreg) \
+	DEFINE_TRACE_FN(name, reg, unreg)
+
+#undef DEFINE_EVENT_PRINT
+#define DEFINE_EVENT_PRINT(template, name, proto, args, print)	\
+	DEFINE_TRACE(name)
+
+#undef DEFINE_EVENT_CONDITION
+#define DEFINE_EVENT_CONDITION(template, name, proto, args, cond) \
+	DEFINE_EVENT(template, name, PARAMS(proto), PARAMS(args))
+
+#undef DECLARE_TRACE
+#define DECLARE_TRACE(name, proto, args)	\
+	DEFINE_TRACE(name)
+
+#undef TRACE_INCLUDE
+#undef __TRACE_INCLUDE
+
+#ifndef TRACE_INCLUDE_FILE
+# define TRACE_INCLUDE_FILE TRACE_SYSTEM
+# define UNDEF_TRACE_INCLUDE_FILE
+#endif
+
+#ifndef TRACE_INCLUDE_PATH
+# define __TRACE_INCLUDE(system) <trace/events/system.h>
+# define UNDEF_TRACE_INCLUDE_PATH
+#else
+# define __TRACE_INCLUDE(system) __stringify(TRACE_INCLUDE_PATH/system.h)
+#endif
+
+# define TRACE_INCLUDE(system) __TRACE_INCLUDE(system)
+
+/* Let the trace headers be reread */
+#define TRACE_HEADER_MULTI_READ
+
+//#include TRACE_INCLUDE(TRACE_INCLUDE_FILE)
+#include "rmnet.h"
+
+/* Make all open coded DECLARE_TRACE nops */
+#undef DECLARE_TRACE
+#define DECLARE_TRACE(name, proto, args)
+
+#ifdef TRACEPOINTS_ENABLED
+#include <trace/trace_events.h>
+#include <trace/perf.h>
+#endif
+
+#undef TRACE_EVENT
+#undef TRACE_EVENT_FN
+#undef TRACE_EVENT_FN_COND
+#undef TRACE_EVENT_CONDITION
+#undef DECLARE_EVENT_CLASS
+#undef DEFINE_EVENT
+#undef DEFINE_EVENT_FN
+#undef DEFINE_EVENT_PRINT
+#undef DEFINE_EVENT_CONDITION
+#undef TRACE_HEADER_MULTI_READ
+#undef DECLARE_TRACE
+
+/* Only undef what we defined in this file */
+#ifdef UNDEF_TRACE_INCLUDE_FILE
+# undef TRACE_INCLUDE_FILE
+# undef UNDEF_TRACE_INCLUDE_FILE
+#endif
+
+#ifdef UNDEF_TRACE_INCLUDE_PATH
+# undef TRACE_INCLUDE_PATH
+# undef UNDEF_TRACE_INCLUDE_PATH
+#endif
+
+/* We may be processing more files */
+#define CREATE_TRACE_POINTS
+
+#endif /* CREATE_TRACE_POINTS */
diff --git a/drivers/staging/em9190/inc/devicetable.h b/drivers/staging/em9190/inc/devicetable.h
new file mode 100644
index 000000000000..2a7fcad76b07
--- /dev/null
+++ b/drivers/staging/em9190/inc/devicetable.h
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Device tables which are exported to userspace via
+ * scripts/mod/file2alias.c.  You must keep that file in sync with this
+ * header.
+ */
+
+#ifndef DEVICETABLE_H
+#define DEVICETABLE_H
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <linux/uuid.h>
+typedef unsigned long kernel_ulong_t;
+#endif
+
+
+#define MHI_NAME_SIZE 32
+
+/**
+ * struct mhi_device_id - MHI device identification
+ * @chan: MHI channel name
+ * @driver_data: driver data;
+ */
+
+struct mhi_device_id {
+	const char chan[MHI_NAME_SIZE];
+	kernel_ulong_t driver_data;
+};
+
+#endif /* DEVICETABLE_H */
diff --git a/drivers/staging/em9190/inc/esoc_client.h b/drivers/staging/em9190/inc/esoc_client.h
new file mode 100644
index 000000000000..0bd5459cb413
--- /dev/null
+++ b/drivers/staging/em9190/inc/esoc_client.h
@@ -0,0 +1,92 @@
+/* Copyright (c) 2014, 2017-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __ESOC_CLIENT_H_
+#define __ESOC_CLIENT_H_
+
+#include <linux/device.h>
+#include "esoc_ctrl.h"
+#include <linux/notifier.h>
+
+enum esoc_client_hook_prio {
+	ESOC_MHI_HOOK,
+	ESOC_CNSS_HOOK,
+	ESOC_MAX_HOOKS
+};
+
+struct esoc_link_data {
+	enum esoc_client_hook_prio prio;
+	__u64 link_id;
+};
+
+/* Flag values used with the power_on and power_off hooks */
+#define ESOC_HOOK_MDM_CRASH	0x0001 /* In crash handling path */
+#define ESOC_HOOK_MDM_DOWN	0x0002 /* MDM about to go down */
+
+struct esoc_client_hook {
+	char *name;
+	void *priv;
+	enum esoc_client_hook_prio prio;
+	int (*esoc_link_power_on)(void *priv, unsigned int flags);
+	void (*esoc_link_power_off)(void *priv, unsigned int flags);
+	u64 (*esoc_link_get_id)(void *priv);
+	void (*esoc_link_mdm_crash)(void *priv);
+};
+
+/*
+ * struct esoc_desc: Describes an external soc
+ * @name: external soc name
+ * @priv: private data for external soc
+ */
+struct esoc_desc {
+	const char *name;
+	const char *link;
+	const char *link_info;
+	void *priv;
+};
+
+#ifdef CONFIG_ESOC_CLIENT
+/* Can return probe deferral */
+struct esoc_desc *devm_register_esoc_client(struct device *dev,
+							const char *name);
+void devm_unregister_esoc_client(struct device *dev,
+						struct esoc_desc *esoc_desc);
+int esoc_register_client_notifier(struct notifier_block *nb);
+int esoc_register_client_hook(struct esoc_desc *desc,
+				struct esoc_client_hook *client_hook);
+int esoc_unregister_client_hook(struct esoc_desc *desc,
+				struct esoc_client_hook *client_hook);
+#else
+static inline struct esoc_desc *devm_register_esoc_client(struct device *dev,
+							const char *name)
+{
+	return NULL;
+}
+static inline void devm_unregister_esoc_client(struct device *dev,
+						struct esoc_desc *esoc_desc)
+{
+}
+static inline int esoc_register_client_notifier(struct notifier_block *nb)
+{
+	return -EIO;
+}
+static inline int esoc_register_client_hook(struct esoc_desc *desc,
+				struct esoc_client_hook *client_hook)
+{
+	return -EIO;
+}
+static inline int esoc_unregister_client_hook(struct esoc_desc *desc,
+				struct esoc_client_hook *client_hook)
+{
+	return -EIO;
+}
+#endif
+#endif
diff --git a/drivers/staging/em9190/inc/esoc_ctrl.h b/drivers/staging/em9190/inc/esoc_ctrl.h
new file mode 100644
index 000000000000..631fa449c386
--- /dev/null
+++ b/drivers/staging/em9190/inc/esoc_ctrl.h
@@ -0,0 +1,94 @@
+#ifndef _UAPI_ESOC_CTRL_H_
+#define _UAPI_ESOC_CTRL_H_
+
+#include <linux/types.h>
+
+#define ESOC_CODE		0xCC
+
+#define ESOC_CMD_EXE		_IOW(ESOC_CODE, 1, unsigned int)
+#define ESOC_WAIT_FOR_REQ	_IOR(ESOC_CODE, 2, unsigned int)
+#define ESOC_NOTIFY		_IOW(ESOC_CODE, 3, unsigned int)
+#define ESOC_GET_STATUS		_IOR(ESOC_CODE, 4, unsigned int)
+#define ESOC_GET_ERR_FATAL	_IOR(ESOC_CODE, 5, unsigned int)
+#define ESOC_WAIT_FOR_CRASH	_IOR(ESOC_CODE, 6, unsigned int)
+#define ESOC_REG_REQ_ENG	_IO(ESOC_CODE, 7)
+#define ESOC_REG_CMD_ENG	_IO(ESOC_CODE, 8)
+#define ESOC_GET_LINK_ID	_IOWR(ESOC_CODE, 9, __u64)
+#define ESOC_SET_BOOT_FAIL_ACT	_IOW(ESOC_CODE, 10, unsigned int)
+#define ESOC_SET_N_PON_TRIES	_IOW(ESOC_CODE, 11, unsigned int)
+
+#define ESOC_REQ_SEND_SHUTDOWN	ESOC_REQ_SEND_SHUTDOWN
+#define ESOC_REQ_CRASH_SHUTDOWN ESOC_REQ_CRASH_SHUTDOWN
+#define ESOC_PON_RETRY		ESOC_PON_RETRY
+#define ESOC_BOOT_FAIL_ACTION
+#define ESOC_LINK_ID
+
+enum esoc_boot_fail_action {
+	BOOT_FAIL_ACTION_RETRY,
+	BOOT_FAIL_ACTION_COLD_RESET,
+	BOOT_FAIL_ACTION_SHUTDOWN,
+	BOOT_FAIL_ACTION_PANIC,
+	BOOT_FAIL_ACTION_NOP,
+	BOOT_FAIL_ACTION_S3_RESET,
+	BOOT_FAIL_ACTION_LAST,
+};
+
+enum esoc_evt {
+	ESOC_RUN_STATE = 0x1,
+	ESOC_UNEXPECTED_RESET,
+	ESOC_ERR_FATAL,
+	ESOC_IN_DEBUG,
+	ESOC_REQ_ENG_ON,
+	ESOC_REQ_ENG_OFF,
+	ESOC_CMD_ENG_ON,
+	ESOC_CMD_ENG_OFF,
+	ESOC_INVALID_STATE,
+	ESOC_RETRY_PON_EVT,
+};
+
+enum esoc_cmd {
+	ESOC_PWR_ON = 1,
+	ESOC_PWR_OFF,
+	ESOC_FORCE_PWR_OFF,
+	ESOC_RESET,
+	ESOC_PREPARE_DEBUG,
+	ESOC_EXE_DEBUG,
+	ESOC_EXIT_DEBUG,
+};
+
+enum esoc_notify {
+	ESOC_IMG_XFER_DONE = 1,
+	ESOC_BOOT_DONE,
+	ESOC_BOOT_FAIL,
+	ESOC_IMG_XFER_RETRY,
+	ESOC_IMG_XFER_FAIL,
+	ESOC_UPGRADE_AVAILABLE,
+	ESOC_DEBUG_DONE,
+	ESOC_DEBUG_FAIL,
+	ESOC_PRIMARY_CRASH,
+	ESOC_PRIMARY_REBOOT,
+	ESOC_PON_RETRY,
+};
+
+enum esoc_req {
+	ESOC_REQ_IMG = 1,
+	ESOC_REQ_DEBUG,
+	ESOC_REQ_SHUTDOWN,
+	ESOC_REQ_SEND_SHUTDOWN,
+	ESOC_REQ_CRASH_SHUTDOWN,
+};
+
+#ifdef __KERNEL__
+/**
+ * struct esoc_handle: Handle for clients of esoc
+ * @name: name of the external soc.
+ * @link: link of external soc.
+ * @id: id of external soc.
+ */
+struct esoc_handle {
+	const char *name;
+	const char *link;
+	unsigned int id;
+};
+#endif
+#endif
diff --git a/drivers/staging/em9190/inc/if_link_rmnet.h b/drivers/staging/em9190/inc/if_link_rmnet.h
new file mode 100644
index 000000000000..ceadd61218c3
--- /dev/null
+++ b/drivers/staging/em9190/inc/if_link_rmnet.h
@@ -0,0 +1,14 @@
+#ifndef _IF_LINK_RMNET_H
+#define _IF_LINK_RMNET_H
+
+/* rmnet section */
+
+#define RMNET_FLAGS_INGRESS_DEAGGREGATION         (1U << 0)
+#define RMNET_FLAGS_INGRESS_MAP_COMMANDS          (1U << 1)
+#define RMNET_FLAGS_INGRESS_MAP_CKSUMV4           (1U << 2)
+#define RMNET_FLAGS_EGRESS_MAP_CKSUMV4            (1U << 3)
+#define RMNET_FLAGS_INGRESS_COALESCE              (1U << 4)
+#define RMNET_FLAGS_INGRESS_MAP_CKSUMV5           (1U << 5)
+#define RMNET_FLAGS_EGRESS_MAP_CKSUMV5            (1U << 6)
+
+#endif
diff --git a/drivers/staging/em9190/inc/io.h b/drivers/staging/em9190/inc/io.h
new file mode 100644
index 000000000000..e174fe5e208a
--- /dev/null
+++ b/drivers/staging/em9190/inc/io.h
@@ -0,0 +1,300 @@
+/*
+ * Based on arch/arm/include/asm/io.h
+ *
+ * Copyright (C) 1996-2000 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_IO_H
+#define __ASM_IO_H
+
+#ifdef __KERNEL__
+
+#include <linux/types.h>
+
+#include <asm/byteorder.h>
+#include <asm/barrier.h>
+#include <asm/memory.h>
+#include <asm/pgtable.h>
+#include <asm/early_ioremap.h>
+#include <asm/alternative.h>
+#include <asm/cpufeature.h>
+#include <linux/msm_rtb.h>
+
+#include <xen/xen.h>
+
+/*
+ * Generic IO read/write.  These perform native-endian accesses.
+ * that some architectures will want to re-define __raw_{read,write}w.
+ */
+static inline void __raw_writeb_no_log(u8 val, volatile void __iomem *addr)
+{
+	asm volatile("strb %w0, [%1]" : : "rZ" (val), "r" (addr));
+}
+
+static inline void __raw_writew_no_log(u16 val, volatile void __iomem *addr)
+{
+	asm volatile("strh %w0, [%1]" : : "rZ" (val), "r" (addr));
+}
+
+static inline void __raw_writel_no_log(u32 val, volatile void __iomem *addr)
+{
+	asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
+}
+
+static inline void __raw_writeq_no_log(u64 val, volatile void __iomem *addr)
+{
+	asm volatile("str %x0, [%1]" : : "rZ" (val), "r" (addr));
+}
+
+static inline u8 __raw_readb_no_log(const volatile void __iomem *addr)
+{
+	u8 val;
+	asm volatile(ALTERNATIVE("ldrb %w0, [%1]",
+				 "ldarb %w0, [%1]",
+				 ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE)
+		     : "=r" (val) : "r" (addr));
+	return val;
+}
+
+static inline u16 __raw_readw_no_log(const volatile void __iomem *addr)
+{
+	u16 val;
+
+	asm volatile(ALTERNATIVE("ldrh %w0, [%1]",
+				 "ldarh %w0, [%1]",
+				 ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE)
+		     : "=r" (val) : "r" (addr));
+	return val;
+}
+
+static inline u32 __raw_readl_no_log(const volatile void __iomem *addr)
+{
+	u32 val;
+	asm volatile(ALTERNATIVE("ldr %w0, [%1]",
+				 "ldar %w0, [%1]",
+				 ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE)
+		     : "=r" (val) : "r" (addr));
+	return val;
+}
+
+static inline u64 __raw_readq_no_log(const volatile void __iomem *addr)
+{
+	u64 val;
+	asm volatile(ALTERNATIVE("ldr %0, [%1]",
+				 "ldar %0, [%1]",
+				 ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE)
+		     : "=r" (val) : "r" (addr));
+	return val;
+}
+
+/*
+ * There may be cases when  clients don't want to support or can't support the
+ * logging, The appropriate functions can be used but clinets should carefully
+ * consider why they can't support the logging
+ */
+
+#define __raw_write_logged(v, a, _t) ({ \
+	int _ret; \
+	volatile void __iomem *_a = (a); \
+	void *_addr = (void __force *)(_a); \
+	_ret = uncached_logk(LOGK_WRITEL, _addr); \
+	ETB_WAYPOINT; \
+	__raw_write##_t##_no_log((v), _a); \
+	if (_ret) \
+		LOG_BARRIER; \
+	})
+
+#define __raw_writeb(v, a)	__raw_write_logged((v), a, b)
+#define __raw_writew(v, a)	__raw_write_logged((v), a, w)
+#define __raw_writel(v, a)	__raw_write_logged((v), a, l)
+#define __raw_writeq(v, a)	__raw_write_logged((v), a, q)
+
+#define __raw_read_logged(a, _l, _t)    ({ \
+	_t __a; \
+	const volatile void __iomem *_a = (a); \
+	void *_addr = (void __force *)(_a); \
+	int _ret; \
+	_ret = uncached_logk(LOGK_READL, _addr); \
+	ETB_WAYPOINT; \
+	__a = __raw_read##_l##_no_log(_a); \
+	if (_ret) \
+		LOG_BARRIER; \
+	__a; \
+	})
+
+#define __raw_readb(a)		__raw_read_logged((a), b, u8)
+#define __raw_readw(a)		__raw_read_logged((a), w, u16)
+#define __raw_readl(a)		__raw_read_logged((a), l, u32)
+#define __raw_readq(a)		__raw_read_logged((a), q, u64)
+
+/* IO barriers */
+#define __iormb(v)							\
+({									\
+	unsigned long tmp;						\
+									\
+	rmb();								\
+									\
+	/*								\
+	 * Create a dummy control dependency from the IO read to any	\
+	 * later instructions. This ensures that a subsequent call to	\
+	 * udelay() will be ordered due to the ISB in get_cycles().	\
+	 */								\
+	asm volatile("eor	%0, %1, %1\n"				\
+		     "cbnz	%0, ."					\
+		     : "=r" (tmp) : "r" ((unsigned long)(v))		\
+		     : "memory");					\
+})
+
+#define __iowmb()		wmb()
+
+#define mmiowb()		do { } while (0)
+
+/*
+ * Relaxed I/O memory access primitives. These follow the Device memory
+ * ordering rules but do not guarantee any ordering relative to Normal memory
+ * accesses.
+ */
+#define readb_relaxed(c)	({ u8  __r = __raw_readb(c); __r; })
+#define readw_relaxed(c)	({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })
+#define readl_relaxed(c)	({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })
+#define readq_relaxed(c)	({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })
+
+#define writeb_relaxed(v,c)	((void)__raw_writeb((v),(c)))
+#define writew_relaxed(v,c)	((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))
+#define writel_relaxed(v,c)	((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))
+#define writeq_relaxed(v,c)	((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))
+
+#define readb_relaxed_no_log(c)	({ u8 __v = __raw_readb_no_log(c); __v; })
+#define readw_relaxed_no_log(c) \
+	({ u16 __v = le16_to_cpu((__force __le16)__raw_readw_no_log(c)); __v; })
+#define readl_relaxed_no_log(c) \
+	({ u32 __v = le32_to_cpu((__force __le32)__raw_readl_no_log(c)); __v; })
+#define readq_relaxed_no_log(c) \
+	({ u64 __v = le64_to_cpu((__force __le64)__raw_readq_no_log(c)); __v; })
+
+#define writeb_relaxed_no_log(v, c)	((void)__raw_writeb_no_log((v), (c)))
+#define writew_relaxed_no_log(v, c) \
+	((void)__raw_writew_no_log((__force u16)cpu_to_le32(v), (c)))
+#define writel_relaxed_no_log(v, c) \
+	((void)__raw_writel_no_log((__force u32)cpu_to_le32(v), (c)))
+#define writeq_relaxed_no_log(v, c) \
+	((void)__raw_writeq_no_log((__force u64)cpu_to_le32(v), (c)))
+
+/*
+ * I/O memory access primitives. Reads are ordered relative to any
+ * following Normal memory access. Writes are ordered relative to any prior
+ * Normal memory access.
+ */
+#define readb(c)		({ u8  __v = readb_relaxed(c); __iormb(__v); __v; })
+#define readw(c)		({ u16 __v = readw_relaxed(c); __iormb(__v); __v; })
+#define readl(c)		({ u32 __v = readl_relaxed(c); __iormb(__v); __v; })
+#define readq(c)		({ u64 __v = readq_relaxed(c); __iormb(__v); __v; })
+
+#define writeb(v,c)		({ __iowmb(); writeb_relaxed((v),(c)); })
+#define writew(v,c)		({ __iowmb(); writew_relaxed((v),(c)); })
+#define writel(v,c)		({ __iowmb(); writel_relaxed((v),(c)); })
+#define writeq(v,c)		({ __iowmb(); writeq_relaxed((v),(c)); })
+
+#define readb_no_log(c) \
+		({ u8  __v = readb_relaxed_no_log(c); __iormb(__v); __v; })
+#define readw_no_log(c) \
+		({ u16 __v = readw_relaxed_no_log(c); __iormb(__v); __v; })
+#define readl_no_log(c) \
+		({ u32 __v = readl_relaxed_no_log(c); __iormb(__v); __v; })
+#define readq_no_log(c) \
+		({ u64 __v = readq_relaxed_no_log(c); __iormb(__v); __v; })
+
+#define writeb_no_log(v, c) \
+		({ __iowmb(); writeb_relaxed_no_log((v), (c)); })
+#define writew_no_log(v, c) \
+		({ __iowmb(); writew_relaxed_no_log((v), (c)); })
+#define writel_no_log(v, c) \
+		({ __iowmb(); writel_relaxed_no_log((v), (c)); })
+#define writeq_no_log(v, c) \
+		({ __iowmb(); writeq_relaxed_no_log((v), (c)); })
+
+/*
+ *  I/O port access primitives.
+ */
+#define arch_has_dev_port()	(1)
+#define IO_SPACE_LIMIT		(PCI_IO_SIZE - 1)
+#define PCI_IOBASE		((void __iomem *)PCI_IO_START)
+
+/*
+ * String version of I/O memory access operations.
+ */
+extern void __memcpy_fromio(void *, const volatile void __iomem *, size_t);
+extern void __memcpy_toio(volatile void __iomem *, const void *, size_t);
+extern void __memset_io(volatile void __iomem *, int, size_t);
+
+#define memset_io(c,v,l)	__memset_io((c),(v),(l))
+#define memcpy_fromio(a,c,l)	__memcpy_fromio((a),(c),(l))
+#define memcpy_toio(c,a,l)	__memcpy_toio((c),(a),(l))
+
+/*
+ * I/O memory mapping functions.
+ */
+extern void __iomem *__ioremap(phys_addr_t phys_addr, size_t size, pgprot_t prot);
+extern void __iounmap(volatile void __iomem *addr);
+extern void __iomem *ioremap_cache(phys_addr_t phys_addr, size_t size);
+
+#define ioremap(addr, size)		__ioremap((addr), (size), __pgprot(PROT_DEVICE_nGnRE))
+#define ioremap_nocache(addr, size)	__ioremap((addr), (size), __pgprot(PROT_DEVICE_nGnRE))
+#define ioremap_wc(addr, size)		__ioremap((addr), (size), __pgprot(PROT_NORMAL_NC))
+#define ioremap_wt(addr, size)		__ioremap((addr), (size), __pgprot(PROT_DEVICE_nGnRE))
+#define iounmap				__iounmap
+
+/*
+ * PCI configuration space mapping function.
+ *
+ * The PCI specification disallows posted write configuration transactions.
+ * Add an arch specific pci_remap_cfgspace() definition that is implemented
+ * through nGnRnE device memory attribute as recommended by the ARM v8
+ * Architecture reference manual Issue A.k B2.8.2 "Device memory".
+ */
+#define pci_remap_cfgspace(addr, size) __ioremap((addr), (size), __pgprot(PROT_DEVICE_nGnRnE))
+
+/*
+ * io{read,write}{16,32,64}be() macros
+ */
+#define ioread16be(p)		({ __u16 __v = be16_to_cpu((__force __be16)__raw_readw(p)); __iormb(__v); __v; })
+#define ioread32be(p)		({ __u32 __v = be32_to_cpu((__force __be32)__raw_readl(p)); __iormb(__v); __v; })
+#define ioread64be(p)		({ __u64 __v = be64_to_cpu((__force __be64)__raw_readq(p)); __iormb(__v); __v; })
+
+#define iowrite16be(v,p)	({ __iowmb(); __raw_writew((__force __u16)cpu_to_be16(v), p); })
+#define iowrite32be(v,p)	({ __iowmb(); __raw_writel((__force __u32)cpu_to_be32(v), p); })
+#define iowrite64be(v,p)	({ __iowmb(); __raw_writeq((__force __u64)cpu_to_be64(v), p); })
+
+#include <asm-generic/io.h>
+
+/*
+ * More restrictive address range checking than the default implementation
+ * (PHYS_OFFSET and PHYS_MASK taken into account).
+ */
+#define ARCH_HAS_VALID_PHYS_ADDR_RANGE
+extern int valid_phys_addr_range(phys_addr_t addr, size_t size);
+extern int valid_mmap_phys_addr_range(unsigned long pfn, size_t size);
+
+extern int devmem_is_allowed(unsigned long pfn);
+
+struct bio_vec;
+extern bool xen_biovec_phys_mergeable(const struct bio_vec *vec1,
+				      const struct bio_vec *vec2);
+#define BIOVEC_PHYS_MERGEABLE(vec1, vec2)				\
+	(__BIOVEC_PHYS_MERGEABLE(vec1, vec2) &&				\
+	 (!xen_domain() || xen_biovec_phys_mergeable(vec1, vec2)))
+
+#endif	/* __KERNEL__ */
+#endif	/* __ASM_IO_H */
diff --git a/drivers/staging/em9190/inc/ipc_logging.h b/drivers/staging/em9190/inc/ipc_logging.h
new file mode 100644
index 000000000000..5c1937f3af07
--- /dev/null
+++ b/drivers/staging/em9190/inc/ipc_logging.h
@@ -0,0 +1,290 @@
+/* Copyright (c) 2012-2015,2017 The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _IPC_LOGGING_H
+#define _IPC_LOGGING_H
+
+#include <linux/types.h>
+
+#define MAX_MSG_SIZE 255
+
+enum {
+	TSV_TYPE_MSG_START = 1,
+	TSV_TYPE_SKB = TSV_TYPE_MSG_START,
+	TSV_TYPE_STRING,
+	TSV_TYPE_MSG_END = TSV_TYPE_STRING,
+};
+
+struct tsv_header {
+	unsigned char type;
+	unsigned char size; /* size of data field */
+};
+
+struct encode_context {
+	struct tsv_header hdr;
+	char buff[MAX_MSG_SIZE];
+	int offset;
+};
+
+struct decode_context {
+	int output_format;      /* 0 = debugfs */
+	char *buff;             /* output buffer */
+	int size;               /* size of output buffer */
+};
+
+#if defined(CONFIG_IPC_LOGGING)
+/*
+ * ipc_log_context_create: Create a debug log context
+ *                         Should not be called from atomic context
+ *
+ * @max_num_pages: Number of pages of logging space required (max. 10)
+ * @mod_name     : Name of the directory entry under DEBUGFS
+ * @user_version : Version number of user-defined message formats
+ *
+ * returns context id on success, NULL on failure
+ */
+void *ipc_log_context_create(int max_num_pages, const char *modname,
+		uint16_t user_version);
+
+/*
+ * msg_encode_start: Start encoding a log message
+ *
+ * @ectxt: Temporary storage to hold the encoded message
+ * @type:  Root event type defined by the module which is logging
+ */
+void msg_encode_start(struct encode_context *ectxt, uint32_t type);
+
+/*
+ * tsv_timestamp_write: Writes the current timestamp count
+ *
+ * @ectxt: Context initialized by calling msg_encode_start()
+ */
+int tsv_timestamp_write(struct encode_context *ectxt);
+
+/*
+ * tsv_qtimer_write: Writes the current QTimer timestamp count
+ *
+ * @ectxt: Context initialized by calling msg_encode_start()
+ */
+int tsv_qtimer_write(struct encode_context *ectxt);
+
+/*
+ * tsv_pointer_write: Writes a data pointer
+ *
+ * @ectxt:   Context initialized by calling msg_encode_start()
+ * @pointer: Pointer value to write
+ */
+int tsv_pointer_write(struct encode_context *ectxt, void *pointer);
+
+/*
+ * tsv_int32_write: Writes a 32-bit integer value
+ *
+ * @ectxt: Context initialized by calling msg_encode_start()
+ * @n:     Integer to write
+ */
+int tsv_int32_write(struct encode_context *ectxt, int32_t n);
+
+/*
+ * tsv_int32_write: Writes a 32-bit integer value
+ *
+ * @ectxt: Context initialized by calling msg_encode_start()
+ * @n:     Integer to write
+ */
+int tsv_byte_array_write(struct encode_context *ectxt,
+			 void *data, int data_size);
+
+/*
+ * msg_encode_end: Complete the message encode process
+ *
+ * @ectxt: Temporary storage which holds the encoded message
+ */
+void msg_encode_end(struct encode_context *ectxt);
+
+/*
+ * msg_encode_end: Complete the message encode process
+ *
+ * @ectxt: Temporary storage which holds the encoded message
+ */
+void ipc_log_write(void *ctxt, struct encode_context *ectxt);
+
+/*
+ * ipc_log_string: Helper function to log a string
+ *
+ * @ilctxt: Debug Log Context created using ipc_log_context_create()
+ * @fmt:    Data specified using format specifiers
+ */
+int ipc_log_string(void *ilctxt, const char *fmt, ...) __printf(2, 3);
+
+/**
+ * ipc_log_extract - Reads and deserializes log
+ *
+ * @ilctxt:  logging context
+ * @buff:    buffer to receive the data
+ * @size:    size of the buffer
+ * @returns: 0 if no data read; >0 number of bytes read; < 0 error
+ *
+ * If no data is available to be read, then the ilctxt::read_avail
+ * completion is reinitialized.  This allows clients to block
+ * until new log data is save.
+ */
+int ipc_log_extract(void *ilctxt, char *buff, int size);
+
+/*
+ * Print a string to decode context.
+ * @dctxt   Decode context
+ * @args   printf args
+ */
+#define IPC_SPRINTF_DECODE(dctxt, args...) \
+do { \
+	int i; \
+	i = scnprintf(dctxt->buff, dctxt->size, args); \
+	dctxt->buff += i; \
+	dctxt->size -= i; \
+} while (0)
+
+/*
+ * tsv_timestamp_read: Reads a timestamp
+ *
+ * @ectxt:  Context retrieved by reading from log space
+ * @dctxt:  Temporary storage to hold the decoded message
+ * @format: Output format while dumping through DEBUGFS
+ */
+void tsv_timestamp_read(struct encode_context *ectxt,
+			struct decode_context *dctxt, const char *format);
+
+/*
+ * tsv_qtimer_read: Reads a QTimer timestamp
+ *
+ * @ectxt:  Context retrieved by reading from log space
+ * @dctxt:  Temporary storage to hold the decoded message
+ * @format: Output format while dumping through DEBUGFS
+ */
+void tsv_qtimer_read(struct encode_context *ectxt,
+		     struct decode_context *dctxt, const char *format);
+
+/*
+ * tsv_pointer_read: Reads a data pointer
+ *
+ * @ectxt:  Context retrieved by reading from log space
+ * @dctxt:  Temporary storage to hold the decoded message
+ * @format: Output format while dumping through DEBUGFS
+ */
+void tsv_pointer_read(struct encode_context *ectxt,
+		      struct decode_context *dctxt, const char *format);
+
+/*
+ * tsv_int32_read: Reads a 32-bit integer value
+ *
+ * @ectxt:  Context retrieved by reading from log space
+ * @dctxt:  Temporary storage to hold the decoded message
+ * @format: Output format while dumping through DEBUGFS
+ */
+int32_t tsv_int32_read(struct encode_context *ectxt,
+		       struct decode_context *dctxt, const char *format);
+
+/*
+ * tsv_int32_read: Reads a 32-bit integer value
+ *
+ * @ectxt:  Context retrieved by reading from log space
+ * @dctxt:  Temporary storage to hold the decoded message
+ * @format: Output format while dumping through DEBUGFS
+ */
+void tsv_byte_array_read(struct encode_context *ectxt,
+			 struct decode_context *dctxt, const char *format);
+
+/*
+ * add_deserialization_func: Register a deserialization function to
+ *                           to unpack the subevents of a main event
+ *
+ * @ctxt: Debug log context to which the deserialization function has
+ *        to be registered
+ * @type: Main/Root event, defined by the module which is logging, to
+ *        which this deserialization function has to be registered.
+ * @dfune: Deserialization function to be registered
+ *
+ * return 0 on success, -ve value on FAILURE
+ */
+int add_deserialization_func(void *ctxt, int type,
+			void (*dfunc)(struct encode_context *,
+				      struct decode_context *));
+
+/*
+ * ipc_log_context_destroy: Destroy debug log context
+ *
+ * @ctxt: debug log context created by calling ipc_log_context_create API.
+ */
+int ipc_log_context_destroy(void *ctxt);
+
+#else
+
+static inline void *ipc_log_context_create(int max_num_pages,
+	const char *modname, uint16_t user_version)
+{ return NULL; }
+
+static inline void msg_encode_start(struct encode_context *ectxt,
+	uint32_t type) { }
+
+static inline int tsv_timestamp_write(struct encode_context *ectxt)
+{ return -EINVAL; }
+
+static inline int tsv_qtimer_write(struct encode_context *ectxt)
+{ return -EINVAL; }
+
+static inline int tsv_pointer_write(struct encode_context *ectxt, void *pointer)
+{ return -EINVAL; }
+
+static inline int tsv_int32_write(struct encode_context *ectxt, int32_t n)
+{ return -EINVAL; }
+
+static inline int tsv_byte_array_write(struct encode_context *ectxt,
+			 void *data, int data_size)
+{ return -EINVAL; }
+
+static inline void msg_encode_end(struct encode_context *ectxt) { }
+
+static inline void ipc_log_write(void *ctxt, struct encode_context *ectxt) { }
+
+static inline int ipc_log_string(void *ilctxt, const char *fmt, ...)
+{ return -EINVAL; }
+
+static inline int ipc_log_extract(void *ilctxt, char *buff, int size)
+{ return -EINVAL; }
+
+#define IPC_SPRINTF_DECODE(dctxt, args...) do { } while (0)
+
+static inline void tsv_timestamp_read(struct encode_context *ectxt,
+			struct decode_context *dctxt, const char *format) { }
+
+static inline void tsv_qtimer_read(struct encode_context *ectxt,
+			struct decode_context *dctxt, const char *format) { }
+
+static inline void tsv_pointer_read(struct encode_context *ectxt,
+		      struct decode_context *dctxt, const char *format) { }
+
+static inline int32_t tsv_int32_read(struct encode_context *ectxt,
+		       struct decode_context *dctxt, const char *format)
+{ return 0; }
+
+static inline void tsv_byte_array_read(struct encode_context *ectxt,
+			 struct decode_context *dctxt, const char *format) { }
+
+static inline int add_deserialization_func(void *ctxt, int type,
+			void (*dfunc)(struct encode_context *,
+				      struct decode_context *))
+{ return 0; }
+
+static inline int ipc_log_context_destroy(void *ctxt)
+{ return 0; }
+
+#endif
+
+#endif
diff --git a/drivers/staging/em9190/inc/mhi.h b/drivers/staging/em9190/inc/mhi.h
new file mode 100644
index 000000000000..122c2599a67f
--- /dev/null
+++ b/drivers/staging/em9190/inc/mhi.h
@@ -0,0 +1,822 @@
+/* Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ */
+#ifndef _MHI_H_
+#define _MHI_H_
+
+struct mhi_chan;
+struct mhi_event;
+struct mhi_ctxt;
+struct mhi_cmd;
+struct image_info;
+struct bhi_vec_entry;
+struct mhi_timesync;
+struct mhi_buf_info;
+
+/**
+ * enum MHI_CB - MHI callback
+ * @MHI_CB_IDLE: MHI entered idle state
+ * @MHI_CB_PENDING_DATA: New data available for client to process
+ * @MHI_CB_LPM_ENTER: MHI host entered low power mode
+ * @MHI_CB_LPM_EXIT: MHI host about to exit low power mode
+ * @MHI_CB_EE_RDDM: MHI device entered RDDM execution enviornment
+ * @MHI_CB_EE_MISSION_MODE: MHI device entered Mission Mode ee
+ * @MHI_CB_SYS_ERROR: MHI device enter error state (may recover)
+ * @MHI_CB_FATAL_ERROR: MHI device entered fatal error
+ * @MHI_CB_BW_REQ: Received a bandwidth switch request from device
+ */
+enum MHI_CB {
+	MHI_CB_IDLE,
+	MHI_CB_PENDING_DATA,
+	MHI_CB_LPM_ENTER,
+	MHI_CB_LPM_EXIT,
+	MHI_CB_EE_RDDM,
+	MHI_CB_EE_MISSION_MODE,
+	MHI_CB_SYS_ERROR,
+	MHI_CB_FATAL_ERROR,
+	MHI_CB_BW_REQ,
+};
+
+/**
+ * enum MHI_DEBUG_LEVL - various debugging level
+ */
+enum MHI_DEBUG_LEVEL {
+	MHI_MSG_LVL_VERBOSE,
+	MHI_MSG_LVL_INFO,
+	MHI_MSG_LVL_ERROR,
+	MHI_MSG_LVL_CRITICAL,
+	MHI_MSG_LVL_MASK_ALL,
+};
+
+/**
+ * enum MHI_FLAGS - Transfer flags
+ * @MHI_EOB: End of buffer for bulk transfer
+ * @MHI_EOT: End of transfer
+ * @MHI_CHAIN: Linked transfer
+ */
+enum MHI_FLAGS {
+	MHI_EOB,
+	MHI_EOT,
+	MHI_CHAIN,
+};
+
+/**
+ * enum mhi_device_type - Device types
+ * @MHI_XFER_TYPE: Handles data transfer
+ * @MHI_TIMESYNC_TYPE: Use for timesync feature
+ * @MHI_CONTROLLER_TYPE: Control device
+ */
+enum mhi_device_type {
+	MHI_XFER_TYPE,
+	MHI_TIMESYNC_TYPE,
+	MHI_CONTROLLER_TYPE,
+};
+
+/**
+ * enum mhi_ee - device current execution enviornment
+ * @MHI_EE_PBL - device in PBL
+ * @MHI_EE_SBL - device in SBL
+ * @MHI_EE_AMSS - device in mission mode (firmware fully loaded)
+ * @MHI_EE_RDDM - device in ram dump collection mode
+ * @MHI_EE_WFW - device in WLAN firmware mode
+ * @MHI_EE_PTHRU - device in PBL but configured in pass thru mode
+ * @MHI_EE_EDL - device in emergency download mode
+ */
+enum mhi_ee {
+	MHI_EE_PBL,
+	MHI_EE_SBL,
+	MHI_EE_AMSS,
+	MHI_EE_RDDM,
+	MHI_EE_WFW,
+	MHI_EE_PTHRU,
+	MHI_EE_EDL,
+	MHI_EE_MAX_SUPPORTED = MHI_EE_EDL,
+	MHI_EE_DISABLE_TRANSITION, /* local EE, not related to mhi spec */
+	MHI_EE_NOT_SUPPORTED,
+	MHI_EE_MAX,
+};
+
+/**
+ * enum mhi_dev_state - device current MHI state
+ */
+enum mhi_dev_state {
+	MHI_STATE_RESET = 0x0,
+	MHI_STATE_READY = 0x1,
+	MHI_STATE_M0 = 0x2,
+	MHI_STATE_M1 = 0x3,
+	MHI_STATE_M2 = 0x4,
+	MHI_STATE_M3 = 0x5,
+	MHI_STATE_M3_FAST = 0x6,
+	MHI_STATE_BHI  = 0x7,
+	MHI_STATE_SYS_ERR  = 0xFF,
+	MHI_STATE_MAX,
+};
+
+/**
+ * struct mhi_link_info - bw requirement
+ * target_link_speed - as defined by TLS bits in LinkControl reg
+ * target_link_width - as defined by NLW bits in LinkStatus reg
+ */
+struct mhi_link_info {
+	unsigned int target_link_speed;
+	unsigned int target_link_width;
+};
+
+#define MHI_VOTE_BUS BIT(0) /* do not disable the bus */
+#define MHI_VOTE_DEVICE BIT(1) /* prevent mhi device from entering lpm */
+
+/**
+ * struct image_info - firmware and rddm table table
+ * @mhi_buf - Contain device firmware and rddm table
+ * @entries - # of entries in table
+ */
+struct image_info {
+	struct mhi_buf *mhi_buf;
+	struct bhi_vec_entry *bhi_vec;
+	u32 entries;
+};
+
+struct mhi_mem_props {
+	u64 virt_aligned;
+	u64 virt_unaligned;
+	u64 phys_aligned;
+	u64 phys_unaligned;
+	u64 size;
+	u64 alignment;
+	dma_addr_t handle;
+    u64 alloc_size;
+};
+
+/**
+ * struct mhi_controller - Master controller structure for external modem
+ * @dev: Device associated with this controller
+ * @of_node: DT that has MHI configuration information
+ * @regs: Points to base of MHI MMIO register space
+ * @bhi: Points to base of MHI BHI register space
+ * @bhie: Points to base of MHI BHIe register space
+ * @wake_db: MHI WAKE doorbell register address
+ * @dev_id: PCIe device id of the external device
+ * @domain: PCIe domain the device connected to
+ * @bus: PCIe bus the device assigned to
+ * @slot: PCIe slot for the modem
+ * @iova_start: IOMMU starting address for data
+ * @iova_stop: IOMMU stop address for data
+ * @fw_image: Firmware image name for normal booting
+ * @edl_image: Firmware image name for emergency download mode
+ * @fbc_download: MHI host needs to do complete image transfer
+ * @rddm_size: RAM dump size that host should allocate for debugging purpose
+ * @sbl_size: SBL image size
+ * @seg_len: BHIe vector size
+ * @fbc_image: Points to firmware image buffer
+ * @rddm_image: Points to RAM dump buffer
+ * @max_chan: Maximum number of channels controller support
+ * @mhi_chan: Points to channel configuration table
+ * @lpm_chans: List of channels that require LPM notifications
+ * @total_ev_rings: Total # of event rings allocated
+ * @hw_ev_rings: Number of hardware event rings
+ * @sw_ev_rings: Number of software event rings
+ * @msi_required: Number of msi required to operate
+ * @msi_allocated: Number of msi allocated by bus master
+ * @irq: base irq # to request
+ * @mhi_event: MHI event ring configurations table
+ * @mhi_cmd: MHI command ring configurations table
+ * @mhi_ctxt: MHI device context, shared memory between host and device
+ * @timeout_ms: Timeout in ms for state transitions
+ * @pm_state: Power management state
+ * @ee: MHI device execution environment
+ * @dev_state: MHI STATE
+ * @mhi_link_info: requested link bandwidth by device
+ * @status_cb: CB function to notify various power states to but master
+ * @link_status: Query link status in case of abnormal value read from device
+ * @runtime_get: Async runtime resume function
+ * @runtimet_put: Release votes
+ * @time_get: Return host time in us
+ * @lpm_disable: Request controller to disable link level low power modes
+ * @lpm_enable: Controller may enable link level low power modes again
+ * @priv_data: Points to bus master's private data
+ */
+struct mhi_controller {
+	struct list_head node;
+	struct mhi_device *mhi_dev;
+
+	/* device node for iommu ops */
+	struct device *dev;
+	struct device_node *of_node;
+
+	/* mmio base */
+	phys_addr_t base_addr;
+	void __iomem *regs;
+	void __iomem *bhi;
+	void __iomem *bhie;
+	void __iomem *wake_db;
+
+	/* device topology */
+	u32 dev_id;
+	u32 domain;
+	u32 bus;
+	u32 slot;
+	u32 family_number;
+	u32 device_number;
+	u32 major_version;
+	u32 minor_version;
+
+	/* addressing window */
+	dma_addr_t iova_start;
+	dma_addr_t iova_stop;
+
+	/* fw images */
+	const char *fw_image;
+	const char *edl_image;
+
+	/* mhi host manages downloading entire fbc images */
+	bool fbc_download;
+	size_t rddm_size;
+	size_t sbl_size;
+	size_t seg_len;
+	u32 session_id;
+	u32 sequence_id;
+	struct image_info *fbc_image;
+	struct image_info *rddm_image;
+
+	/* physical channel config data */
+	u32 max_chan;
+	struct mhi_chan *mhi_chan;
+	struct list_head lpm_chans; /* these chan require lpm notification */
+
+	/* physical event config data */
+	u32 total_ev_rings;
+	u32 hw_ev_rings;
+	u32 sw_ev_rings;
+	u32 msi_required;
+	u32 msi_allocated;
+	bool msi_active;
+	int *irq; /* interrupt table */
+	struct mhi_event *mhi_event;
+
+	/* cmd rings */
+	struct mhi_cmd *mhi_cmd;
+
+	/* mhi context (shared with device) */
+	struct mhi_ctxt *mhi_ctxt;
+
+	u32 timeout_ms;
+
+	/* caller should grab pm_mutex for suspend/resume operations */
+	struct mutex pm_mutex;
+	bool pre_init;
+	rwlock_t pm_lock;
+	u32 pm_state;
+	u32 saved_pm_state; /* saved state during fast suspend */
+	u32 db_access; /* db access only on these states */
+	enum mhi_ee ee;
+	u32 ee_table[MHI_EE_MAX]; /* ee conversion from dev to host */
+	enum mhi_dev_state dev_state;
+	enum mhi_dev_state saved_dev_state;
+	bool wake_set;
+	atomic_t dev_wake;
+	atomic_t alloc_size;
+	atomic_t pending_pkts;
+	struct list_head transition_list;
+	spinlock_t transition_lock;
+	spinlock_t wlock;
+
+    /* mux for device creation/removal */
+	struct mutex dev_mutex;
+
+	/* target bandwidth info */
+	struct mhi_link_info mhi_link_info;
+
+	/* debug counters */
+	u32 M0, M2, M3, M3_FAST;
+
+	/* worker for different state transitions */
+	struct work_struct st_worker;
+	struct work_struct fw_worker;
+	struct work_struct syserr_worker;
+	wait_queue_head_t state_event;
+
+	/* shadow functions */
+	void (*status_cb)(struct mhi_controller *, void *, enum MHI_CB);
+	int (*link_status)(struct mhi_controller *, void *);
+	void (*wake_get)(struct mhi_controller *, bool);
+	void (*wake_put)(struct mhi_controller *, bool);
+	void (*wake_toggle)(struct mhi_controller *mhi_cntrl);
+	int (*runtime_get)(struct mhi_controller *, void *);
+	void (*runtime_put)(struct mhi_controller *, void *);
+	u64 (*time_get)(struct mhi_controller *mhi_cntrl, void *priv);
+	int (*lpm_disable)(struct mhi_controller *mhi_cntrl, void *priv);
+	int (*lpm_enable)(struct mhi_controller *mhi_cntrl, void *priv);
+	int (*map_single)(struct mhi_controller *mhi_cntrl,
+			  struct mhi_buf_info *buf);
+	void (*unmap_single)(struct mhi_controller *mhi_cntrl,
+			     struct mhi_buf_info *buf);
+	void (*tsync_log)(struct mhi_controller *mhi_cntrl, u64 remote_time);
+
+	/* channel to control DTR messaging */
+	struct mhi_device *dtr_dev;
+
+	/* bounce buffer settings */
+	bool bounce_buf;
+	size_t buffer_len;
+
+	/* supports time sync feature */
+	struct mhi_timesync *mhi_tsync;
+	struct mhi_device *tsync_dev;
+	u64 local_timer_freq;
+	u64 remote_timer_freq;
+
+	/* kernel log level */
+	enum MHI_DEBUG_LEVEL klog_lvl;
+
+	/* private log level controller driver to set */
+	enum MHI_DEBUG_LEVEL log_lvl;
+
+	/* controller specific data */
+	void *priv_data;
+	void *log_buf;
+	struct dentry *dentry;
+	struct dentry *parent;
+
+    struct mhi_ctrl_seg *ctrl_seg;
+    struct mhi_mem_props mem_props;
+
+	struct tasklet_struct intvec_task;
+	struct work_struct intvec_worker;
+
+	enum mhi_ee previous_ee;
+	enum mhi_dev_state previous_dev_state;
+
+	bool mission_mode_done;
+	bool create_device_done;
+
+	bool timesync_done;
+};
+
+/**
+ * struct mhi_device - mhi device structure associated bind to channel
+ * @dev: Device associated with the channels
+ * @mtu: Maximum # of bytes controller support
+ * @ul_chan_id: MHI channel id for UL transfer
+ * @dl_chan_id: MHI channel id for DL transfer
+ * @tiocm: Device current terminal settings
+ * @early_notif: This device needs an early notification in case of error
+ * with external modem.
+ * @dev_vote: Keep external device in active state
+ * @bus_vote: Keep physical bus (pci, spi) in active state
+ * @priv: Driver private data
+ */
+struct mhi_device {
+	struct device dev;
+	u32 dev_id;
+	u32 domain;
+	u32 bus;
+	u32 slot;
+	size_t mtu;
+	int ul_chan_id;
+	int dl_chan_id;
+	int ul_event_id;
+	int dl_event_id;
+	u32 tiocm;
+	bool early_notif;
+	const struct mhi_device_id *id;
+	const char *chan_name;
+	struct mhi_controller *mhi_cntrl;
+	struct mhi_chan *ul_chan;
+	struct mhi_chan *dl_chan;
+	atomic_t dev_vote;
+	atomic_t bus_vote;
+	enum mhi_device_type dev_type;
+	void *priv_data;
+	int (*ul_xfer)(struct mhi_device *, struct mhi_chan *, void *,
+		       size_t, enum MHI_FLAGS);
+	int (*dl_xfer)(struct mhi_device *, struct mhi_chan *, void *,
+		       size_t, enum MHI_FLAGS);
+	void (*status_cb)(struct mhi_device *, enum MHI_CB);
+	bool (*ul_full)(struct mhi_device *, struct mhi_chan *);
+};
+
+/**
+ * struct mhi_result - Completed buffer information
+ * @buf_addr: Address of data buffer
+ * @dir: Channel direction
+ * @bytes_xfer: # of bytes transferred
+ * @transaction_status: Status of last trasnferred
+ */
+struct mhi_result {
+	void *buf_addr;
+	enum dma_data_direction dir;
+	size_t bytes_xferd;
+	int transaction_status;
+};
+
+/**
+ * struct mhi_buf - Describes the buffer
+ * @page: buffer as a page
+ * @buf: cpu address for the buffer
+ * @phys_addr: physical address of the buffer
+ * @dma_addr: iommu address for the buffer
+ * @skb: skb of ip packet
+ * @len: # of bytes
+ * @name: Buffer label, for offload channel configurations name must be:
+ * ECA - Event context array data
+ * CCA - Channel context array data
+ */
+struct mhi_buf {
+	struct list_head node;
+	struct page *page;
+	void *buf;
+	phys_addr_t phys_addr;
+	dma_addr_t dma_addr;
+	struct sk_buff *skb;
+	size_t len;
+	u16 pkt_num;
+	size_t pkt_len;
+	const char *name; /* ECA, CCA */
+};
+
+/**
+ * struct mhi_driver - mhi driver information
+ * @id_table: NULL terminated channel ID names
+ * @ul_xfer_cb: UL data transfer callback
+ * @dl_xfer_cb: DL data transfer callback
+ * @status_cb: Asynchronous status callback
+ */
+struct mhi_driver {
+	const struct mhi_device_id *id_table;
+	int (*probe)(struct mhi_device *, const struct mhi_device_id *id);
+	void (*remove)(struct mhi_device *);
+	void (*ul_xfer_cb)(struct mhi_device *, struct mhi_result *);
+	void (*dl_xfer_cb)(struct mhi_device *, struct mhi_result *);
+	void (*status_cb)(struct mhi_device *, enum MHI_CB mhi_cb);
+	struct device_driver driver;
+};
+
+#define to_mhi_driver(drv) container_of(drv, struct mhi_driver, driver)
+#define to_mhi_device(dev) container_of(dev, struct mhi_device, dev)
+
+static inline void mhi_device_set_devdata(struct mhi_device *mhi_dev,
+					  void *priv)
+{
+	mhi_dev->priv_data = priv;
+}
+
+static inline void *mhi_device_get_devdata(struct mhi_device *mhi_dev)
+{
+	return mhi_dev->priv_data;
+}
+
+/**
+ * mhi_queue_transfer - Queue a buffer to hardware
+ * All transfers are asyncronous transfers
+ * @mhi_dev: Device associated with the channels
+ * @dir: Data direction
+ * @buf: Data buffer (skb for hardware channels)
+ * @len: Size in bytes
+ * @mflags: Interrupt flags for the device
+ */
+static inline int mhi_queue_transfer(struct mhi_device *mhi_dev,
+				     enum dma_data_direction dir,
+				     void *buf,
+				     size_t len,
+				     enum MHI_FLAGS mflags)
+{
+	if (dir == DMA_TO_DEVICE)
+		return mhi_dev->ul_xfer(mhi_dev, mhi_dev->ul_chan, buf, len,
+					mflags);
+	else
+		return mhi_dev->dl_xfer(mhi_dev, mhi_dev->dl_chan, buf, len,
+					mflags);
+}
+
+/**
+ * mhi_queue_full - check if queue is full
+ * @mhi_dev: Device associated with the channels
+ */
+static inline bool mhi_queue_full(struct mhi_device *mhi_dev)
+{
+	bool ret = false;
+
+	if (mhi_dev->ul_full) {
+
+		ret = mhi_dev->ul_full(mhi_dev, mhi_dev->ul_chan);
+	}
+
+	return ret;
+}
+
+static inline void *mhi_controller_get_devdata(struct mhi_controller *mhi_cntrl)
+{
+	return mhi_cntrl->priv_data;
+}
+
+static inline void mhi_free_controller(struct mhi_controller *mhi_cntrl)
+{
+	kfree(mhi_cntrl);
+}
+
+/**
+ * mhi_driver_register - Register driver with MHI framework
+ * @mhi_drv: mhi_driver structure
+ */
+int mhi_driver_register(struct mhi_driver *mhi_drv);
+
+/**
+ * mhi_driver_unregister - Unregister a driver for mhi_devices
+ * @mhi_drv: mhi_driver structure
+ */
+void mhi_driver_unregister(struct mhi_driver *mhi_drv);
+
+/**
+ * mhi_device_configure - configure ECA or CCA context
+ * For offload channels that client manage, call this
+ * function to configure channel context or event context
+ * array associated with the channel
+ * @mhi_div: Device associated with the channels
+ * @dir: Direction of the channel
+ * @mhi_buf: Configuration data
+ * @elements: # of configuration elements
+ */
+int mhi_device_configure(struct mhi_device *mhi_div,
+			 enum dma_data_direction dir,
+			 struct mhi_buf *mhi_buf,
+			 int elements);
+
+/**
+ * mhi_device_get - disable low power modes
+ * Only disables lpm, does not immediately exit low power mode
+ * if controller already in a low power mode
+ * @mhi_dev: Device associated with the channels
+ * @vote: requested vote (bus, device or both)
+ */
+void mhi_device_get(struct mhi_device *mhi_dev, int vote);
+
+/**
+ * mhi_device_get_sync - disable low power modes
+ * Synchronously disable device & or bus low power, exit low power mode if
+ * controller already in a low power state
+ * @mhi_dev: Device associated with the channels
+ * @vote: requested vote (bus, device or both)
+ */
+int mhi_device_get_sync(struct mhi_device *mhi_dev, int vote);
+
+/**
+ * mhi_device_put - re-enable low power modes
+ * @mhi_dev: Device associated with the channels
+ * @vote: vote to remove
+ */
+void mhi_device_put(struct mhi_device *mhi_dev, int vote);
+
+/**
+ * mhi_prepare_for_transfer - setup channel for data transfer
+ * Moves both UL and DL channel from RESET to START state
+ * @mhi_dev: Device associated with the channels
+ */
+int mhi_prepare_for_transfer(struct mhi_device *mhi_dev);
+
+/**
+ * mhi_unprepare_from_transfer -unprepare the channels
+ * Moves both UL and DL channels to RESET state
+ * @mhi_dev: Device associated with the channels
+ */
+void mhi_unprepare_from_transfer(struct mhi_device *mhi_dev);
+
+/**
+ * mhi_get_no_free_descriptors - Get transfer ring length
+ * Get # of TD available to queue buffers
+ * @mhi_dev: Device associated with the channels
+ * @dir: Direction of the channel
+ */
+int mhi_get_no_free_descriptors(struct mhi_device *mhi_dev,
+				enum dma_data_direction dir);
+
+/**
+ * mhi_poll - poll for any available data to consume
+ * This is only applicable for DL direction
+ * @mhi_dev: Device associated with the channels
+ * @budget: In descriptors to service before returning
+ */
+int mhi_poll(struct mhi_device *mhi_dev, u32 budget);
+
+/**
+ * mhi_ioctl - user space IOCTL support for MHI channels
+ * Native support for setting  TIOCM
+ * @mhi_dev: Device associated with the channels
+ * @cmd: IOCTL cmd
+ * @arg: Optional parameter, iotcl cmd specific
+ */
+long mhi_ioctl(struct mhi_device *mhi_dev, unsigned int cmd, unsigned long arg);
+
+/**
+ * mhi_alloc_controller - Allocate mhi_controller structure
+ * Allocate controller structure and additional data for controller
+ * private data. You may get the private data pointer by calling
+ * mhi_controller_get_devdata
+ * @size: # of additional bytes to allocate
+ */
+struct mhi_controller *mhi_alloc_controller(size_t size);
+
+/**
+ * of_register_mhi_controller - Register MHI controller
+ * Registers MHI controller with MHI bus framework. DT must be supported
+ * @mhi_cntrl: MHI controller to register
+ */
+int of_register_mhi_controller(struct mhi_controller *mhi_cntrl);
+
+void mhi_unregister_mhi_controller(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_bdf_to_controller - Look up a registered controller
+ * Search for controller based on device identification
+ * @domain: RC domain of the device
+ * @bus: Bus device connected to
+ * @slot: Slot device assigned to
+ * @dev_id: Device Identification
+ */
+struct mhi_controller *mhi_bdf_to_controller(u32 domain, u32 bus, u32 slot,
+					     u32 dev_id);
+
+/**
+ * mhi_prepare_for_power_up - Do pre-initialization before power up
+ * This is optional, call this before power up if controller do not
+ * want bus framework to automatically free any allocated memory during shutdown
+ * process.
+ * @mhi_cntrl: MHI controller
+ */
+int mhi_prepare_for_power_up(struct mhi_controller *mhi_cntrl);
+
+int mhi_init_ctrl_seq(struct mhi_controller *mhi_cntrl);
+
+int mhi_deinit_ctrl_seq(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_async_power_up - Starts MHI power up sequence
+ * @mhi_cntrl: MHI controller
+ */
+int mhi_async_power_up(struct mhi_controller *mhi_cntrl);
+int mhi_sync_power_up(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_power_down - Start MHI power down sequence
+ * @mhi_cntrl: MHI controller
+ * @graceful: link is still accessible, do a graceful shutdown process otherwise
+ * we will shutdown host w/o putting device into RESET state
+ */
+void mhi_power_down(struct mhi_controller *mhi_cntrl, bool graceful);
+
+/**
+ * mhi_unprepare_after_powre_down - free any allocated memory for power up
+ * @mhi_cntrl: MHI controller
+ */
+void mhi_unprepare_after_power_down(struct mhi_controller *mhi_cntrl);
+
+int mhi_close_all_channel(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_pm_suspend - Move MHI into a suspended state
+ * Transition to MHI state M3 state from M0||M1||M2 state
+ * @mhi_cntrl: MHI controller
+ */
+int mhi_pm_suspend(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_pm_fast_suspend - Move host into suspend state while keeping
+ * the device in active state.
+ * @mhi_cntrl: MHI controller
+ * @notify_client: if true, clients will get a notification about lpm transition
+ */
+int mhi_pm_fast_suspend(struct mhi_controller *mhi_cntrl, bool notify_client);
+
+/**
+ * mhi_pm_resume - Resume MHI from suspended state
+ * Transition to MHI state M0 state from M3 state
+ * @mhi_cntrl: MHI controller
+ */
+int mhi_pm_resume(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_pm_fast_resume - Move host into resume state from fast suspend state
+ * @mhi_cntrl: MHI controller
+ * @notify_client: if true, clients will get a notification about lpm transition
+ */
+int mhi_pm_fast_resume(struct mhi_controller *mhi_cntrl, bool notify_client);
+
+/**
+ * mhi_download_rddm_img - Download ramdump image from device for
+ * debugging purpose.
+ * @mhi_cntrl: MHI controller
+ * @in_panic: If we trying to capture image while in kernel panic
+ */
+int mhi_download_rddm_img(struct mhi_controller *mhi_cntrl, bool in_panic);
+
+/**
+ * mhi_force_rddm_mode - Force external device into rddm mode
+ * to collect device ramdump. This is useful if host driver assert
+ * and we need to see device state as well.
+ * @mhi_cntrl: MHI controller
+ */
+int mhi_force_rddm_mode(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_get_remote_time_sync - Get external soc time relative to local soc time
+ * using MMIO method.
+ * @mhi_dev: Device associated with the channels
+ * @t_host: Pointer to output local soc time
+ * @t_dev: Pointer to output remote soc time
+ */
+int mhi_get_remote_time_sync(struct mhi_device *mhi_dev,
+			     u64 *t_host,
+			     u64 *t_dev);
+
+/**
+ * mhi_get_mhi_state - Return MHI state of device
+ * @mhi_cntrl: MHI controller
+ */
+enum mhi_dev_state mhi_get_mhi_state(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_set_mhi_state - Set device state
+ * @mhi_cntrl: MHI controller
+ * @state: state to set
+ */
+void mhi_set_mhi_state(struct mhi_controller *mhi_cntrl,
+		       enum mhi_dev_state state);
+
+
+/**
+ * mhi_is_active - helper function to determine if MHI in active state
+ * @mhi_dev: client device
+ */
+static inline bool mhi_is_active(struct mhi_device *mhi_dev)
+{
+	struct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;
+
+	return (mhi_cntrl->dev_state >= MHI_STATE_M0 &&
+		mhi_cntrl->dev_state <= MHI_STATE_M3_FAST);
+}
+
+/**
+ * mhi_control_error - MHI controller went into unrecoverable error state.
+ * Will transition MHI into Linkdown state. Do not call from atomic
+ * context.
+ * @mhi_cntrl: MHI controller
+ */
+void mhi_control_error(struct mhi_controller *mhi_cntrl);
+
+/**
+ * mhi_debug_reg_dump - dump MHI registers for debug purpose
+ * @mhi_cntrl: MHI controller
+ */
+void mhi_debug_reg_dump(struct mhi_controller *mhi_cntrl);
+
+enum mhi_msg_level {
+	MHI_MSG_VERBOSE = 0x0,
+	MHI_MSG_INFO = 0x1,
+	MHI_MSG_DBG = 0x2,
+	MHI_MSG_WARNING = 0x3,
+	MHI_MSG_ERROR = 0x4,
+	MHI_MSG_CRITICAL = 0x5,
+	MHI_MSG_reserved = 0x80000000
+};
+
+extern bool debug;
+extern int debug_level;
+
+#define MHI_VERB(fmt, ...) do { \
+		if (debug && debug_level<= MHI_MSG_VERBOSE) \
+			pr_debug("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+
+#define MHI_LOG(fmt, ...) do {	\
+		if (debug && debug_level<= MHI_MSG_LVL_INFO) \
+			pr_info("[I][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+
+#define MHI_ERR(fmt, ...) do {	\
+		if (debug && debug_level <= MHI_MSG_LVL_ERROR) \
+			pr_err("[E][%s] " fmt, __func__, ##__VA_ARGS__); \
+} while (0)
+
+#define MHI_CRITICAL(fmt, ...) do { \
+		if (debug && debug_level <= MHI_MSG_LVL_CRITICAL) \
+			pr_alert("[C][%s] " fmt, __func__, ##__VA_ARGS__); \
+} while (0)
+
+#define DEFAULT 1
+
+#if DEFAULT
+#define MHI_DEFAULT(fmt, ...) do { \
+			pr_info("[D][%s] " fmt, __func__, ##__VA_ARGS__);\
+} while (0)
+#else
+#define MHI_DEFAULT(fmt, ...)
+#endif
+
+#endif /* _MHI_H_ */
diff --git a/drivers/staging/em9190/inc/mod_devicetable.h b/drivers/staging/em9190/inc/mod_devicetable.h
new file mode 100644
index 000000000000..a08bfd52b360
--- /dev/null
+++ b/drivers/staging/em9190/inc/mod_devicetable.h
@@ -0,0 +1,724 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Device tables which are exported to userspace via
+ * scripts/mod/file2alias.c.  You must keep that file in sync with this
+ * header.
+ */
+
+#ifndef LINUX_MOD_DEVICETABLE_H
+#define LINUX_MOD_DEVICETABLE_H
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <linux/uuid.h>
+typedef unsigned long kernel_ulong_t;
+#endif
+
+#define PCI_ANY_ID (~0)
+
+struct pci_device_id {
+	__u32 vendor, device;		/* Vendor and device ID or PCI_ANY_ID*/
+	__u32 subvendor, subdevice;	/* Subsystem ID's or PCI_ANY_ID */
+	__u32 class, class_mask;	/* (class,subclass,prog-if) triplet */
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+
+#define IEEE1394_MATCH_VENDOR_ID	0x0001
+#define IEEE1394_MATCH_MODEL_ID		0x0002
+#define IEEE1394_MATCH_SPECIFIER_ID	0x0004
+#define IEEE1394_MATCH_VERSION		0x0008
+
+struct ieee1394_device_id {
+	__u32 match_flags;
+	__u32 vendor_id;
+	__u32 model_id;
+	__u32 specifier_id;
+	__u32 version;
+	kernel_ulong_t driver_data;
+};
+
+
+/*
+ * Device table entry for "new style" table-driven USB drivers.
+ * User mode code can read these tables to choose which modules to load.
+ * Declare the table as a MODULE_DEVICE_TABLE.
+ *
+ * A probe() parameter will point to a matching entry from this table.
+ * Use the driver_info field for each match to hold information tied
+ * to that match:  device quirks, etc.
+ *
+ * Terminate the driver's table with an all-zeroes entry.
+ * Use the flag values to control which fields are compared.
+ */
+
+/**
+ * struct usb_device_id - identifies USB devices for probing and hotplugging
+ * @match_flags: Bit mask controlling which of the other fields are used to
+ *	match against new devices. Any field except for driver_info may be
+ *	used, although some only make sense in conjunction with other fields.
+ *	This is usually set by a USB_DEVICE_*() macro, which sets all
+ *	other fields in this structure except for driver_info.
+ * @idVendor: USB vendor ID for a device; numbers are assigned
+ *	by the USB forum to its members.
+ * @idProduct: Vendor-assigned product ID.
+ * @bcdDevice_lo: Low end of range of vendor-assigned product version numbers.
+ *	This is also used to identify individual product versions, for
+ *	a range consisting of a single device.
+ * @bcdDevice_hi: High end of version number range.  The range of product
+ *	versions is inclusive.
+ * @bDeviceClass: Class of device; numbers are assigned
+ *	by the USB forum.  Products may choose to implement classes,
+ *	or be vendor-specific.  Device classes specify behavior of all
+ *	the interfaces on a device.
+ * @bDeviceSubClass: Subclass of device; associated with bDeviceClass.
+ * @bDeviceProtocol: Protocol of device; associated with bDeviceClass.
+ * @bInterfaceClass: Class of interface; numbers are assigned
+ *	by the USB forum.  Products may choose to implement classes,
+ *	or be vendor-specific.  Interface classes specify behavior only
+ *	of a given interface; other interfaces may support other classes.
+ * @bInterfaceSubClass: Subclass of interface; associated with bInterfaceClass.
+ * @bInterfaceProtocol: Protocol of interface; associated with bInterfaceClass.
+ * @bInterfaceNumber: Number of interface; composite devices may use
+ *	fixed interface numbers to differentiate between vendor-specific
+ *	interfaces.
+ * @driver_info: Holds information used by the driver.  Usually it holds
+ *	a pointer to a descriptor understood by the driver, or perhaps
+ *	device flags.
+ *
+ * In most cases, drivers will create a table of device IDs by using
+ * USB_DEVICE(), or similar macros designed for that purpose.
+ * They will then export it to userspace using MODULE_DEVICE_TABLE(),
+ * and provide it to the USB core through their usb_driver structure.
+ *
+ * See the usb_match_id() function for information about how matches are
+ * performed.  Briefly, you will normally use one of several macros to help
+ * construct these entries.  Each entry you provide will either identify
+ * one or more specific products, or will identify a class of products
+ * which have agreed to behave the same.  You should put the more specific
+ * matches towards the beginning of your table, so that driver_info can
+ * record quirks of specific products.
+ */
+struct usb_device_id {
+	/* which fields to match against? */
+	__u16		match_flags;
+
+	/* Used for product specific matches; range is inclusive */
+	__u16		idVendor;
+	__u16		idProduct;
+	__u16		bcdDevice_lo;
+	__u16		bcdDevice_hi;
+
+	/* Used for device class matches */
+	__u8		bDeviceClass;
+	__u8		bDeviceSubClass;
+	__u8		bDeviceProtocol;
+
+	/* Used for interface class matches */
+	__u8		bInterfaceClass;
+	__u8		bInterfaceSubClass;
+	__u8		bInterfaceProtocol;
+
+	/* Used for vendor-specific interface matches */
+	__u8		bInterfaceNumber;
+
+	/* not matched against */
+	kernel_ulong_t	driver_info
+		__attribute__((aligned(sizeof(kernel_ulong_t))));
+};
+
+/* Some useful macros to use to create struct usb_device_id */
+#define USB_DEVICE_ID_MATCH_VENDOR		0x0001
+#define USB_DEVICE_ID_MATCH_PRODUCT		0x0002
+#define USB_DEVICE_ID_MATCH_DEV_LO		0x0004
+#define USB_DEVICE_ID_MATCH_DEV_HI		0x0008
+#define USB_DEVICE_ID_MATCH_DEV_CLASS		0x0010
+#define USB_DEVICE_ID_MATCH_DEV_SUBCLASS	0x0020
+#define USB_DEVICE_ID_MATCH_DEV_PROTOCOL	0x0040
+#define USB_DEVICE_ID_MATCH_INT_CLASS		0x0080
+#define USB_DEVICE_ID_MATCH_INT_SUBCLASS	0x0100
+#define USB_DEVICE_ID_MATCH_INT_PROTOCOL	0x0200
+#define USB_DEVICE_ID_MATCH_INT_NUMBER		0x0400
+
+#define HID_ANY_ID				(~0)
+#define HID_BUS_ANY				0xffff
+#define HID_GROUP_ANY				0x0000
+
+struct hid_device_id {
+	__u16 bus;
+	__u16 group;
+	__u32 vendor;
+	__u32 product;
+	kernel_ulong_t driver_data;
+};
+
+/* s390 CCW devices */
+struct ccw_device_id {
+	__u16	match_flags;	/* which fields to match against */
+
+	__u16	cu_type;	/* control unit type     */
+	__u16	dev_type;	/* device type           */
+	__u8	cu_model;	/* control unit model    */
+	__u8	dev_model;	/* device model          */
+
+	kernel_ulong_t driver_info;
+};
+
+#define CCW_DEVICE_ID_MATCH_CU_TYPE		0x01
+#define CCW_DEVICE_ID_MATCH_CU_MODEL		0x02
+#define CCW_DEVICE_ID_MATCH_DEVICE_TYPE		0x04
+#define CCW_DEVICE_ID_MATCH_DEVICE_MODEL	0x08
+
+/* s390 AP bus devices */
+struct ap_device_id {
+	__u16 match_flags;	/* which fields to match against */
+	__u8 dev_type;		/* device type */
+	kernel_ulong_t driver_info;
+};
+
+#define AP_DEVICE_ID_MATCH_CARD_TYPE		0x01
+#define AP_DEVICE_ID_MATCH_QUEUE_TYPE		0x02
+
+/* s390 css bus devices (subchannels) */
+struct css_device_id {
+	__u8 match_flags;
+	__u8 type; /* subchannel type */
+	kernel_ulong_t driver_data;
+};
+
+#define ACPI_ID_LEN	9
+
+struct acpi_device_id {
+	__u8 id[ACPI_ID_LEN];
+	kernel_ulong_t driver_data;
+	__u32 cls;
+	__u32 cls_msk;
+};
+
+#define PNP_ID_LEN	8
+#define PNP_MAX_DEVICES	8
+
+struct pnp_device_id {
+	__u8 id[PNP_ID_LEN];
+	kernel_ulong_t driver_data;
+};
+
+struct pnp_card_device_id {
+	__u8 id[PNP_ID_LEN];
+	kernel_ulong_t driver_data;
+	struct {
+		__u8 id[PNP_ID_LEN];
+	} devs[PNP_MAX_DEVICES];
+};
+
+
+#define SERIO_ANY	0xff
+
+struct serio_device_id {
+	__u8 type;
+	__u8 extra;
+	__u8 id;
+	__u8 proto;
+};
+
+struct hda_device_id {
+	__u32 vendor_id;
+	__u32 rev_id;
+	__u8 api_version;
+	const char *name;
+	unsigned long driver_data;
+};
+
+/*
+ * Struct used for matching a device
+ */
+struct of_device_id {
+	char	name[32];
+	char	type[32];
+	char	compatible[128];
+	const void *data;
+};
+
+/* VIO */
+struct vio_device_id {
+	char type[32];
+	char compat[32];
+};
+
+/* PCMCIA */
+
+struct pcmcia_device_id {
+	__u16		match_flags;
+
+	__u16		manf_id;
+	__u16 		card_id;
+
+	__u8  		func_id;
+
+	/* for real multi-function devices */
+	__u8  		function;
+
+	/* for pseudo multi-function devices */
+	__u8  		device_no;
+
+	__u32 		prod_id_hash[4];
+
+	/* not matched against in kernelspace */
+	const char *	prod_id[4];
+
+	/* not matched against */
+	kernel_ulong_t	driver_info;
+	char *		cisfile;
+};
+
+#define PCMCIA_DEV_ID_MATCH_MANF_ID	0x0001
+#define PCMCIA_DEV_ID_MATCH_CARD_ID	0x0002
+#define PCMCIA_DEV_ID_MATCH_FUNC_ID	0x0004
+#define PCMCIA_DEV_ID_MATCH_FUNCTION	0x0008
+#define PCMCIA_DEV_ID_MATCH_PROD_ID1	0x0010
+#define PCMCIA_DEV_ID_MATCH_PROD_ID2	0x0020
+#define PCMCIA_DEV_ID_MATCH_PROD_ID3	0x0040
+#define PCMCIA_DEV_ID_MATCH_PROD_ID4	0x0080
+#define PCMCIA_DEV_ID_MATCH_DEVICE_NO	0x0100
+#define PCMCIA_DEV_ID_MATCH_FAKE_CIS	0x0200
+#define PCMCIA_DEV_ID_MATCH_ANONYMOUS	0x0400
+
+/* Input */
+#define INPUT_DEVICE_ID_EV_MAX		0x1f
+#define INPUT_DEVICE_ID_KEY_MIN_INTERESTING	0x71
+#define INPUT_DEVICE_ID_KEY_MAX		0x2ff
+#define INPUT_DEVICE_ID_REL_MAX		0x0f
+#define INPUT_DEVICE_ID_ABS_MAX		0x3f
+#define INPUT_DEVICE_ID_MSC_MAX		0x07
+#define INPUT_DEVICE_ID_LED_MAX		0x0f
+#define INPUT_DEVICE_ID_SND_MAX		0x07
+#define INPUT_DEVICE_ID_FF_MAX		0x7f
+#define INPUT_DEVICE_ID_SW_MAX		0x20
+#define INPUT_DEVICE_ID_PROP_MAX	0x1f
+
+#define INPUT_DEVICE_ID_MATCH_BUS	1
+#define INPUT_DEVICE_ID_MATCH_VENDOR	2
+#define INPUT_DEVICE_ID_MATCH_PRODUCT	4
+#define INPUT_DEVICE_ID_MATCH_VERSION	8
+
+#define INPUT_DEVICE_ID_MATCH_EVBIT	0x0010
+#define INPUT_DEVICE_ID_MATCH_KEYBIT	0x0020
+#define INPUT_DEVICE_ID_MATCH_RELBIT	0x0040
+#define INPUT_DEVICE_ID_MATCH_ABSBIT	0x0080
+#define INPUT_DEVICE_ID_MATCH_MSCIT	0x0100
+#define INPUT_DEVICE_ID_MATCH_LEDBIT	0x0200
+#define INPUT_DEVICE_ID_MATCH_SNDBIT	0x0400
+#define INPUT_DEVICE_ID_MATCH_FFBIT	0x0800
+#define INPUT_DEVICE_ID_MATCH_SWBIT	0x1000
+#define INPUT_DEVICE_ID_MATCH_PROPBIT	0x2000
+
+struct input_device_id {
+
+	kernel_ulong_t flags;
+
+	__u16 bustype;
+	__u16 vendor;
+	__u16 product;
+	__u16 version;
+
+	kernel_ulong_t evbit[INPUT_DEVICE_ID_EV_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t keybit[INPUT_DEVICE_ID_KEY_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t relbit[INPUT_DEVICE_ID_REL_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t absbit[INPUT_DEVICE_ID_ABS_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t mscbit[INPUT_DEVICE_ID_MSC_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t ledbit[INPUT_DEVICE_ID_LED_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t sndbit[INPUT_DEVICE_ID_SND_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t ffbit[INPUT_DEVICE_ID_FF_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t swbit[INPUT_DEVICE_ID_SW_MAX / BITS_PER_LONG + 1];
+	kernel_ulong_t propbit[INPUT_DEVICE_ID_PROP_MAX / BITS_PER_LONG + 1];
+
+	kernel_ulong_t driver_info;
+};
+
+/* EISA */
+
+#define EISA_SIG_LEN   8
+
+/* The EISA signature, in ASCII form, null terminated */
+struct eisa_device_id {
+	char          sig[EISA_SIG_LEN];
+	kernel_ulong_t driver_data;
+};
+
+#define EISA_DEVICE_MODALIAS_FMT "eisa:s%s"
+
+struct parisc_device_id {
+	__u8	hw_type;	/* 5 bits used */
+	__u8	hversion_rev;	/* 4 bits */
+	__u16	hversion;	/* 12 bits */
+	__u32	sversion;	/* 20 bits */
+};
+
+#define PA_HWTYPE_ANY_ID	0xff
+#define PA_HVERSION_REV_ANY_ID	0xff
+#define PA_HVERSION_ANY_ID	0xffff
+#define PA_SVERSION_ANY_ID	0xffffffff
+
+/* SDIO */
+
+#define SDIO_ANY_ID (~0)
+
+struct sdio_device_id {
+	__u8	class;			/* Standard interface or SDIO_ANY_ID */
+	__u16	vendor;			/* Vendor or SDIO_ANY_ID */
+	__u16	device;			/* Device ID or SDIO_ANY_ID */
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+/* SSB core, see drivers/ssb/ */
+struct ssb_device_id {
+	__u16	vendor;
+	__u16	coreid;
+	__u8	revision;
+	__u8	__pad;
+} __attribute__((packed, aligned(2)));
+#define SSB_DEVICE(_vendor, _coreid, _revision)  \
+	{ .vendor = _vendor, .coreid = _coreid, .revision = _revision, }
+
+#define SSB_ANY_VENDOR		0xFFFF
+#define SSB_ANY_ID		0xFFFF
+#define SSB_ANY_REV		0xFF
+
+/* Broadcom's specific AMBA core, see drivers/bcma/ */
+struct bcma_device_id {
+	__u16	manuf;
+	__u16	id;
+	__u8	rev;
+	__u8	class;
+} __attribute__((packed,aligned(2)));
+#define BCMA_CORE(_manuf, _id, _rev, _class)  \
+	{ .manuf = _manuf, .id = _id, .rev = _rev, .class = _class, }
+
+#define BCMA_ANY_MANUF		0xFFFF
+#define BCMA_ANY_ID		0xFFFF
+#define BCMA_ANY_REV		0xFF
+#define BCMA_ANY_CLASS		0xFF
+
+struct virtio_device_id {
+	__u32 device;
+	__u32 vendor;
+};
+#define VIRTIO_DEV_ANY_ID	0xffffffff
+
+/*
+ * For Hyper-V devices we use the device guid as the id.
+ */
+struct hv_vmbus_device_id {
+	uuid_le guid;
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+/* rpmsg */
+
+#define RPMSG_NAME_SIZE			32
+#define RPMSG_DEVICE_MODALIAS_FMT	"rpmsg:%s"
+
+struct rpmsg_device_id {
+	char name[RPMSG_NAME_SIZE];
+};
+
+/* i2c */
+
+#define I2C_NAME_SIZE	20
+#define I2C_MODULE_PREFIX "i2c:"
+
+struct i2c_device_id {
+	char name[I2C_NAME_SIZE];
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+/* pci_epf */
+
+#define PCI_EPF_NAME_SIZE	20
+#define PCI_EPF_MODULE_PREFIX	"pci_epf:"
+
+struct pci_epf_device_id {
+	char name[PCI_EPF_NAME_SIZE];
+	kernel_ulong_t driver_data;
+};
+
+/* spi */
+
+#define SPI_NAME_SIZE	32
+#define SPI_MODULE_PREFIX "spi:"
+
+struct spi_device_id {
+	char name[SPI_NAME_SIZE];
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+#define SPMI_NAME_SIZE	32
+#define SPMI_MODULE_PREFIX "spmi:"
+
+struct spmi_device_id {
+	char name[SPMI_NAME_SIZE];
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+/* soundwire */
+
+#define SOUNDWIRE_NAME_SIZE	32
+#define SOUNDWIRE_MODULE_PREFIX "swr:"
+
+struct swr_device_id {
+	char name[SOUNDWIRE_NAME_SIZE];
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+/* dmi */
+enum dmi_field {
+	DMI_NONE,
+	DMI_BIOS_VENDOR,
+	DMI_BIOS_VERSION,
+	DMI_BIOS_DATE,
+	DMI_SYS_VENDOR,
+	DMI_PRODUCT_NAME,
+	DMI_PRODUCT_VERSION,
+	DMI_PRODUCT_SERIAL,
+	DMI_PRODUCT_UUID,
+	DMI_PRODUCT_FAMILY,
+	DMI_BOARD_VENDOR,
+	DMI_BOARD_NAME,
+	DMI_BOARD_VERSION,
+	DMI_BOARD_SERIAL,
+	DMI_BOARD_ASSET_TAG,
+	DMI_CHASSIS_VENDOR,
+	DMI_CHASSIS_TYPE,
+	DMI_CHASSIS_VERSION,
+	DMI_CHASSIS_SERIAL,
+	DMI_CHASSIS_ASSET_TAG,
+	DMI_STRING_MAX,
+};
+
+struct dmi_strmatch {
+	unsigned char slot:7;
+	unsigned char exact_match:1;
+	char substr[79];
+};
+
+struct dmi_system_id {
+	int (*callback)(const struct dmi_system_id *);
+	const char *ident;
+	struct dmi_strmatch matches[4];
+	void *driver_data;
+};
+/*
+ * struct dmi_device_id appears during expansion of
+ * "MODULE_DEVICE_TABLE(dmi, x)". Compiler doesn't look inside it
+ * but this is enough for gcc 3.4.6 to error out:
+ *	error: storage size of '__mod_dmi_device_table' isn't known
+ */
+#define dmi_device_id dmi_system_id
+
+#define DMI_MATCH(a, b)	{ .slot = a, .substr = b }
+#define DMI_EXACT_MATCH(a, b)	{ .slot = a, .substr = b, .exact_match = 1 }
+
+#define SLIMBUS_NAME_SIZE	32
+#define SLIMBUS_MODULE_PREFIX "slim:"
+
+struct slim_device_id {
+	char name[SLIMBUS_NAME_SIZE];
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+#define PLATFORM_NAME_SIZE	20
+#define PLATFORM_MODULE_PREFIX	"platform:"
+
+struct platform_device_id {
+	char name[PLATFORM_NAME_SIZE];
+	kernel_ulong_t driver_data;
+};
+
+#define MDIO_NAME_SIZE		32
+#define MDIO_MODULE_PREFIX	"mdio:"
+
+#define MDIO_ID_FMT "%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d"
+#define MDIO_ID_ARGS(_id) \
+	(_id)>>31, ((_id)>>30) & 1, ((_id)>>29) & 1, ((_id)>>28) & 1,	\
+	((_id)>>27) & 1, ((_id)>>26) & 1, ((_id)>>25) & 1, ((_id)>>24) & 1, \
+	((_id)>>23) & 1, ((_id)>>22) & 1, ((_id)>>21) & 1, ((_id)>>20) & 1, \
+	((_id)>>19) & 1, ((_id)>>18) & 1, ((_id)>>17) & 1, ((_id)>>16) & 1, \
+	((_id)>>15) & 1, ((_id)>>14) & 1, ((_id)>>13) & 1, ((_id)>>12) & 1, \
+	((_id)>>11) & 1, ((_id)>>10) & 1, ((_id)>>9) & 1, ((_id)>>8) & 1, \
+	((_id)>>7) & 1, ((_id)>>6) & 1, ((_id)>>5) & 1, ((_id)>>4) & 1, \
+	((_id)>>3) & 1, ((_id)>>2) & 1, ((_id)>>1) & 1, (_id) & 1
+
+/**
+ * struct mdio_device_id - identifies PHY devices on an MDIO/MII bus
+ * @phy_id: The result of
+ *     (mdio_read(&MII_PHYSID1) << 16 | mdio_read(&PHYSID2)) & @phy_id_mask
+ *     for this PHY type
+ * @phy_id_mask: Defines the significant bits of @phy_id.  A value of 0
+ *     is used to terminate an array of struct mdio_device_id.
+ */
+struct mdio_device_id {
+	__u32 phy_id;
+	__u32 phy_id_mask;
+};
+
+struct zorro_device_id {
+	__u32 id;			/* Device ID or ZORRO_WILDCARD */
+	kernel_ulong_t driver_data;	/* Data private to the driver */
+};
+
+#define ZORRO_WILDCARD			(0xffffffff)	/* not official */
+
+#define ZORRO_DEVICE_MODALIAS_FMT	"zorro:i%08X"
+
+#define ISAPNP_ANY_ID		0xffff
+struct isapnp_device_id {
+	unsigned short card_vendor, card_device;
+	unsigned short vendor, function;
+	kernel_ulong_t driver_data;	/* data private to the driver */
+};
+
+/**
+ * struct amba_id - identifies a device on an AMBA bus
+ * @id: The significant bits if the hardware device ID
+ * @mask: Bitmask specifying which bits of the id field are significant when
+ *	matching.  A driver binds to a device when ((hardware device ID) & mask)
+ *	== id.
+ * @data: Private data used by the driver.
+ */
+struct amba_id {
+	unsigned int		id;
+	unsigned int		mask;
+	void			*data;
+};
+
+/**
+ * struct mips_cdmm_device_id - identifies devices in MIPS CDMM bus
+ * @type:	Device type identifier.
+ */
+struct mips_cdmm_device_id {
+	__u8	type;
+};
+
+/*
+ * Match x86 CPUs for CPU specific drivers.
+ * See documentation of "x86_match_cpu" for details.
+ */
+
+/*
+ * MODULE_DEVICE_TABLE expects this struct to be called x86cpu_device_id.
+ * Although gcc seems to ignore this error, clang fails without this define.
+ */
+#define x86cpu_device_id x86_cpu_id
+struct x86_cpu_id {
+	__u16 vendor;
+	__u16 family;
+	__u16 model;
+	__u16 feature;	/* bit index */
+	kernel_ulong_t driver_data;
+};
+
+#define X86_FEATURE_MATCH(x) \
+	{ X86_VENDOR_ANY, X86_FAMILY_ANY, X86_MODEL_ANY, x }
+
+#define X86_VENDOR_ANY 0xffff
+#define X86_FAMILY_ANY 0
+#define X86_MODEL_ANY  0
+#define X86_FEATURE_ANY 0	/* Same as FPU, you can't test for that */
+
+/*
+ * Generic table type for matching CPU features.
+ * @feature:	the bit number of the feature (0 - 65535)
+ */
+
+struct cpu_feature {
+	__u16	feature;
+};
+
+#define IPACK_ANY_FORMAT 0xff
+#define IPACK_ANY_ID (~0)
+struct ipack_device_id {
+	__u8  format;			/* Format version or IPACK_ANY_ID */
+	__u32 vendor;			/* Vendor ID or IPACK_ANY_ID */
+	__u32 device;			/* Device ID or IPACK_ANY_ID */
+};
+
+#define MEI_CL_MODULE_PREFIX "mei:"
+#define MEI_CL_NAME_SIZE 32
+#define MEI_CL_VERSION_ANY 0xff
+
+/**
+ * struct mei_cl_device_id - MEI client device identifier
+ * @name: helper name
+ * @uuid: client uuid
+ * @version: client protocol version
+ * @driver_info: information used by the driver.
+ *
+ * identifies mei client device by uuid and name
+ */
+struct mei_cl_device_id {
+	char name[MEI_CL_NAME_SIZE];
+	uuid_le uuid;
+	__u8    version;
+	kernel_ulong_t driver_info;
+};
+
+/* RapidIO */
+
+#define RIO_ANY_ID	0xffff
+
+/**
+ * struct rio_device_id - RIO device identifier
+ * @did: RapidIO device ID
+ * @vid: RapidIO vendor ID
+ * @asm_did: RapidIO assembly device ID
+ * @asm_vid: RapidIO assembly vendor ID
+ *
+ * Identifies a RapidIO device based on both the device/vendor IDs and
+ * the assembly device/vendor IDs.
+ */
+struct rio_device_id {
+	__u16 did, vid;
+	__u16 asm_did, asm_vid;
+};
+
+struct mcb_device_id {
+	__u16 device;
+	kernel_ulong_t driver_data;
+};
+
+struct ulpi_device_id {
+	__u16 vendor;
+	__u16 product;
+	kernel_ulong_t driver_data;
+};
+
+/**
+ * struct fsl_mc_device_id - MC object device identifier
+ * @vendor: vendor ID
+ * @obj_type: MC object type
+ *
+ * Type of entries in the "device Id" table for MC object devices supported by
+ * a MC object device driver. The last entry of the table has vendor set to 0x0
+ */
+struct fsl_mc_device_id {
+	__u16 vendor;
+	const char obj_type[16];
+};
+
+#define MHI_NAME_SIZE 32
+
+/**
+ * struct mhi_device_id - MHI device identification
+ * @chan: MHI channel name
+ * @driver_data: driver data;
+ */
+
+struct mhi_device_id {
+	const char chan[MHI_NAME_SIZE];
+	kernel_ulong_t driver_data;
+};
+
+
+
+
+#endif /* LINUX_MOD_DEVICETABLE_H */
diff --git a/drivers/staging/em9190/inc/msm-bus.h b/drivers/staging/em9190/inc/msm-bus.h
new file mode 100644
index 000000000000..0946f546f715
--- /dev/null
+++ b/drivers/staging/em9190/inc/msm-bus.h
@@ -0,0 +1,262 @@
+/* Copyright (c) 2010-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _ARCH_ARM_MACH_MSM_BUS_H
+#define _ARCH_ARM_MACH_MSM_BUS_H
+
+#include <linux/types.h>
+#include <linux/input.h>
+#include <linux/platform_device.h>
+//#include <soc/qcom/tcs.h>
+
+/*
+ * Macros for clients to convert their data to ib and ab
+ * Ws : Time window over which to transfer the data in SECONDS
+ * Bs : Size of the data block in bytes
+ * Per : Recurrence period
+ * Tb : Throughput bandwidth to prevent stalling
+ * R  : Ratio of actual bandwidth used to Tb
+ * Ib : Instantaneous bandwidth
+ * Ab : Arbitrated bandwidth
+ *
+ * IB_RECURRBLOCK and AB_RECURRBLOCK:
+ * These are used if the requirement is to transfer a
+ * recurring block of data over a known time window.
+ *
+ * IB_THROUGHPUTBW and AB_THROUGHPUTBW:
+ * These are used for CPU style masters. Here the requirement
+ * is to have minimum throughput bandwidth available to avoid
+ * stalling.
+ */
+#define IB_RECURRBLOCK(Ws, Bs) ((Ws) == 0 ? 0 : ((Bs)/(Ws)))
+#define AB_RECURRBLOCK(Ws, Per) ((Ws) == 0 ? 0 : ((Bs)/(Per)))
+#define IB_THROUGHPUTBW(Tb) (Tb)
+#define AB_THROUGHPUTBW(Tb, R) ((Tb) * (R))
+#define MSM_BUS_MAX_TCS_CMDS 16
+
+struct msm_bus_vectors {
+	int src; /* Master */
+	int dst; /* Slave */
+	uint64_t ab; /* Arbitrated bandwidth */
+	uint64_t ib; /* Instantaneous bandwidth */
+};
+
+struct msm_bus_paths {
+	int num_paths;
+	struct msm_bus_vectors *vectors;
+};
+
+struct msm_bus_lat_vectors {
+	uint64_t fal_ns; /* First Access Latency */
+	uint64_t idle_t_ns; /* Idle Time */
+};
+
+struct msm_bus_scale_pdata {
+	struct msm_bus_paths *usecase;
+	struct msm_bus_lat_vectors *usecase_lat;
+	int num_usecases;
+	const char *name;
+	/*
+	 * If the active_only flag is set to 1, the BW request is applied
+	 * only when at least one CPU is active (powered on). If the flag
+	 * is set to 0, then the BW request is always applied irrespective
+	 * of the CPU state.
+	 */
+	unsigned int active_only;
+	/*
+	 * If the ALC(Active Latency Client) flag is set to 1,
+	 * use lat_usecases for latency voting.
+	 */
+	unsigned int alc;
+};
+
+struct msm_bus_client_handle {
+	char *name;
+	int mas;
+	int slv;
+	int first_hop;
+	struct device *mas_dev;
+	u64 cur_act_ib;
+	u64 cur_act_ab;
+	u64 cur_dual_ib;
+	u64 cur_dual_ab;
+	bool active_only;
+};
+
+struct msm_bus_tcs_usecase {
+	int num_cmds;
+	//struct tcs_cmd cmds[MSM_BUS_MAX_TCS_CMDS];
+};
+
+struct msm_bus_tcs_handle {
+	int num_usecases;
+	struct msm_bus_tcs_usecase *usecases;
+};
+
+/* Scaling APIs */
+
+/*
+ * This function returns a handle to the client. This should be used to
+ * call msm_bus_scale_client_update_request.
+ * The function returns 0 if bus driver is unable to register a client
+ */
+
+#if (defined(CONFIG_QCOM_BUS_SCALING) ||\
+	defined(CONFIG_QCOM_BUS_TOPOLOGY_ADHOC))
+int __init msm_bus_fabric_init_driver(void);
+uint32_t msm_bus_scale_register_client(struct msm_bus_scale_pdata *pdata);
+int msm_bus_scale_client_update_request(uint32_t cl, unsigned int index);
+void msm_bus_scale_unregister_client(uint32_t cl);
+int msm_bus_scale_client_update_context(uint32_t cl, bool active_only,
+							unsigned int ctx_idx);
+
+struct msm_bus_client_handle*
+msm_bus_scale_register(uint32_t mas, uint32_t slv, char *name,
+							bool active_only);
+void msm_bus_scale_unregister(struct msm_bus_client_handle *cl);
+int msm_bus_scale_update_bw(struct msm_bus_client_handle *cl, u64 ab, u64 ib);
+int msm_bus_scale_update_bw_context(struct msm_bus_client_handle *cl,
+		u64 act_ab, u64 act_ib, u64 dual_ib, u64 dual_ab);
+int msm_bus_scale_query_tcs_cmd(struct msm_bus_tcs_usecase *tcs_usecase,
+					uint32_t cl, unsigned int index);
+int msm_bus_scale_query_tcs_cmd_all(struct msm_bus_tcs_handle *tcs_handle,
+					uint32_t cl);
+
+/* AXI Port configuration APIs */
+int msm_bus_axi_porthalt(int master_port);
+int msm_bus_axi_portunhalt(int master_port);
+
+#else
+static inline int __init msm_bus_fabric_init_driver(void) { return 0; }
+static struct msm_bus_client_handle dummy_cl;
+
+static inline uint32_t
+msm_bus_scale_register_client(struct msm_bus_scale_pdata *pdata)
+{
+	return 1;
+}
+
+static inline int
+msm_bus_scale_client_update_request(uint32_t cl, unsigned int index)
+{
+	return 0;
+}
+
+static inline int
+msm_bus_scale_client_update_context(uint32_t cl, bool active_only,
+							unsigned int ctx_idx)
+{
+	return 0;
+}
+
+static inline void
+msm_bus_scale_unregister_client(uint32_t cl)
+{
+}
+
+static inline int msm_bus_axi_porthalt(int master_port)
+{
+	return 0;
+}
+
+static inline int msm_bus_axi_portunhalt(int master_port)
+{
+	return 0;
+}
+
+static inline struct msm_bus_client_handle*
+msm_bus_scale_register(uint32_t mas, uint32_t slv, char *name,
+							bool active_only)
+{
+	return &dummy_cl;
+}
+
+static inline void msm_bus_scale_unregister(struct msm_bus_client_handle *cl)
+{
+}
+
+static inline int
+msm_bus_scale_update_bw(struct msm_bus_client_handle *cl, u64 ab, u64 ib)
+{
+	return 0;
+}
+
+static inline int
+msm_bus_scale_update_bw_context(struct msm_bus_client_handle *cl, u64 act_ab,
+				u64 act_ib, u64 dual_ib, u64 dual_ab)
+
+{
+	return 0;
+}
+
+static inline int msm_bus_scale_query_tcs_cmd(struct msm_bus_tcs_usecase
+						*tcs_usecase, uint32_t cl,
+						unsigned int index)
+{
+	return 0;
+}
+
+static inline int msm_bus_scale_query_tcs_cmd_all(struct msm_bus_tcs_handle
+						*tcs_handle, uint32_t cl)
+{
+	return 0;
+}
+
+#endif
+
+#if defined(CONFIG_OF) && defined(CONFIG_QCOM_BUS_SCALING)
+struct msm_bus_scale_pdata *msm_bus_pdata_from_node(
+		struct platform_device *pdev, struct device_node *of_node);
+struct msm_bus_scale_pdata *msm_bus_cl_get_pdata(struct platform_device *pdev);
+struct msm_bus_scale_pdata *msm_bus_cl_get_pdata_from_dev(struct device *dev);
+void msm_bus_cl_clear_pdata(struct msm_bus_scale_pdata *pdata);
+#else
+static inline struct msm_bus_scale_pdata
+*msm_bus_cl_get_pdata(struct platform_device *pdev)
+{
+	return NULL;
+}
+
+static inline struct msm_bus_scale_pdata
+*msm_bus_cl_get_pdata_from_dev(struct device *dev)
+{
+	return NULL;
+}
+
+static inline struct msm_bus_scale_pdata *msm_bus_pdata_from_node(
+		struct platform_device *pdev, struct device_node *of_node)
+{
+	return NULL;
+}
+
+static inline void msm_bus_cl_clear_pdata(struct msm_bus_scale_pdata *pdata)
+{
+}
+#endif
+
+#ifdef CONFIG_DEBUG_BUS_VOTER
+int msm_bus_floor_vote_context(const char *name, u64 floor_hz,
+						bool active_only);
+int msm_bus_floor_vote(const char *name, u64 floor_hz);
+#else
+static inline int msm_bus_floor_vote(const char *name, u64 floor_hz)
+{
+	return -EINVAL;
+}
+
+static inline int msm_bus_floor_vote_context(const char *name, u64 floor_hz,
+						bool active_only)
+{
+	return -EINVAL;
+}
+#endif /*defined(CONFIG_DEBUG_BUS_VOTER) && defined(CONFIG_BUS_TOPOLOGY_ADHOC)*/
+#endif /*_ARCH_ARM_MACH_MSM_BUS_H*/
diff --git a/drivers/staging/em9190/inc/msm_ep_pcie.h b/drivers/staging/em9190/inc/msm_ep_pcie.h
new file mode 100644
index 000000000000..720a0811f85c
--- /dev/null
+++ b/drivers/staging/em9190/inc/msm_ep_pcie.h
@@ -0,0 +1,291 @@
+/* Copyright (c) 2015, 2017, 2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __MSM_EP_PCIE_H
+#define __MSM_EP_PCIE_H
+
+#include <linux/types.h>
+
+enum ep_pcie_link_status {
+	EP_PCIE_LINK_DISABLED,
+	EP_PCIE_LINK_UP,
+	EP_PCIE_LINK_ENABLED,
+};
+
+enum ep_pcie_event {
+	EP_PCIE_EVENT_INVALID = 0,
+	EP_PCIE_EVENT_PM_D0 = 0x1,
+	EP_PCIE_EVENT_PM_D3_HOT = 0x2,
+	EP_PCIE_EVENT_PM_D3_COLD = 0x4,
+	EP_PCIE_EVENT_PM_RST_DEAST = 0x8,
+	EP_PCIE_EVENT_LINKDOWN = 0x10,
+	EP_PCIE_EVENT_LINKUP = 0x20,
+	EP_PCIE_EVENT_MHI_A7 = 0x40,
+	EP_PCIE_EVENT_MMIO_WRITE = 0x80,
+};
+
+enum ep_pcie_irq_event {
+	EP_PCIE_INT_EVT_LINK_DOWN = 1,
+	EP_PCIE_INT_EVT_BME,
+	EP_PCIE_INT_EVT_PM_TURNOFF,
+	EP_PCIE_INT_EVT_DEBUG,
+	EP_PCIE_INT_EVT_LTR,
+	EP_PCIE_INT_EVT_MHI_Q6,
+	EP_PCIE_INT_EVT_MHI_A7,
+	EP_PCIE_INT_EVT_DSTATE_CHANGE,
+	EP_PCIE_INT_EVT_L1SUB_TIMEOUT,
+	EP_PCIE_INT_EVT_MMIO_WRITE,
+	EP_PCIE_INT_EVT_CFG_WRITE,
+	EP_PCIE_INT_EVT_BRIDGE_FLUSH_N,
+	EP_PCIE_INT_EVT_LINK_UP,
+	EP_PCIE_INT_EVT_EDMA = 22,
+	EP_PCIE_INT_EVT_MAX = 13,
+};
+
+enum ep_pcie_trigger {
+	EP_PCIE_TRIGGER_CALLBACK,
+	EP_PCIE_TRIGGER_COMPLETION,
+};
+
+enum ep_pcie_options {
+	EP_PCIE_OPT_NULL = 0,
+	EP_PCIE_OPT_AST_WAKE = 0x1,
+	EP_PCIE_OPT_POWER_ON = 0x2,
+	EP_PCIE_OPT_ENUM = 0x4,
+	EP_PCIE_OPT_ENUM_ASYNC = 0x8,
+	EP_PCIE_OPT_ALL = 0xFFFFFFFF,
+};
+
+struct ep_pcie_notify {
+	enum ep_pcie_event event;
+	void *user;
+	void *data;
+	u32 options;
+};
+
+struct ep_pcie_register_event {
+	u32 events;
+	void *user;
+	enum ep_pcie_trigger mode;
+	void (*callback)(struct ep_pcie_notify *notify);
+	struct ep_pcie_notify notify;
+	struct completion *completion;
+	u32 options;
+};
+
+struct ep_pcie_iatu {
+	u32 start;
+	u32 end;
+	u32 tgt_lower;
+	u32 tgt_upper;
+};
+
+struct ep_pcie_msi_config {
+	u32 lower;
+	u32 upper;
+	u32 data;
+	u32 msg_num;
+};
+
+struct ep_pcie_db_config {
+	u8 base;
+	u8 end;
+	u32 tgt_addr;
+};
+
+struct ep_pcie_hw {
+	struct list_head node;
+	u32 device_id;
+	void **private_data;
+	int (*register_event)(struct ep_pcie_register_event *reg);
+	int (*deregister_event)(void);
+	enum ep_pcie_link_status (*get_linkstatus)(void);
+	int (*config_outbound_iatu)(struct ep_pcie_iatu entries[],
+				u32 num_entries);
+	int (*get_msi_config)(struct ep_pcie_msi_config *cfg);
+	int (*trigger_msi)(u32 idx);
+	int (*wakeup_host)(void);
+	int (*enable_endpoint)(enum ep_pcie_options opt);
+	int (*disable_endpoint)(void);
+	int (*config_db_routing)(struct ep_pcie_db_config chdb_cfg,
+				struct ep_pcie_db_config erdb_cfg);
+	int (*mask_irq_event)(enum ep_pcie_irq_event event,
+				bool enable);
+};
+
+/*
+ * ep_pcie_register_drv - register HW driver.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function registers PCIe HW driver to PCIe endpoint service
+ * layer.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_register_drv(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_deregister_drv - deregister HW driver.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function deregisters PCIe HW driver to PCIe endpoint service
+ * layer.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_deregister_drv(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_get_phandle - get PCIe endpoint HW driver handle.
+ * @id:	PCIe endpoint device ID
+ *
+ * This function deregisters PCIe HW driver from PCIe endpoint service
+ * layer.
+ *
+ * Return: PCIe endpoint HW driver handle
+ */
+struct ep_pcie_hw *ep_pcie_get_phandle(u32 id);
+
+/*
+ * ep_pcie_register_event - register event with PCIe driver.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @reg:	event structure
+ *
+ * This function gives PCIe client driver an option to register
+ * event with PCIe driver.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_register_event(struct ep_pcie_hw *phandle,
+	struct ep_pcie_register_event *reg);
+
+/*
+ * ep_pcie_deregister_event - deregister event with PCIe driver.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function gives PCIe client driver an option to deregister
+ * existing event with PCIe driver.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_deregister_event(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_get_linkstatus - indicate the status of PCIe link.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function tells PCIe client about the status of PCIe link.
+ *
+ * Return: status of PCIe link
+ */
+enum ep_pcie_link_status ep_pcie_get_linkstatus(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_config_outbound_iatu - configure outbound iATU.
+ * @entries:	iatu entries
+ * @num_entries:	number of iatu entries
+ *
+ * This function configures the outbound iATU for PCIe
+ * client's access to the regions in the host memory which
+ * are specified by the SW on host side.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_config_outbound_iatu(struct ep_pcie_hw *phandle,
+				struct ep_pcie_iatu entries[],
+				u32 num_entries);
+
+/*
+ * ep_pcie_get_msi_config - get MSI config info.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @cfg:	pointer to MSI config
+ *
+ * This function returns MSI config info.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_get_msi_config(struct ep_pcie_hw *phandle,
+				struct ep_pcie_msi_config *cfg);
+
+/*
+ * ep_pcie_trigger_msi - trigger an MSI.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @idx:	MSI index number
+ *
+ * This function allows PCIe client to trigger an MSI
+ * on host side.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_trigger_msi(struct ep_pcie_hw *phandle, u32 idx);
+
+/*
+ * ep_pcie_wakeup_host - wake up the host.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function asserts WAKE GPIO to wake up the host.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_wakeup_host(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_enable_endpoint - enable PCIe endpoint.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @opt:	endpoint enable options
+ *
+ * This function is to enable the PCIe endpoint device.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_enable_endpoint(struct ep_pcie_hw *phandle,
+				enum ep_pcie_options opt);
+
+/*
+ * ep_pcie_disable_endpoint - disable PCIe endpoint.
+ * @phandle:	PCIe endpoint HW driver handle
+ *
+ * This function is to disable the PCIe endpoint device.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_disable_endpoint(struct ep_pcie_hw *phandle);
+
+/*
+ * ep_pcie_config_db_routing - Configure routing of doorbells to another block.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @chdb_cfg:	channel doorbell config
+ * @erdb_cfg:	event ring doorbell config
+ *
+ * This function allows PCIe core to route the doorbells intended
+ * for another entity via a target address.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_config_db_routing(struct ep_pcie_hw *phandle,
+				struct ep_pcie_db_config chdb_cfg,
+				struct ep_pcie_db_config erdb_cfg);
+
+/*
+ * ep_pcie_mask_irq_event - enable and disable IRQ event.
+ * @phandle:	PCIe endpoint HW driver handle
+ * @event:	IRQ event
+ * @enable:     true to enable that IRQ event and false to disable
+ *
+ * This function is to enable and disable IRQ event.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int ep_pcie_mask_irq_event(struct ep_pcie_hw *phandle,
+				enum ep_pcie_irq_event event,
+				bool enable);
+#endif
diff --git a/drivers/staging/em9190/inc/msm_mhi_dev.h b/drivers/staging/em9190/inc/msm_mhi_dev.h
new file mode 100644
index 000000000000..88ec5492ca75
--- /dev/null
+++ b/drivers/staging/em9190/inc/msm_mhi_dev.h
@@ -0,0 +1,260 @@
+/* Copyright (c) 2015-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __MSM_MHI_DEV_H
+#define __MSM_MHI_DEV_H
+
+#include <linux/types.h>
+#include <linux/dma-mapping.h>
+
+#define DMA_SYNC		1
+#define DMA_ASYNC		0
+
+enum cb_reason {
+	MHI_DEV_TRE_AVAILABLE = 0,
+	MHI_DEV_CTRL_UPDATE,
+};
+
+struct mhi_dev_client_cb_reason {
+	uint32_t		ch_id;
+	enum cb_reason		reason;
+};
+
+struct mhi_dev_client {
+	struct list_head		list;
+	struct mhi_dev_channel		*channel;
+	void (*event_trigger)(struct mhi_dev_client_cb_reason *cb);
+
+	/* mhi_dev calls are fully synchronous -- only one call may be
+	 * active per client at a time for now.
+	 */
+	struct mutex			write_lock;
+	wait_queue_head_t		wait;
+
+	/* trace logs */
+	spinlock_t			tr_lock;
+	unsigned int			tr_head;
+	unsigned int			tr_tail;
+	struct mhi_dev_trace		*tr_log;
+
+	/* client buffers */
+	struct mhi_dev_iov		*iov;
+	uint32_t			nr_iov;
+};
+
+enum mhi_ctrl_info {
+	MHI_STATE_CONFIGURED = 0,
+	MHI_STATE_CONNECTED = 1,
+	MHI_STATE_DISCONNECTED = 2,
+	MHI_STATE_INVAL,
+};
+
+struct mhi_req {
+	u32                             chan;
+	u32                             mode;
+	u32				chain;
+	void                            *buf;
+	dma_addr_t                      dma;
+	u32                             snd_cmpl;
+	void                            *context;
+	size_t                          len;
+	size_t                          transfer_len;
+	uint32_t                        rd_offset;
+	struct mhi_dev_client           *client;
+	struct list_head                list;
+	union mhi_dev_ring_element_type *el;
+	void (*client_cb)(void *req);
+};
+
+/* SW channel client list */
+enum mhi_client_channel {
+	MHI_CLIENT_LOOPBACK_OUT = 0,
+	MHI_CLIENT_LOOPBACK_IN = 1,
+	MHI_CLIENT_SAHARA_OUT = 2,
+	MHI_CLIENT_SAHARA_IN = 3,
+	MHI_CLIENT_DIAG_OUT = 4,
+	MHI_CLIENT_DIAG_IN = 5,
+	MHI_CLIENT_SSR_OUT = 6,
+	MHI_CLIENT_SSR_IN = 7,
+	MHI_CLIENT_QDSS_OUT = 8,
+	MHI_CLIENT_QDSS_IN = 9,
+	MHI_CLIENT_EFS_OUT = 10,
+	MHI_CLIENT_EFS_IN = 11,
+	MHI_CLIENT_MBIM_OUT = 12,
+	MHI_CLIENT_MBIM_IN = 13,
+	MHI_CLIENT_QMI_OUT = 14,
+	MHI_CLIENT_QMI_IN = 15,
+	MHI_CLIENT_IP_CTRL_0_OUT = 16,
+	MHI_CLIENT_IP_CTRL_0_IN = 17,
+	MHI_CLIENT_IP_CTRL_1_OUT = 18,
+	MHI_CLIENT_IP_CTRL_1_IN = 19,
+	MHI_CLIENT_IPCR_OUT = 20,
+	MHI_CLIENT_IPCR_IN = 21,
+	MHI_CLIENT_IP_CTRL_3_OUT = 22,
+	MHI_CLIENT_IP_CTRL_3_IN = 23,
+	MHI_CLIENT_IP_CTRL_4_OUT = 24,
+	MHI_CLIENT_IP_CTRL_4_IN = 25,
+	MHI_CLIENT_IP_CTRL_5_OUT = 26,
+	MHI_CLIENT_IP_CTRL_5_IN = 27,
+	MHI_CLIENT_IP_CTRL_6_OUT = 28,
+	MHI_CLIENT_IP_CTRL_6_IN = 29,
+	MHI_CLIENT_IP_CTRL_7_OUT = 30,
+	MHI_CLIENT_IP_CTRL_7_IN = 31,
+	MHI_CLIENT_DUN_OUT = 32,
+	MHI_CLIENT_DUN_IN = 33,
+	MHI_CLIENT_IP_SW_0_OUT = 34,
+	MHI_CLIENT_IP_SW_0_IN = 35,
+	MHI_CLIENT_ADB_OUT = 36,
+	MHI_CLIENT_ADB_IN = 37,
+	MHI_CLIENT_IP_SW_2_OUT = 38,
+	MHI_CLIENT_IP_SW_2_IN = 39,
+	MHI_CLIENT_IP_SW_3_OUT = 40,
+	MHI_CLIENT_IP_SW_3_IN = 41,
+	MHI_CLIENT_CSVT_OUT = 42,
+	MHI_CLIENT_CSVT_IN = 43,
+	MHI_CLIENT_SMCT_OUT = 44,
+	MHI_CLIENT_SMCT_IN = 45,
+	MHI_CLIENT_IP_SW_4_OUT  = 46,
+	MHI_CLIENT_IP_SW_4_IN  = 47,
+	MHI_MAX_SOFTWARE_CHANNELS,
+	MHI_CLIENT_TEST_OUT = 60,
+	MHI_CLIENT_TEST_IN = 61,
+	MHI_CLIENT_RESERVED_1_LOWER = 62,
+	MHI_CLIENT_RESERVED_1_UPPER = 99,
+	MHI_CLIENT_IP_HW_0_OUT = 100,
+	MHI_CLIENT_IP_HW_0_IN = 101,
+	MHI_CLIENT_ADPL_IN = 102,
+	MHI_CLIENT_RESERVED_2_LOWER = 102,
+	MHI_CLIENT_RESERVED_2_UPPER = 127,
+	MHI_MAX_CHANNELS = 102,
+	MHI_CLIENT_INVALID = 0xFFFFFFFF
+};
+
+struct mhi_dev_client_cb_data {
+	void			*user_data;
+	enum mhi_client_channel	channel;
+	enum mhi_ctrl_info	ctrl_info;
+};
+
+typedef void (*mhi_state_cb)(struct mhi_dev_client_cb_data *cb_dat);
+
+struct mhi_dev_ready_cb_info {
+	struct list_head		list;
+	mhi_state_cb			cb;
+	struct mhi_dev_client_cb_data	cb_data;
+};
+
+#if defined(CONFIG_MSM_MHI_DEV)
+/**
+ * mhi_dev_open_channel() - Channel open for a given client done prior
+ *		to read/write.
+ * @chan_id:	Software Channel ID for the assigned client.
+ * @handle_client: Structure device for client handle.
+ * @notifier: Client issued callback notification.
+ */
+int mhi_dev_open_channel(uint32_t chan_id,
+		struct mhi_dev_client **handle_client,
+		void (*event_trigger)(struct mhi_dev_client_cb_reason *cb));
+
+/**
+ * mhi_dev_close_channel() - Channel close for a given client.
+ */
+int mhi_dev_close_channel(struct mhi_dev_client *handle_client);
+
+/**
+ * mhi_dev_read_channel() - Channel read for a given client
+ * @mreq:       mreq is the client argument which includes meta info
+ *              like write data location, buffer len, read offset, mode,
+ *              chain and client call back function which will be invoked
+ *              when data read is completed.
+ */
+int mhi_dev_read_channel(struct mhi_req *mreq);
+
+/**
+ * mhi_dev_write_channel() - Channel write for a given software client.
+ * @wreq	wreq is the client argument which includes meta info like
+ *              client handle, read data location, buffer length, mode,
+ *              and client call back function which will free the packet.
+ *              when data write is completed.
+ */
+int mhi_dev_write_channel(struct mhi_req *wreq);
+
+/**
+ * mhi_dev_channel_isempty() - Checks if there is any pending TRE's to process.
+ * @handle_client:	Client Handle issued during mhi_dev_open_channel
+ */
+int mhi_dev_channel_isempty(struct mhi_dev_client *handle);
+
+/**
+ * mhi_ctrl_state_info() - Provide MHI state info
+ *		@idx: Channel number idx. Look at channel_state_info and
+ *		pass the index for the corresponding channel.
+ *		@info: Return the control info.
+ *		MHI_STATE=CONFIGURED - MHI device is present but not ready
+ *					for data traffic.
+ *		MHI_STATE=CONNECTED - MHI device is ready for data transfer.
+ *		MHI_STATE=DISCONNECTED - MHI device has its pipes suspended.
+ *		exposes device nodes for the supported MHI software
+ *		channels.
+ */
+int mhi_ctrl_state_info(uint32_t idx, uint32_t *info);
+
+/**
+ * mhi_register_state_cb() - Clients can register and receive callback after
+ *		MHI channel is connected or disconnected.
+ */
+int mhi_register_state_cb(void (*mhi_state_cb)
+			(struct mhi_dev_client_cb_data *cb_data), void *data,
+			enum mhi_client_channel channel);
+
+#else
+static inline int mhi_dev_open_channel(uint32_t chan_id,
+		struct mhi_dev_client **handle_client,
+		void (*event_trigger)(struct mhi_dev_client_cb_reason *cb))
+{
+	return -EINVAL;
+};
+
+static inline int mhi_dev_close_channel(struct mhi_dev_client *handle_client)
+{
+	return -EINVAL;
+};
+
+static inline int mhi_dev_read_channel(struct mhi_req *mreq)
+{
+	return -EINVAL;
+};
+
+static inline int mhi_dev_write_channel(struct mhi_req *wreq)
+{
+	return -EINVAL;
+};
+
+static inline int mhi_dev_channel_isempty(struct mhi_dev_client *handle)
+{
+	return -EINVAL;
+};
+
+static inline int mhi_ctrl_state_info(uint32_t idx, uint32_t *info)
+{
+	return -EINVAL;
+};
+
+static inline int mhi_register_state_cb(void (*mhi_state_cb)
+			(struct mhi_dev_client_cb_data *cb_data), void *data,
+			enum mhi_client_channel channel)
+{
+	return -EINVAL;
+};
+#endif
+
+#endif /* _MSM_MHI_DEV_H*/
diff --git a/drivers/staging/em9190/inc/msm_pcie.h b/drivers/staging/em9190/inc/msm_pcie.h
new file mode 100644
index 000000000000..ea835f7cc2e8
--- /dev/null
+++ b/drivers/staging/em9190/inc/msm_pcie.h
@@ -0,0 +1,265 @@
+/* Copyright (c) 2014-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __MSM_PCIE_H
+#define __MSM_PCIE_H
+
+#include <linux/types.h>
+#include <linux/pci.h>
+
+enum msm_pcie_config {
+	MSM_PCIE_CONFIG_INVALID = 0,
+	MSM_PCIE_CONFIG_NO_CFG_RESTORE = 0x1,
+	MSM_PCIE_CONFIG_LINKDOWN = 0x2,
+	MSM_PCIE_CONFIG_NO_RECOVERY = 0x4,
+};
+
+enum msm_pcie_pm_opt {
+	MSM_PCIE_SUSPEND,
+	MSM_PCIE_RESUME,
+	MSM_PCIE_DISABLE_PC,
+	MSM_PCIE_ENABLE_PC,
+};
+
+enum msm_pcie_event {
+	MSM_PCIE_EVENT_INVALID = 0,
+	MSM_PCIE_EVENT_LINKDOWN = 0x1,
+	MSM_PCIE_EVENT_LINKUP = 0x2,
+	MSM_PCIE_EVENT_WAKEUP = 0x4,
+	MSM_PCIE_EVENT_L1SS_TIMEOUT = BIT(3),
+};
+
+enum msm_pcie_trigger {
+	MSM_PCIE_TRIGGER_CALLBACK,
+	MSM_PCIE_TRIGGER_COMPLETION,
+};
+
+struct msm_pcie_notify {
+	enum msm_pcie_event event;
+	void *user;
+	void *data;
+	u32 options;
+};
+
+struct msm_pcie_register_event {
+	u32 events;
+	void *user;
+	enum msm_pcie_trigger mode;
+	void (*callback)(struct msm_pcie_notify *notify);
+	struct msm_pcie_notify notify;
+	struct completion *completion;
+	u32 options;
+};
+
+#if 0
+#ifdef CONFIG_PCI_MSM_MSI
+void msm_msi_config_access(struct irq_domain *domain, bool allow);
+void msm_msi_config(struct irq_domain *domain);
+int msm_msi_init(struct device *dev);
+#else
+static inline void msm_msi_config_access(struct irq_domain *domain, bool allow)
+{
+}
+
+static inline void msm_msi_config(struct irq_domain *domain)
+{
+}
+
+static inline int msm_msi_init(struct device *dev)
+{
+	return -EINVAL;
+}
+#endif
+#endif
+
+#ifdef CONFIG_PCI_MSM
+
+/**
+ * msm_pcie_set_link_bandwidth - updates the number of lanes and speed of PCIe
+ * link.
+ * @pci_dev:		client's pci device structure
+ * @target_link_speed:	gen speed
+ * @target_link_width:	number of lanes
+ *
+ * This function gives PCIe clients the control to update the number of lanes
+ * and gen speed of the link.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_set_link_bandwidth(struct pci_dev *pci_dev, u16 target_link_speed,
+				u16 target_link_width);
+
+/**
+ * msm_pcie_l1ss_timeout_disable - disable L1ss timeout feature
+ * @pci_dev:	client's pci device structure
+ *
+ * This function gives PCIe clients the control to disable L1ss timeout
+ * feature.
+ */
+void msm_pcie_l1ss_timeout_disable(struct pci_dev *pci_dev);
+
+/**
+ * msm_pcie_l1ss_timeout_enable - enable L1ss timeout feature
+ * @pci_dev:	client's pci device structure
+ *
+ * This function gives PCIe clients the control to enable L1ss timeout
+ * feature.
+ */
+void msm_pcie_l1ss_timeout_enable(struct pci_dev *pci_dev);
+
+/**
+ * msm_pcie_pm_control - control the power state of a PCIe link.
+ * @pm_opt:	power management operation
+ * @busnr:	bus number of PCIe endpoint
+ * @user:	handle of the caller
+ * @data:	private data from the caller
+ * @options:	options for pm control
+ *
+ * This function gives PCIe endpoint device drivers the control to change
+ * the power state of a PCIe link for their device.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_pm_control(enum msm_pcie_pm_opt pm_opt, u32 busnr, void *user,
+			void *data, u32 options);
+
+/**
+ * msm_pcie_register_event - register an event with PCIe bus driver.
+ * @reg:	event structure
+ *
+ * This function gives PCIe endpoint device drivers an option to register
+ * events with PCIe bus driver.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_register_event(struct msm_pcie_register_event *reg);
+
+/**
+ * msm_pcie_deregister_event - deregister an event with PCIe bus driver.
+ * @reg:	event structure
+ *
+ * This function gives PCIe endpoint device drivers an option to deregister
+ * events with PCIe bus driver.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_deregister_event(struct msm_pcie_register_event *reg);
+
+/**
+ * msm_pcie_recover_config - recover config space.
+ * @dev:	pci device structure
+ *
+ * This function recovers the config space of both RC and Endpoint.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_recover_config(struct pci_dev *dev);
+
+/**
+ * msm_pcie_enumerate - enumerate Endpoints.
+ * @rc_idx:	RC that Endpoints connect to.
+ *
+ * This function enumerates Endpoints connected to RC.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_enumerate(u32 rc_idx);
+
+/**
+ * msm_pcie_recover_config - recover config space.
+ * @dev:	pci device structure
+ *
+ * This function recovers the config space of both RC and Endpoint.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_recover_config(struct pci_dev *dev);
+
+/**
+ * msm_pcie_shadow_control - control the shadowing of PCIe config space.
+ * @dev:	pci device structure
+ * @enable:	shadowing should be enabled or disabled
+ *
+ * This function gives PCIe endpoint device drivers the control to enable
+ * or disable the shadowing of PCIe config space.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_shadow_control(struct pci_dev *dev, bool enable);
+
+/*
+ * msm_pcie_debug_info - run a PCIe specific debug testcase.
+ * @dev:	pci device structure
+ * @option:	specifies which PCIe debug testcase to execute
+ * @base:	PCIe specific range
+ * @offset:	offset of destination register
+ * @mask:	mask the bit(s) of destination register
+ * @value:	value to be written to destination register
+ *
+ * This function gives PCIe endpoint device drivers the control to
+ * run a debug testcase.
+ *
+ * Return: 0 on success, negative value on error
+ */
+int msm_pcie_debug_info(struct pci_dev *dev, u32 option, u32 base,
+			u32 offset, u32 mask, u32 value);
+
+#else /* !CONFIG_PCI_MSM */
+static inline int msm_pcie_pm_control(enum msm_pcie_pm_opt pm_opt, u32 busnr,
+			void *user, void *data, u32 options)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_l1ss_timeout_disable(struct pci_dev *pci_dev)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_l1ss_timeout_enable(struct pci_dev *pci_dev)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_register_event(struct msm_pcie_register_event *reg)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_deregister_event(struct msm_pcie_register_event *reg)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_recover_config(struct pci_dev *dev)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_enumerate(u32 rc_idx)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_shadow_control(struct pci_dev *dev, bool enable)
+{
+	return -ENODEV;
+}
+
+static inline int msm_pcie_debug_info(struct pci_dev *dev, u32 option, u32 base,
+			u32 offset, u32 mask, u32 value)
+{
+	return -ENODEV;
+}
+#endif /* CONFIG_PCI_MSM */
+
+#endif /* __MSM_PCIE_H */
diff --git a/drivers/staging/em9190/inc/msm_rmnet.h b/drivers/staging/em9190/inc/msm_rmnet.h
new file mode 100644
index 000000000000..11edb35af795
--- /dev/null
+++ b/drivers/staging/em9190/inc/msm_rmnet.h
@@ -0,0 +1,164 @@
+#ifndef _UAPI_MSM_RMNET_H_
+#define _UAPI_MSM_RMNET_H_
+
+/* Bitmap macros for RmNET driver operation mode. */
+#define RMNET_MODE_NONE     (0x00)
+#define RMNET_MODE_LLP_ETH  (0x01)
+#define RMNET_MODE_LLP_IP   (0x02)
+#define RMNET_MODE_QOS      (0x04)
+#define RMNET_MODE_MASK     (RMNET_MODE_LLP_ETH | \
+			     RMNET_MODE_LLP_IP  | \
+			     RMNET_MODE_QOS)
+
+#define RMNET_IS_MODE_QOS(mode)  \
+	((mode & RMNET_MODE_QOS) == RMNET_MODE_QOS)
+#define RMNET_IS_MODE_IP(mode)   \
+	((mode & RMNET_MODE_LLP_IP) == RMNET_MODE_LLP_IP)
+
+/* IOCTL commands
+ * Values chosen to not conflict with other drivers in the ecosystem
+ */
+
+#define RMNET_IOCTL_SET_LLP_ETHERNET 0x000089F1 /* Set Ethernet protocol  */
+#define RMNET_IOCTL_SET_LLP_IP       0x000089F2 /* Set RAWIP protocol     */
+#define RMNET_IOCTL_GET_LLP          0x000089F3 /* Get link protocol      */
+#define RMNET_IOCTL_SET_QOS_ENABLE   0x000089F4 /* Set QoS header enabled */
+#define RMNET_IOCTL_SET_QOS_DISABLE  0x000089F5 /* Set QoS header disabled*/
+#define RMNET_IOCTL_GET_QOS          0x000089F6 /* Get QoS header state   */
+#define RMNET_IOCTL_GET_OPMODE       0x000089F7 /* Get operation mode     */
+#define RMNET_IOCTL_OPEN             0x000089F8 /* Open transport port    */
+#define RMNET_IOCTL_CLOSE            0x000089F9 /* Close transport port   */
+#define RMNET_IOCTL_FLOW_ENABLE      0x000089FA /* Flow enable            */
+#define RMNET_IOCTL_FLOW_DISABLE     0x000089FB /* Flow disable           */
+#define RMNET_IOCTL_FLOW_SET_HNDL    0x000089FC /* Set flow handle        */
+#define RMNET_IOCTL_EXTENDED         0x000089FD /* Extended IOCTLs        */
+
+/* RmNet Data Required IOCTLs */
+#define RMNET_IOCTL_GET_SUPPORTED_FEATURES     0x0000   /* Get features    */
+#define RMNET_IOCTL_SET_MRU                    0x0001   /* Set MRU         */
+#define RMNET_IOCTL_GET_MRU                    0x0002   /* Get MRU         */
+#define RMNET_IOCTL_GET_EPID                   0x0003   /* Get endpoint ID */
+#define RMNET_IOCTL_GET_DRIVER_NAME            0x0004   /* Get driver name */
+#define RMNET_IOCTL_ADD_MUX_CHANNEL            0x0005   /* Add MUX ID      */
+#define RMNET_IOCTL_SET_EGRESS_DATA_FORMAT     0x0006   /* Set EDF         */
+#define RMNET_IOCTL_SET_INGRESS_DATA_FORMAT    0x0007   /* Set IDF         */
+#define RMNET_IOCTL_SET_AGGREGATION_COUNT      0x0008   /* Set agg count   */
+#define RMNET_IOCTL_GET_AGGREGATION_COUNT      0x0009   /* Get agg count   */
+#define RMNET_IOCTL_SET_AGGREGATION_SIZE       0x000A   /* Set agg size    */
+#define RMNET_IOCTL_GET_AGGREGATION_SIZE       0x000B   /* Get agg size    */
+#define RMNET_IOCTL_FLOW_CONTROL               0x000C   /* Do flow control */
+#define RMNET_IOCTL_GET_DFLT_CONTROL_CHANNEL   0x000D   /* For legacy use  */
+#define RMNET_IOCTL_GET_HWSW_MAP               0x000E   /* Get HW/SW map   */
+#define RMNET_IOCTL_SET_RX_HEADROOM            0x000F   /* RX Headroom     */
+#define RMNET_IOCTL_GET_EP_PAIR                0x0010   /* Endpoint pair   */
+#define RMNET_IOCTL_SET_QOS_VERSION            0x0011   /* 8/6 byte QoS hdr*/
+#define RMNET_IOCTL_GET_QOS_VERSION            0x0012   /* 8/6 byte QoS hdr*/
+#define RMNET_IOCTL_GET_SUPPORTED_QOS_MODES    0x0013   /* Get QoS modes   */
+#define RMNET_IOCTL_SET_SLEEP_STATE            0x0014   /* Set sleep state */
+#define RMNET_IOCTL_SET_XLAT_DEV_INFO          0x0015   /* xlat dev name   */
+#define RMNET_IOCTL_DEREGISTER_DEV             0x0016   /* Dereg a net dev */
+#define RMNET_IOCTL_GET_SG_SUPPORT             0x0017   /* Query sg support*/
+#define RMNET_IOCTL_SET_OFFLOAD                0x0018   /* Set IPA offload */
+
+/* Return values for the RMNET_IOCTL_GET_SUPPORTED_FEATURES IOCTL */
+#define RMNET_IOCTL_FEAT_NOTIFY_MUX_CHANNEL              (1<<0)
+#define RMNET_IOCTL_FEAT_SET_EGRESS_DATA_FORMAT          (1<<1)
+#define RMNET_IOCTL_FEAT_SET_INGRESS_DATA_FORMAT         (1<<2)
+#define RMNET_IOCTL_FEAT_SET_AGGREGATION_COUNT           (1<<3)
+#define RMNET_IOCTL_FEAT_GET_AGGREGATION_COUNT           (1<<4)
+#define RMNET_IOCTL_FEAT_SET_AGGREGATION_SIZE            (1<<5)
+#define RMNET_IOCTL_FEAT_GET_AGGREGATION_SIZE            (1<<6)
+#define RMNET_IOCTL_FEAT_FLOW_CONTROL                    (1<<7)
+#define RMNET_IOCTL_FEAT_GET_DFLT_CONTROL_CHANNEL        (1<<8)
+#define RMNET_IOCTL_FEAT_GET_HWSW_MAP                    (1<<9)
+
+/* Input values for the RMNET_IOCTL_SET_EGRESS_DATA_FORMAT IOCTL  */
+#define RMNET_IOCTL_EGRESS_FORMAT_MAP                  (1<<1)
+#define RMNET_IOCTL_EGRESS_FORMAT_AGGREGATION          (1<<2)
+#define RMNET_IOCTL_EGRESS_FORMAT_MUXING               (1<<3)
+#define RMNET_IOCTL_EGRESS_FORMAT_CHECKSUM             (1<<4)
+
+/* Input values for the RMNET_IOCTL_SET_INGRESS_DATA_FORMAT IOCTL */
+#define RMNET_IOCTL_INGRESS_FORMAT_MAP                 (1<<1)
+#define RMNET_IOCTL_INGRESS_FORMAT_DEAGGREGATION       (1<<2)
+#define RMNET_IOCTL_INGRESS_FORMAT_DEMUXING            (1<<3)
+#define RMNET_IOCTL_INGRESS_FORMAT_CHECKSUM            (1<<4)
+#define RMNET_IOCTL_INGRESS_FORMAT_AGG_DATA            (1<<5)
+
+/* Input values for the RMNET_IOCTL_SET_OFFLOAD */
+#define RMNET_IOCTL_OFFLOAD_FORMAT_NONE                   (0)
+#define RMNET_IOCTL_COALESCING_FORMAT_TCP              (1<<0)
+#define RMNET_IOCTL_COALESCING_FORMAT_UDP              (1<<1)
+
+/* User space may not have this defined. */
+#ifndef IFNAMSIZ
+#define IFNAMSIZ 16
+#endif
+
+struct rmnet_ioctl_extended_s {
+	uint32_t   extended_ioctl;
+	union {
+		uint32_t data; /* Generic data field for most extended IOCTLs */
+
+		/* Return values for
+		 *    RMNET_IOCTL_GET_DRIVER_NAME
+		 *    RMNET_IOCTL_GET_DFLT_CONTROL_CHANNEL
+		 */
+		int8_t if_name[IFNAMSIZ];
+
+		/* Input values for the RMNET_IOCTL_ADD_MUX_CHANNEL IOCTL */
+		struct {
+			uint32_t  mux_id;
+			int8_t    vchannel_name[IFNAMSIZ];
+		} rmnet_mux_val;
+
+		/* Input values for the RMNET_IOCTL_FLOW_CONTROL IOCTL */
+		struct {
+			uint8_t   flow_mode;
+			uint8_t   mux_id;
+		} flow_control_prop;
+
+		/* Return values for RMNET_IOCTL_GET_EP_PAIR */
+		struct {
+			uint32_t   consumer_pipe_num;
+			uint32_t   producer_pipe_num;
+		} ipa_ep_pair;
+
+		struct {
+			uint32_t __data; /* Placeholder for legacy data*/
+			uint32_t agg_size;
+			uint32_t agg_count;
+		} ingress_format;
+
+		/* Input values for the RMNET_IOCTL_SET_OFFLOAD */
+		struct {
+			uint32_t   flags;
+			uint8_t    mux_id;
+		} offload_params;
+	} u;
+};
+
+struct rmnet_ioctl_data_s {
+	union {
+		uint32_t	operation_mode;
+		uint32_t	tcm_handle;
+	} u;
+};
+
+#define RMNET_IOCTL_QOS_MODE_6   (1<<0)
+#define RMNET_IOCTL_QOS_MODE_8   (1<<1)
+
+/* QMI QoS header definition */
+struct QMI_QOS_HDR_S {
+	unsigned char    version;
+	unsigned char    flags;
+	uint32_t         flow_id;
+} __attribute((__packed__));
+
+/* QMI QoS 8-byte header. */
+struct qmi_qos_hdr8_s {
+	struct QMI_QOS_HDR_S   hdr;
+	uint8_t                reserved[2];
+} __attribute((__packed__));
+
+#endif /* _UAPI_MSM_RMNET_H_ */
diff --git a/drivers/staging/em9190/inc/qmi_rmnet.h b/drivers/staging/em9190/inc/qmi_rmnet.h
new file mode 100644
index 000000000000..eb7fc1c80ff0
--- /dev/null
+++ b/drivers/staging/em9190/inc/qmi_rmnet.h
@@ -0,0 +1,143 @@
+/*
+ * Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _QMI_RMNET_H
+#define _QMI_RMNET_H
+
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+
+struct qmi_rmnet_ps_ind {
+	void (*ps_on_handler)(void *);
+	void (*ps_off_handler)(void *);
+	struct list_head list;
+};
+
+
+#ifdef CONFIG_QCOM_QMI_RMNET
+void qmi_rmnet_qmi_exit(void *qmi_pt, void *port);
+void qmi_rmnet_change_link(struct net_device *dev, void *port, void *tcm_pt);
+void qmi_rmnet_enable_all_flows(struct net_device *dev);
+bool qmi_rmnet_all_flows_enabled(struct net_device *dev);
+#else
+static inline void qmi_rmnet_qmi_exit(void *qmi_pt, void *port)
+{
+}
+
+static inline void
+qmi_rmnet_change_link(struct net_device *dev, void *port, void *tcm_pt)
+{
+}
+
+static inline void
+qmi_rmnet_enable_all_flows(struct net_device *dev)
+{
+}
+
+static inline bool
+qmi_rmnet_all_flows_enabled(struct net_device *dev)
+{
+	return true;
+}
+#endif
+
+#ifdef CONFIG_QCOM_QMI_DFC
+void *qmi_rmnet_qos_init(struct net_device *real_dev, u8 mux_id);
+void qmi_rmnet_qos_exit(struct net_device *dev, void *qos);
+void qmi_rmnet_burst_fc_check(struct net_device *dev,
+			      int ip_type, u32 mark, unsigned int len);
+int qmi_rmnet_get_queue(struct net_device *dev, struct sk_buff *skb);
+#else
+static inline void *
+qmi_rmnet_qos_init(struct net_device *real_dev, u8 mux_id)
+{
+	return NULL;
+}
+
+static inline void qmi_rmnet_qos_exit(struct net_device *dev, void *qos)
+{
+}
+
+static inline void
+qmi_rmnet_burst_fc_check(struct net_device *dev,
+			 int ip_type, u32 mark, unsigned int len)
+{
+}
+
+static inline int qmi_rmnet_get_queue(struct net_device *dev,
+				       struct sk_buff *skb)
+{
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_QCOM_QMI_POWER_COLLAPSE
+int qmi_rmnet_set_powersave_mode(void *port, uint8_t enable);
+void qmi_rmnet_work_init(void *port);
+void qmi_rmnet_work_exit(void *port);
+void qmi_rmnet_work_maybe_restart(void *port);
+void qmi_rmnet_set_dl_msg_active(void *port);
+bool qmi_rmnet_ignore_grant(void *port);
+
+int qmi_rmnet_ps_ind_register(void *port,
+			      struct qmi_rmnet_ps_ind *ps_ind);
+int qmi_rmnet_ps_ind_deregister(void *port,
+				struct qmi_rmnet_ps_ind *ps_ind);
+void qmi_rmnet_ps_off_notify(void *port);
+void qmi_rmnet_ps_on_notify(void *port);
+
+#else
+static inline int qmi_rmnet_set_powersave_mode(void *port, uint8_t enable)
+{
+	return 0;
+}
+static inline void qmi_rmnet_work_init(void *port)
+{
+}
+static inline void qmi_rmnet_work_exit(void *port)
+{
+}
+static inline void qmi_rmnet_work_maybe_restart(void *port)
+{
+
+}
+static inline void qmi_rmnet_set_dl_msg_active(void *port)
+{
+}
+static inline bool qmi_rmnet_ignore_grant(void *port)
+{
+	return false;
+}
+
+static inline int qmi_rmnet_ps_ind_register(struct rmnet_port *port,
+				     struct qmi_rmnet_ps_ind *ps_ind)
+{
+	return 0;
+}
+static inline int qmi_rmnet_ps_ind_deregister(struct rmnet_port *port,
+				       struct qmi_rmnet_ps_ind *ps_ind)
+{
+	return 0;
+}
+
+static inline void qmi_rmnet_ps_off_notify(struct rmnet_port *port)
+{
+
+}
+
+static inline void qmi_rmnet_ps_on_notify(struct rmnet_port *port)
+{
+
+}
+#endif
+#endif /*_QMI_RMNET_H*/
diff --git a/drivers/staging/em9190/inc/rmnet.h b/drivers/staging/em9190/inc/rmnet.h
new file mode 100644
index 000000000000..12dce0b432ce
--- /dev/null
+++ b/drivers/staging/em9190/inc/rmnet.h
@@ -0,0 +1,44 @@
+/* Copyright (c) 2018, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM rmnet
+
+#if !defined(_TRACE_RMNET_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_RMNET_H
+
+#include <linux/skbuff.h>
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(rmnet_xmit_skb,
+
+	TP_PROTO(struct sk_buff *skb),
+
+	TP_ARGS(skb),
+
+	TP_STRUCT__entry(
+		__string(dev_name, skb->dev->name)
+		__field(unsigned int, len)
+	),
+
+	TP_fast_assign(
+		__assign_str(dev_name, skb->dev->name);
+		__entry->len = skb->len;
+	),
+
+	TP_printk("dev_name=%s len=%u", __get_str(dev_name), __entry->len)
+);
+
+#endif /* _TRACE_RMNET_H */
+
+#define TRACE_INCLUDE_PATH "../"
+#include "define_trace.h"
diff --git a/drivers/staging/em9190/inc/rmnet_qmi.h b/drivers/staging/em9190/inc/rmnet_qmi.h
new file mode 100644
index 000000000000..fcf48d9f2d8f
--- /dev/null
+++ b/drivers/staging/em9190/inc/rmnet_qmi.h
@@ -0,0 +1,88 @@
+/*
+ * Copyright (c) 2018-2019, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _RMNET_QMI_H
+#define _RMNET_QMI_H
+
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+
+void rmnet_map_tx_qmap_cmd(struct sk_buff *qmap_skb);
+
+#ifdef CONFIG_QCOM_QMI_RMNET
+void *rmnet_get_qmi_pt(void *port);
+void *rmnet_get_qos_pt(struct net_device *dev);
+void *rmnet_get_rmnet_port(struct net_device *dev);
+struct net_device *rmnet_get_rmnet_dev(void *port, u8 mux_id);
+void rmnet_reset_qmi_pt(void *port);
+void rmnet_init_qmi_pt(void *port, void *qmi);
+void rmnet_enable_all_flows(void *port);
+bool rmnet_all_flows_enabled(void *port);
+void rmnet_set_powersave_format(void *port);
+void rmnet_clear_powersave_format(void *port);
+void rmnet_get_packets(void *port, u64 *rx, u64 *tx);
+int rmnet_get_powersave_notif(void *port);
+#else
+static inline void *rmnet_get_qmi_pt(void *port)
+{
+	return NULL;
+}
+
+static inline void *rmnet_get_qos_pt(struct net_device *dev)
+{
+	return NULL;
+}
+
+static inline void *rmnet_get_rmnet_port(struct net_device *dev)
+{
+	return NULL;
+}
+
+static inline struct net_device *rmnet_get_rmnet_dev(void *port,
+						     u8 mux_id)
+{
+	return NULL;
+}
+
+static inline void rmnet_reset_qmi_pt(void *port)
+{
+}
+
+static inline void rmnet_init_qmi_pt(void *port, void *qmi)
+{
+}
+
+static inline void rmnet_enable_all_flows(void *port)
+{
+}
+
+static inline bool rmnet_all_flows_enabled(void *port)
+{
+	return true;
+}
+
+static inline void rmnet_set_port_format(void *port)
+{
+}
+
+static inline void rmnet_get_packets(void *port, u64 *rx, u64 *tx)
+{
+}
+
+static inline int rmnet_get_powersave_notif(void *port)
+{
+	return 0;
+}
+
+#endif /* CONFIG_QCOM_QMI_RMNET */
+#endif /*_RMNET_QMI_H*/
diff --git a/drivers/staging/em9190/readme.txt b/drivers/staging/em9190/readme.txt
new file mode 100644
index 000000000000..907e934d68d8
--- /dev/null
+++ b/drivers/staging/em9190/readme.txt
@@ -0,0 +1,247 @@
+Sierra Wireless PCIe Driver
+===========================
+Supported Devices: EM9190 DV1/DV2/DV3
+
+How to Build the driver
+=======================
+run "make" to generate the driver binaries: /controllers/mhictrl.ko 
+/devices/mhiuci.ko, /devices/mhinet.ko and /devices/mhitty.ko
+
+mhiuci.ko - (QMI-MBIM) Char Interface Driver
+mhitty.ko - (DM/AT) TTY Interface driver
+mhinet.ko - Network Interface Driver 
+
+How to Install the driver
+========================= 
+1. run "make install"
+2. reboot
+
+How to verify driver installation
+=================================
+"lspci -v" should generate an output as below: 
+
+02:00.0 Unassigned class [ff00]: Qualcomm Device 0306
+	Subsystem: Qualcomm Device a01f
+	Flags: bus master, fast devsel, latency 0, IRQ 123
+	Memory at f7201000 (64-bit, non-prefetchable) [size=4K]
+	Memory at f7200000 (64-bit, non-prefetchable) [size=4K]
+	Capabilities: <access denied>
+	Kernel driver in use: mhictrl
+	Kernel modules: mhictrl
+
+The Char driver exposes two device interfaces:
+1. /dev/mhi_0306_00.02.00_pipe_12, MBIM interface
+2. /dev/mhi_0306_00.02.00_pipe_14, QMI interface
+
+00.02.00 is platform specfic
+
+The TTY driver exposes two device interfaces:
+1. /dev/mhitty0, DM port
+2. /dev/mhitty1, AT Port
+
+How to communicate with AT port 
+==================================
+run "sudo minicom -D /dev/mhitty1". 
+To close it, press CTRL+A and then X
+
+How to establish a data connection 
+==================================
+1. Turn on the radio
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --set-radio-state=on
+
+2. Connect to the MB/cellular network
+sudo mbim-network /dev/mhi_0306_00.02.00_pipe_12 start
+
+3. Get IP addresses
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --query-ip-configuration
+
+[/dev/mhichar0] IPv4 configuration available: 'address, gateway, dns, mtu'
+     IP [0]: '25.168.250.74/30'
+    Gateway: '25.168.250.73'
+    DNS [0]: '64.71.255.254'
+    DNS [1]: '64.71.255.253'
+        MTU: '1460'
+
+[/dev/mhichar0] IPv6 configuration available: 'address, gateway, dns, mtu'
+     IP [0]: '2605:8d80:480:494d:687a:727d:2b6e:2bc3/64'
+    Gateway: '2605:8d80:480:494d:9d20:80cb:80b8:b20e'
+    DNS [0]: '2607:f798:18:10:0:640:7125:5254'
+    DNS [1]: '2607:f798:18:10:0:640:7125:5253'
+        MTU: '1460'
+
+4. Set the IP addresses for the network interface
+sudo ip addr add 25.168.250.74/30 dev mhi_netdev0
+sudo ip addr add 2605:8d80:480:494d:687a:727d:2b6e:2bc3/64 dev mhi_netdev0
+
+5. Set the MTU Size
+sudo ifconfig mhi_netdev0 mtu 1460
+
+6. Bring up the network interface and set the route 
+sudo ip link set mhi_netdev0 up
+sudo ip rout add default dev mhi_netdev0
+
+7. Run "ifconfig" to check the status for mhi_netdev0
+mhi_netdev0: flags=4291<UP,RUNNING>  mtu 1460
+        inet 25.168.250.74  netmask 255.255.255.252  broadcast 0.0.0.0
+        inet6 2605:8d80:480:494d:687a:727d:2b6e:2bc3  prefixlen 64  scopeid 0x0<global>
+        RX packets 2  bytes 224 (224.0 B)
+        RX errors 0  dropped 0  overruns 0  frame 0
+        TX packets 44  bytes 9692 (9.6 KB)
+        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
+
+8. DNS setting may have to be updated (one time only)
+sudo vi /etc/systemd/resolved.conf
+
+[Resolve]
+#DNS=
+FallbackDNS=8.8.8.8
+#Domains=
+#LLMNR=no
+#MulticastDNS=no
+#DNSSEC=no
+#Cache=yes
+#DNSStubListener=yes
+
+Change FallbackDNS to have a value 8.8.8.8 as above. 
+
+Restart systemd-resolved service
+sudo systemctl restart systemd-resolved.service
+
+The data connection should now be established.
+
+How to disconnect a data connection 
+==================================
+
+1. Disonnect to the MB/cellular network
+sudo mbim-network /dev/mhi_0306_00.02.00_pipe_12 stop
+
+2. Bring down the network interface 
+sudo ip link set mhi_netdev0 down
+
+How to enable driver logging
+==================================
+Driver Logging is disabled by default. 
+
+It may be useful to enable driver logging for troubleshooting.
+To enable logging at compile time, the following changes are required and drivers to be recompiled.
+
+1. /core/mhi_init.c
+Change "bool debug = false;" to "bool debug = true;" to enable the log for mhictrl driver.
+
+2. /devices/mhi_netdev.c
+Update "bool debug = false;" to "bool debug = true;" to enable the log for mhinet driver.
+
+3. /devices/mhitty.c
+Update "bool debug = false;" to "bool debug = true;" to enable the log for mhitty driver.
+
+4. /devices/mhiuci.c
+Update "bool debug = false;" to "bool debug = true;" to enable the log for mhiuci driver.
+
+How to enable driver logging at runtime
+=======================================
+Navigate to the directory /sys/module/mhictrl/parameters, run "sudo chmod 666 debug" and then "echo 1 > debug". 
+
+Change mhictrl to mhinet, mhiuci or mhitty in the above directory to enable logging for mhinet, mhiuci or mhitty.
+
+How to update FW 
+==================================
+sudo ./fwdwl-litearm64 -p /dev/mhi_0306_00.02.00_pipe_12 -c MBIM -d /dev/mhiqdl0 -f ./fw -m 4 -l ./fdt.log -e 1
+
+fwdwl-litearm64 is the Firmware Download Tool and FW image and PRI are saved under directory "fw".
+
+How to make multiple PDN connections
+====================================
+
+1. Add VLAN for individual PDP context.
+
+sudo ip link add link mhi_netdev0 name vlan.0 type vlan id 4094
+
+sudo ip link add link mhi_netdev0 name vlan.1 type vlan id 1
+
+Two VLAN interfaces (vlan.0 and vlan.1) were created. To verify, enter "ifconfig -a".
+
+2. Turn on the radio. 
+
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --set-radio-state=on
+
+3. Establish the data connection. 
+
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --connect=apn=lteinternet.apn,ip-type=ipv4,session-id=0
+
+After the connection is established, you can find the IP address, and MTU size. 
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --query-ip-configuration
+
+4. Bring up the interface
+
+sudo ip link set mhi_netdev0 up
+
+sudo ip link set vlan.0 up
+
+sudo ifconfig vlan.0 25.120.167.134/30 mtu 1460 up   (note: IP address and MTU size from 3)
+
+5. Add the route for this IP address.
+
+sudo route add -net 8.8.8.8 netmask 255.255.255.255 gw 25.120.167.134
+
+6.  Establish another data connection.
+
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --connect=apn=ltestaticip.apn,ip-type=ipv4,session-id=1
+
+7. Bring up the second interface
+
+sudo ip link set vlan.1 up
+sudo ifconfig vlan.1 72.139.242.197 mtu 1460 up (note: IP address and MTU size from 6)
+
+8. Add the route for the second IP address.
+
+sudo route add -net 4.2.2.2 netmask 255.255.255.255 gw 72.139.242.197
+
+9. Run the ping test. 
+
+ping 4.2.2.2 -c 1
+
+ping 8.8.8.8 -c 1
+
+You should see the responses for those ping requests.
+
+10. Disconnect data call
+
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --disconnect=0
+sudo mbimcli -d /dev/mhi_0306_00.02.00_pipe_12 -v -p --disconnect=1
+
+11. Bring down the interface
+
+sudo ip link set vlan.0 down
+sudo ip link set vlan.1 down
+sudo ip link set mhi_netdev0 down
+
+12. Remove VLAN
+
+sudo ip link del vlan.0
+sudo ip link del vlan.1
+
+How to use ADB port
+==================================
+run "make adb" to build the drivers and "make install" to install them, 
+then reboot. A new ADB interface like /dev/mhi_0306_00.02.00_pipe_36 should be
+available after reboot. 
+
+To use ADB, run sudo -i and 
+nc -l 5555 > /dev/mhi_0306_00.02.00_pipe_36  < /dev/mhi_0306_00.02.00_pipe_36".
+
+Open a new terminal, and run adb devices, now the ADB device should be listed
+as below. 
+
+List of devices attached
+* daemon not running; starting now at tcp:5037
+* daemon started successfully
+emulator-5554	device
+ 
+
+
+
+
+
+
+
+
diff --git a/drivers/staging/em9190/xdp_readme.txt b/drivers/staging/em9190/xdp_readme.txt
new file mode 100644
index 000000000000..7b3eb48349a2
--- /dev/null
+++ b/drivers/staging/em9190/xdp_readme.txt
@@ -0,0 +1,33 @@
+XDP = eXpress Data Path
+https://en.wikipedia.org/wiki/Express_Data_Path
+
+The XDP technology avoids the overhead of using Socket Buffers (SKBs) in kernel network driver by using Extended Berkley Packet Filters (eBPF). The driver exposes a data receiving callback for an ebpf program which can process packets.
+Steps on how to test XDP using the mhinet network driver. 
+1.	Build network driver for XDP support: make xdp. Install the new driver.
+2.	Establish a data connection, bring up the network interface and assign IP address, gateway. Please refer the readme.txt for details. 
+3.	Compile an ebpf program, for example, xdp_dummy.o.
+4.	Run sudo ip link set dev mhi_netdev0 xdpdrv object xdp_dummy.o to load the ebpf program
+5.	Generate some data traffic and check the results
+6.	After completing the test, run sudo ip link set dev mhi_netdev0 xdpdrv off to remove the ebpf program.
+
+Here is very simple program to drop all the incoming packets.
+
+// SPDX-License-Identifier: GPL-2.0
+
+#define KBUILD_MODNAME "xdp_dummy"
+#include <uapi/linux/bpf.h>
+#include "bpf_helpers.h"
+
+SEC("prog")
+int xdp_dummy(struct xdp_md *ctx)
+{
+    return XDP_DROP;
+}
+
+char _license[] SEC("license") = "GPL";
+
+More useful samples can be found here. 
+
+https://github.com/torvalds/linux/tree/master/samples/bpf
+
+https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/samples/bpf
-- 
2.17.1

